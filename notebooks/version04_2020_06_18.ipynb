{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yi9E6gwR6bw"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "attachments": {
    "186676f7-f9d2-4727-a904-889751b9f6b6.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:186676f7-f9d2-4727-a904-889751b9f6b6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PhXEkPQNR6bx"
   },
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V04\"\n",
    "MODEL_NUMBER = MODEL_PREFIrandom_state=]\n",
    "MODEL_NAME = 'distilbert' # options include 'xlm' or 'distilbert'\n",
    "\n",
    "NUM_EPOCHS = [2]\n",
    "LR = 5e-5\n",
    "MAX_SEQ_LEN = 75\n",
    "SAMPLE_SIZE = 3000\n",
    "PSUEDO_PROB_THRESH_LOW = 0.05\n",
    "PSUEDO_PROB_THRESH_HIGH = 0.8\n",
    "\n",
    "RUN_ON_SAMPLE = 0\n",
    "ON_KAGGLE = False\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    BATCH_SIZE = 64\n",
    "    PREDICT_BATCH_SIZE = 512\n",
    "else:\n",
    "    BATCH_SIZE = 16\n",
    "    PREDICT_BATCH_SIZE = 256\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vBSH2FLlR6b1"
   },
   "outputs": [],
   "source": [
    "if ON_KAGGLE:\n",
    "    RESULTS_DIR = '../working/'\n",
    "    DATA_DIR = '../input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = '../input/tf-xlm-roberta-base/'\n",
    "    else:\n",
    "        MODEL_DIR = '../input/tf-distilbert-base-multilingual-cased/'\n",
    "else:\n",
    "    PATH = \"..\" #\"/content/drive/My Drive/Kaggle/jigsaw-multilingual-toxic-comment-classification\"\n",
    "    RESULTS_DIR = PATH+\"/results/\"\n",
    "    DATA_DIR = PATH+\"/data/\"\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = PATH+\"/models/tf-xlm-roberta-base/\"\n",
    "    else:\n",
    "        MODEL_DIR = PATH+\"/models/distilbert-base-multilingual-cased/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCnsk-7nR6b4"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oW_oT4SZR6b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.optimizers import TriangularCyclicalLearningRate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXbm0XVcR6b8"
   },
   "outputs": [],
   "source": [
    "seeded_value = 12345\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ybWWTMP8R6cA",
    "outputId": "c26e2379-92f7-4bcb-bd93-a07825d058f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 18 16:31:11 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ESteN4q4TAAW",
    "outputId": "a6428c7d-4959-486c-9c24-5dc489c8e04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR4HCYGTR6cG"
   },
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sJh7lbL1R6cH",
    "outputId": "68c74149-67dc-4a43-9d9d-8cec23e054d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iAdsW80fR6cL"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ8ZRVGTR6cO"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wQ_YumuXR6cO"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR+'jigsaw-toxic-comment-train.csv')\n",
    "validation = pd.read_csv(DATA_DIR+'validation.csv')\n",
    "test = pd.read_csv(DATA_DIR+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5Chd2u5AR6cR"
   },
   "outputs": [],
   "source": [
    "train['lang'] = 'en'\n",
    "\n",
    "train['set'] = 'train'\n",
    "validation['set'] = 'valid'\n",
    "test['set'] = 'test'\n",
    "\n",
    "test['toxic'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pZIaKjKXR6cU",
    "outputId": "f2b9d702-b64c-4838-bbcc-b92a243a8e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate', 'lang', 'set'],\n",
      "      dtype='object')\n",
      "Index(['id', 'comment_text', 'lang', 'toxic', 'set'], dtype='object')\n",
      "Index(['id', 'content', 'lang', 'set', 'toxic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(validation.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sNlk-kDDR6cY"
   },
   "outputs": [],
   "source": [
    "train.columns = ['id', 'text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'lang', 'set']\n",
    "validation.columns = ['id', 'text', 'lang', 'toxic', 'set']\n",
    "test.columns = ['id', 'text', 'lang', 'set', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "saymbsyYR6ca"
   },
   "outputs": [],
   "source": [
    "REQ_COLS = ['id', 'set', 'text', 'lang', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train[\"text\"].astype(str)\n",
    "validation['text'] = validation[\"text\"].astype(str)\n",
    "test['text'] = test[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2VCNIC95R6cf"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train[REQ_COLS].sample(SAMPLE_SIZE, random_state=seeded_value),\n",
    "                  validation[REQ_COLS]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-EPYDkCNR6ci",
    "outputId": "d36606e0-9c8d-4e2a-d2f8-82d4c9f15590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UlrDxcmLR6cl",
    "outputId": "7436086b-4453-44a1-d6be-627e2bd63171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>set</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6034</th>\n",
       "      <td>6034</td>\n",
       "      <td>valid</td>\n",
       "      <td>Ciao. La voce Tokyo Mew Mew - Amiche vincenti era stata spostata in modo errato (con copia e incolla) e io ho tentato di rimettere tutto a posto recuperando la cronologia. Potresti per favore dare un occhiata per vedere se adesso va bene? Non vorrei che si fosse perduta qualche recente tua modifica (alla fine le voci erano diventate 3 con le rispettive cronologie) -( Ciao e grazie. Lepido (msg)</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>3724</td>\n",
       "      <td>valid</td>\n",
       "      <td>Venerdì sera. tattica retorica abusata e piuttosto squallida il dire  sempre con questa storia? . L hai usata per giustificare il tuo essere inutilmente parolaio. La usi adesso su una faccenda nuova nuova. Quando crescerai?    JollyRoger    ۩ lo sceriffo</td>\n",
       "      <td>it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    set  \\\n",
       "6034  6034  valid   \n",
       "3724  3724  valid   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "6034  Ciao. La voce Tokyo Mew Mew - Amiche vincenti era stata spostata in modo errato (con copia e incolla) e io ho tentato di rimettere tutto a posto recuperando la cronologia. Potresti per favore dare un occhiata per vedere se adesso va bene? Non vorrei che si fosse perduta qualche recente tua modifica (alla fine le voci erano diventate 3 con le rispettive cronologie) -( Ciao e grazie. Lepido (msg)    \n",
       "3724                                                                                                                                                 Venerdì sera. tattica retorica abusata e piuttosto squallida il dire  sempre con questa storia? . L hai usata per giustificare il tuo essere inutilmente parolaio. La usi adesso su una faccenda nuova nuova. Quando crescerai?    JollyRoger    ۩ lo sceriffo    \n",
       "\n",
       "     lang  toxic  \n",
       "6034   it      0  \n",
       "3724   it      1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CpA5ibkQR6cn",
    "outputId": "dfd2a1f9-d403-43fb-edae-9ca8be5784dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <th>en</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.091333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">valid</th>\n",
       "      <th>es</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.106667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     toxic\n",
       "set   lang                \n",
       "train en    3000  0.091333\n",
       "valid es    2500  0.168800\n",
       "      it    2500  0.195200\n",
       "      tr    3000  0.106667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "57X-txA_raVA",
    "outputId": "3830e4ae-e5f9-4ab5-a689-050b25913c48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">test</th>\n",
       "      <th>es</th>\n",
       "      <td>8438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>10920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>8494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>11012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>10948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>14000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  toxic\n",
       "set  lang              \n",
       "test es     8438      0\n",
       "     fr    10920      0\n",
       "     it     8494      0\n",
       "     pt    11012      0\n",
       "     ru    10948      0\n",
       "     tr    14000      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE>0:\n",
    "    data = data.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)\n",
    "    test = test.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LLX9UbtR6ct"
   },
   "source": [
    "# Tokenizer, Config & Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gnjg9vXYR6cu"
   },
   "source": [
    "1. https://arxiv.org/pdf/1911.02116.pdf\n",
    "2. https://huggingface.co/transformers/model_doc/xlmroberta.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5Z0gl2gTR6cu"
   },
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'xlm':\n",
    "    xlmr_tok = transformers.XLMRobertaTokenizer.from_pretrained(MODEL_DIR)\n",
    "else:\n",
    "    xlmr_tok = transformers.DistilBertTokenizer.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4ufXbNlyR6cy",
    "outputId": "58b13fa9-0347-40fe-9809-5a61db9ce3e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODEL_DIR+\"special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "xlmr_tok.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "W6qX9qMMR6c0",
    "outputId": "c81308f3-9c5b-4723-b4fc-fd7efdd76c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = xlmr_tok.vocab_size\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtEkdI5cR6c6"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "v7zJrrJ7y2tL"
   },
   "outputs": [],
   "source": [
    "def sample_data(data, SAMPLE_SIZE, MAX_SEQ_LEN, valid_lang):\n",
    "    \n",
    "    data_valid = data.loc[data['lang'] == valid_lang]\n",
    "    data_valid = data_valid.copy()\n",
    "    data_valid = data_valid.reset_index(drop=True)\n",
    "    \n",
    "    data_train = data.loc[data['lang'] != valid_lang]\n",
    "    data_train = data_train.copy()\n",
    "    data_train = data_train.reset_index(drop=True)\n",
    "\n",
    "    X_tokens_train, X_att_train = [], []\n",
    "    for t in data_train.text.tolist():\n",
    "        encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "        X_tokens_train.append(encoded_text['input_ids'])\n",
    "        X_att_train.append(encoded_text['attention_mask'])\n",
    "\n",
    "    X_tokens_train, X_att_train = np.array(X_tokens_train), np.array(X_att_train)\n",
    "    Y_toxic_train = data_train['toxic'].values\n",
    "\n",
    "    X_tokens_valid, X_att_valid = [], []\n",
    "    for t in data_valid.text.tolist():\n",
    "        encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "        X_tokens_valid.append(encoded_text['input_ids'])\n",
    "        X_att_valid.append(encoded_text['attention_mask'])\n",
    "\n",
    "    X_tokens_valid, X_att_valid = np.array(X_tokens_valid), np.array(X_att_valid)\n",
    "    Y_toxic_valid = data_valid['toxic'].values\n",
    "\n",
    "    return X_tokens_train, X_att_train, Y_toxic_train, X_tokens_valid, X_att_valid, Y_toxic_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hSV2TFE7R6c-",
    "outputId": "61d8677f-b8cd-497d-e741-c79b026516de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t Training Sample\n",
      " (8000, 75) \t: X_tokens_train  \n",
      " (8000, 75) \t: X_att_train  \n",
      " (8000,) \t: Y_toxic_train  \n",
      " \n",
      " \t Validation Sample\n",
      " (3000, 75) \t: X_tokens_valid  \n",
      " (3000, 75) \t: X_att_valid  \n",
      " (3000,) \t: Y_toxic_valid  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens_train, X_att_train, Y_toxic_train, X_tokens_valid, X_att_valid, Y_toxic_valid = sample_data(data, SAMPLE_SIZE, MAX_SEQ_LEN, valid_lang='en')\n",
    "\n",
    "print(\"\\n \\t Training Sample\\n\",\n",
    "      X_tokens_train.shape, \"\\t: X_tokens_train \", \"\\n\",\n",
    "      X_att_train.shape, \"\\t: X_att_train \", \"\\n\",\n",
    "      Y_toxic_train.shape, \"\\t: Y_toxic_train \", \"\\n\",\n",
    "      \"\\n \\t Validation Sample\\n\",\n",
    "      X_tokens_valid.shape, \"\\t: X_tokens_valid \", \"\\n\",\n",
    "      X_att_valid.shape, \"\\t: X_att_valid \", \"\\n\",\n",
    "      Y_toxic_valid.shape, \"\\t: Y_toxic_valid \", \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "s_cU7WAAxHjS",
    "outputId": "66058960-fec0-46a7-ecb5-a75bb7cbb6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (63812, 75) \t: X_tokens_test  \n",
      " (63812, 75) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens_test, X_att_test = [], []\n",
    "for t in test.text.tolist():\n",
    "    encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "    X_tokens_test.append(encoded_text['input_ids'])\n",
    "    X_att_test.append(encoded_text['attention_mask'])\n",
    "\n",
    "X_tokens_test, X_att_test = np.array(X_tokens_test), np.array(X_att_test)\n",
    "\n",
    "print(\"\\n\",\n",
    "      X_tokens_test.shape, \"\\t: X_tokens_test \", \"\\n\",\n",
    "      X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeR6DmO2R6dN"
   },
   "source": [
    "# Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PKWwyFjzR6dN"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    \n",
    "    if MODEL_NAME == 'xlm':\n",
    "        config = transformers.XLMRobertaConfig.from_pretrained(MODEL_DIR)\n",
    "        model = transformers.TFXLMRobertaModel.from_pretrained(MODEL_DIR, config=config) # TFXLMRobertaForSequenceClassification\n",
    "        x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    else:\n",
    "        config = transformers.DistilBertConfig.from_pretrained(MODEL_DIR)\n",
    "        model = transformers.TFDistilBertModel.from_pretrained(MODEL_DIR) # TFDistilBertForSequenceClassification\n",
    "        x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    toxic_output = tf.keras.layers.Activation('sigmoid', name=\"toxic_output\")(x1)\n",
    "    \n",
    "    model = Model([input_att_flags, input_sequences],\n",
    "                  [toxic_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vNdeazmhR5jP"
   },
   "outputs": [],
   "source": [
    "# def build_model():\n",
    "#     input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "#     input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    \n",
    "#     if MODEL_NAME == 'xlm':\n",
    "#         config = transformers.XLMRobertaConfig.from_pretrained(MODEL_DIR)\n",
    "#         model = transformers.TFXLMRobertaForSequenceClassification.from_pretrained(MODEL_DIR, config=config)\n",
    "#         x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "#     else:\n",
    "#         config = transformers.DistilBertConfig.from_pretrained(MODEL_DIR)\n",
    "#         model = transformers.TFDistilBertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "#         x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    \n",
    "#     x1 = tf.keras.layers.Dense(1)(x[0])\n",
    "#     toxic_output = tf.keras.layers.Activation('sigmoid', name=\"toxic_output\")(x1)\n",
    "    \n",
    "#     model = Model([input_att_flags, input_sequences],\n",
    "#                   [toxic_output])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n-x9lurmR6dQ"
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ugXKFEnqR6dT",
    "outputId": "2c509dc7-da68-4adc-f9d6-9c9151b0fad8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB ((None, 75, 768),)   134734080   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 75, 768)      0           tf_distil_bert_model[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 75, 768)      1180416     dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 75, 768)      3072        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 75, 768)      0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 75, 1)        769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 75, 1)        4           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 75, 1)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 75)           0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            76          flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "toxic_output (Activation)       (None, 1)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 135,918,417\n",
      "Trainable params: 135,916,879\n",
      "Non-trainable params: 1,538\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeR6DmO2R6dN"
   },
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GGb6IwuR6dY"
   },
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.zeros((len(X_tokens_test), 1))\n",
    "timings_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "lpuHmXlWR6dq",
    "outputId": "249a868a-d0a0-482d-965d-e26bd49bff48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Fold: 0 Valid_Lang: en  ==================\n",
      "Train on 8000 samples, validate on 3000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 110s 14ms/sample - loss: 0.4381 - accuracy: 0.8522 - auc: 0.7802 - val_loss: 0.3516 - val_accuracy: 0.9227 - val_auc: 0.8562\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 101s 13ms/sample - loss: 0.3437 - accuracy: 0.9087 - auc: 0.9333 - val_loss: 0.3563 - val_accuracy: 0.9060 - val_auc: 0.8620\n",
      "Number of psuedo samples available: 36373\n",
      "Psuedo Toxicity: 1036\n",
      "Counter({'tr': 10258, 'pt': 6972, 'ru': 5052, 'fr': 4957, 'es': 4804, 'it': 4330})\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Train on 36373 samples, validate on 3000 samples\n",
      "Epoch 1/2\n",
      "36373/36373 [==============================] - 461s 13ms/sample - loss: 0.2008 - accuracy: 0.9993 - auc: 0.9988 - val_loss: 0.3772 - val_accuracy: 0.9127 - val_auc: 0.8327\n",
      "Epoch 2/2\n",
      "36373/36373 [==============================] - 457s 13ms/sample - loss: 0.2029 - accuracy: 0.9980 - auc: 0.9988 - val_loss: 0.4047 - val_accuracy: 0.9040 - val_auc: 0.8095\n",
      "Train ROC-AUC:\t 0.9936077986333778 \n",
      " Valid ROC-AUC:\t 0.862155989096615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      6770\n",
      "           1       0.93      0.87      0.90      1230\n",
      "\n",
      "    accuracy                           0.97      8000\n",
      "   macro avg       0.95      0.93      0.94      8000\n",
      "weighted avg       0.97      0.97      0.97      8000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      2726\n",
      "           1       0.48      0.46      0.47       274\n",
      "\n",
      "    accuracy                           0.91      3000\n",
      "   macro avg       0.72      0.71      0.71      3000\n",
      "weighted avg       0.90      0.91      0.90      3000\n",
      "\n",
      "================== Fold: 1 Valid_Lang: es  ==================\n",
      "Train on 8500 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "8500/8500 [==============================] - 125s 15ms/sample - loss: 0.4041 - accuracy: 0.8711 - auc: 0.8008 - val_loss: 0.4379 - val_accuracy: 0.8312 - val_auc: 0.8189\n",
      "Epoch 2/2\n",
      "8500/8500 [==============================] - 112s 13ms/sample - loss: 0.3274 - accuracy: 0.9114 - auc: 0.9444 - val_loss: 0.4496 - val_accuracy: 0.8472 - val_auc: 0.8015\n",
      "Number of psuedo samples available: 33293\n",
      "Psuedo Toxicity: 252\n",
      "Counter({'tr': 10061, 'pt': 6958, 'ru': 4455, 'fr': 4102, 'it': 3882, 'es': 3835})\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Train on 33293 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "33293/33293 [==============================] - 416s 13ms/sample - loss: 0.1999 - accuracy: 0.9996 - auc: 1.0000 - val_loss: 0.5819 - val_accuracy: 0.8472 - val_auc: 0.7019\n",
      "Epoch 2/2\n",
      "33293/33293 [==============================] - 415s 12ms/sample - loss: 0.2007 - accuracy: 0.9991 - auc: 0.9869 - val_loss: 0.7178 - val_accuracy: 0.8384 - val_auc: 0.7054\n",
      "Train ROC-AUC:\t 0.9427158996276728 \n",
      " Valid ROC-AUC:\t 0.8191685406583983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      7418\n",
      "           1       1.00      0.00      0.01      1082\n",
      "\n",
      "    accuracy                           0.87      8500\n",
      "   macro avg       0.94      0.50      0.47      8500\n",
      "weighted avg       0.89      0.87      0.81      8500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      2078\n",
      "           1       0.00      0.00      0.00       422\n",
      "\n",
      "    accuracy                           0.83      2500\n",
      "   macro avg       0.42      0.50      0.45      2500\n",
      "weighted avg       0.69      0.83      0.75      2500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Fold: 2 Valid_Lang: it  ==================\n",
      "Train on 8500 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "8500/8500 [==============================] - 126s 15ms/sample - loss: 0.3850 - accuracy: 0.8904 - auc: 0.8207 - val_loss: 0.4881 - val_accuracy: 0.7996 - val_auc: 0.7824\n",
      "Epoch 2/2\n",
      "8500/8500 [==============================] - 112s 13ms/sample - loss: 0.3002 - accuracy: 0.9367 - auc: 0.9573 - val_loss: 0.5092 - val_accuracy: 0.7816 - val_auc: 0.7714\n",
      "Number of psuedo samples available: 25157\n",
      "Psuedo Toxicity: 1643\n",
      "Counter({'tr': 10648, 'pt': 4355, 'ru': 3251, 'es': 2830, 'fr': 2304, 'it': 1769})\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Train on 25157 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "25157/25157 [==============================] - 319s 13ms/sample - loss: 0.2023 - accuracy: 0.9986 - auc: 0.9996 - val_loss: 0.6163 - val_accuracy: 0.7964 - val_auc: 0.7360\n",
      "Epoch 2/2\n",
      "25157/25157 [==============================] - 313s 12ms/sample - loss: 0.2020 - accuracy: 0.9984 - auc: 0.9989 - val_loss: 0.7030 - val_accuracy: 0.7732 - val_auc: 0.7489\n",
      "Train ROC-AUC:\t 0.9693660123223506 \n",
      " Valid ROC-AUC:\t 0.7822450542645765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      7484\n",
      "           1       0.87      0.64      0.74      1016\n",
      "\n",
      "    accuracy                           0.95      8500\n",
      "   macro avg       0.91      0.82      0.85      8500\n",
      "weighted avg       0.94      0.95      0.94      8500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      2012\n",
      "           1       0.48      0.30      0.37       488\n",
      "\n",
      "    accuracy                           0.80      2500\n",
      "   macro avg       0.66      0.61      0.62      2500\n",
      "weighted avg       0.77      0.80      0.78      2500\n",
      "\n",
      "================== Fold: 3 Valid_Lang: tr  ==================\n",
      "Train on 8000 samples, validate on 3000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 108s 14ms/sample - loss: 0.4272 - accuracy: 0.8566 - auc: 0.7968 - val_loss: 0.4080 - val_accuracy: 0.9013 - val_auc: 0.8570\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 99s 12ms/sample - loss: 0.3498 - accuracy: 0.9041 - auc: 0.9335 - val_loss: 0.3776 - val_accuracy: 0.8993 - val_auc: 0.8220\n",
      "Number of psuedo samples available: 41002\n",
      "Psuedo Toxicity: 72\n",
      "Counter({'tr': 9728, 'ru': 8188, 'pt': 7913, 'es': 5404, 'it': 5123, 'fr': 4646})\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Train on 41002 samples, validate on 3000 samples\n",
      "Epoch 1/2\n",
      "41002/41002 [==============================] - 513s 13ms/sample - loss: 0.2006 - accuracy: 0.9993 - auc: 0.9706 - val_loss: 0.4394 - val_accuracy: 0.8933 - val_auc: 0.6344\n",
      "Epoch 2/2\n",
      "41002/41002 [==============================] - 505s 12ms/sample - loss: 0.2014 - accuracy: 0.9987 - auc: 0.9373 - val_loss: 0.4638 - val_accuracy: 0.8933 - val_auc: 0.5664\n",
      "Train ROC-AUC:\t 0.940819395539906 \n",
      " Valid ROC-AUC:\t 0.8567257462686567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94      6816\n",
      "           1       0.67      0.73      0.70      1184\n",
      "\n",
      "    accuracy                           0.91      8000\n",
      "   macro avg       0.81      0.83      0.82      8000\n",
      "weighted avg       0.91      0.91      0.91      8000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95      2680\n",
      "           1       0.65      0.17      0.26       320\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.78      0.58      0.61      3000\n",
      "weighted avg       0.88      0.90      0.87      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, valid_lang in enumerate(data.lang.unique()):\n",
    "    print(\"================== Fold:\", num, \"Valid_Lang:\", valid_lang,\" ==================\")\n",
    "    start_time = time()\n",
    "    X_tokens_train, X_att_train, Y_toxic_train, X_tokens_valid, X_att_valid, Y_toxic_valid = sample_data(data,\n",
    "                                                                                                         SAMPLE_SIZE,\n",
    "                                                                                                         MAX_SEQ_LEN,\n",
    "                                                                                                         valid_lang=valid_lang)\n",
    "    \n",
    "    if num>0:\n",
    "        del model\n",
    "        del mcp\n",
    "        del csvl\n",
    "        del adam\n",
    "        del history\n",
    "        del auc\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "    model = build_model()\n",
    "    auc = tf.keras.metrics.AUC()\n",
    "    mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+valid_lang+\"_\"+str(num)+\".h5\", monitor='val_auc',\n",
    "                          verbose=0, save_best_only=True, save_weights_only=True, mode='max', save_freq='epoch')\n",
    "    csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+valid_lang+\"_\"+str(num)+\".csv\",\n",
    "                     separator=\",\", append=True)\n",
    "\n",
    "    model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=LR)\n",
    "    model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy', auc])\n",
    "    \n",
    "    train_time = time()\n",
    "    history = model.fit(x={\"att_flags\":X_att_train,\n",
    "                           \"words\":X_tokens_train},\n",
    "                        y={\"toxic_output\":Y_toxic_train},\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[0],\n",
    "                        validation_data=({\"att_flags\":X_att_valid,\n",
    "                                          \"words\":X_tokens_valid},\n",
    "                                         {\"toxic_output\":Y_toxic_valid}),\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    psuedo_time = time()\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_psuedo = model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                     \"words\":X_tokens_test},\n",
    "                                batch_size=PREDICT_BATCH_SIZE).reshape((-1))\n",
    "    \n",
    "    Y_toxic_psuedo = np.where(pred_psuedo >= PSUEDO_PROB_THRESH, 1, 0)\n",
    "    psuedo_flag = (pred_psuedo >= PSUEDO_PROB_THRESH_HIGH) | (pred_psuedo <= PSUEDO_PROB_THRESH_LOW)\n",
    "    \n",
    "    print(\"Number of psuedo samples available:\", sum(psuedo_flag))\n",
    "    print(\"Psuedo Toxicity:\", sum(Y_toxic_psuedo))\n",
    "    print(Counter(test.lang.values[psuedo_flag]))\n",
    "    print(Counter(test.lang.values))\n",
    "    \n",
    "    history = model.fit(x={\"att_flags\":X_att_test[psuedo_flag],\n",
    "                           \"words\":X_tokens_test[psuedo_flag]},\n",
    "                        y={\"toxic_output\":Y_toxic_psuedo[psuedo_flag]},\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[0],\n",
    "                        validation_data=({\"att_flags\":X_att_valid,\n",
    "                                          \"words\":X_tokens_valid},\n",
    "                                         {\"toxic_output\":Y_toxic_valid}),\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    infer_time = time()\n",
    "    \n",
    "    # Loading best weights per fold\n",
    "    model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+valid_lang+\"_\"+str(num)+\".h5\")\n",
    "\n",
    "    pred_train = model.predict(x = {\"att_flags\":X_att_train,\n",
    "                                    \"words\":X_tokens_train},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_valid = model.predict(x = {\"att_flags\":X_att_valid,\n",
    "                                    \"words\":X_tokens_valid},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    print(\"Train ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic_train, y_score=pred_train))\n",
    "    print(\"Valid ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic_valid, y_score=pred_valid))\n",
    "    \n",
    "    print(classification_report(y_true=Y_toxic_train, y_pred=np.where(pred_train>0.5,1,0)))\n",
    "    print(classification_report(y_true=Y_toxic_valid, y_pred=np.where(pred_valid>0.5,1,0)))\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test += model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                   \"words\":X_tokens_test},\n",
    "                              batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    end_time = time()\n",
    "    timings_dict.update({num:{\n",
    "        'start_time' : ctime(start_time),\n",
    "        'train_time' : ctime(train_time),\n",
    "        'infer_time' : ctime(infer_time),\n",
    "        'psuedo_time' : ctime(psuedo_time),\n",
    "        'end_time' : ctime(end_time),\n",
    "    }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start_time</th>\n",
       "      <td>Thu Jun 18 16:33:36 2020</td>\n",
       "      <td>Thu Jun 18 16:58:21 2020</td>\n",
       "      <td>Thu Jun 18 17:22:15 2020</td>\n",
       "      <td>Thu Jun 18 17:42:50 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>Thu Jun 18 16:33:59 2020</td>\n",
       "      <td>Thu Jun 18 16:58:51 2020</td>\n",
       "      <td>Thu Jun 18 17:22:45 2020</td>\n",
       "      <td>Thu Jun 18 17:43:19 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infer_time</th>\n",
       "      <td>Thu Jun 18 16:55:21 2020</td>\n",
       "      <td>Thu Jun 18 17:19:15 2020</td>\n",
       "      <td>Thu Jun 18 17:39:50 2020</td>\n",
       "      <td>Thu Jun 18 18:06:19 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psuedo_time</th>\n",
       "      <td>Thu Jun 18 16:37:30 2020</td>\n",
       "      <td>Thu Jun 18 17:02:48 2020</td>\n",
       "      <td>Thu Jun 18 17:26:43 2020</td>\n",
       "      <td>Thu Jun 18 17:46:47 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_time</th>\n",
       "      <td>Thu Jun 18 16:58:21 2020</td>\n",
       "      <td>Thu Jun 18 17:22:15 2020</td>\n",
       "      <td>Thu Jun 18 17:42:50 2020</td>\n",
       "      <td>Thu Jun 18 18:09:18 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0                         1  \\\n",
       "start_time   Thu Jun 18 16:33:36 2020  Thu Jun 18 16:58:21 2020   \n",
       "train_time   Thu Jun 18 16:33:59 2020  Thu Jun 18 16:58:51 2020   \n",
       "infer_time   Thu Jun 18 16:55:21 2020  Thu Jun 18 17:19:15 2020   \n",
       "psuedo_time  Thu Jun 18 16:37:30 2020  Thu Jun 18 17:02:48 2020   \n",
       "end_time     Thu Jun 18 16:58:21 2020  Thu Jun 18 17:22:15 2020   \n",
       "\n",
       "                                    2                         3  \n",
       "start_time   Thu Jun 18 17:22:15 2020  Thu Jun 18 17:42:50 2020  \n",
       "train_time   Thu Jun 18 17:22:45 2020  Thu Jun 18 17:43:19 2020  \n",
       "infer_time   Thu Jun 18 17:39:50 2020  Thu Jun 18 18:06:19 2020  \n",
       "psuedo_time  Thu Jun 18 17:26:43 2020  Thu Jun 18 17:46:47 2020  \n",
       "end_time     Thu Jun 18 17:42:50 2020  Thu Jun 18 18:09:18 2020  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(timings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['toxic'] = pred_test/float(data.lang.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','toxic']].to_csv(RESULTS_DIR+\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Mr6qXyMSR6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 18 18:09:18 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
