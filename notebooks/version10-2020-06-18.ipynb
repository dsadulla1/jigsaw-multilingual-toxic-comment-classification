{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yi9E6gwR6bw"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "attachments": {
    "186676f7-f9d2-4727-a904-889751b9f6b6.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:186676f7-f9d2-4727-a904-889751b9f6b6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PhXEkPQNR6bx"
   },
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V10\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "MODEL_NAME = 'distilbert' # options include 'xlm' or 'distilbert'\n",
    "\n",
    "NUM_EPOCHS = [4, 8, 2]\n",
    "LR = 5e-4\n",
    "MAX_SEQ_LEN = 75\n",
    "SAMPLE_SIZE = 6000\n",
    "PSUEDO_PROB_THRESH_LOW = 0.05\n",
    "PSUEDO_PROB_THRESH_HIGH = 0.7\n",
    "\n",
    "RUN_ON_SAMPLE = 0\n",
    "if RUN_ON_SAMPLE>0:\n",
    "    SAMPLE_SIZE = RUN_ON_SAMPLE\n",
    "\n",
    "ON_KAGGLE = False\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    BATCH_SIZE = 32\n",
    "    PREDICT_BATCH_SIZE = 512\n",
    "else:\n",
    "    BATCH_SIZE = 16\n",
    "    PREDICT_BATCH_SIZE = 256\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vBSH2FLlR6b1"
   },
   "outputs": [],
   "source": [
    "if ON_KAGGLE:\n",
    "    RESULTS_DIR = '../working/'\n",
    "    DATA_DIR = '../input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = '../input/tf-xlm-roberta-base/'\n",
    "    else:\n",
    "        MODEL_DIR = '../input/tf-distilbert-base-multilingual-cased/'\n",
    "else:\n",
    "    PATH = \"..\"\n",
    "    RESULTS_DIR = PATH+\"/results/\"\n",
    "    DATA_DIR = PATH+\"/data/\"\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = PATH+\"/models/tf-xlm-roberta-base/\"\n",
    "    else:\n",
    "        MODEL_DIR = PATH+\"/models/distilbert-base-multilingual-cased/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCnsk-7nR6b4"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oW_oT4SZR6b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold, LeaveOneGroupOut, cross_val_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.optimizers import TriangularCyclicalLearningRate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXbm0XVcR6b8"
   },
   "outputs": [],
   "source": [
    "seeded_value = 987258\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ybWWTMP8R6cA",
    "outputId": "c26e2379-92f7-4bcb-bd93-a07825d058f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 20 18:13:23 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ESteN4q4TAAW",
    "outputId": "a6428c7d-4959-486c-9c24-5dc489c8e04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR4HCYGTR6cG"
   },
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sJh7lbL1R6cH",
    "outputId": "68c74149-67dc-4a43-9d9d-8cec23e054d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iAdsW80fR6cL"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ8ZRVGTR6cO"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wQ_YumuXR6cO"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR+'jigsaw-toxic-comment-train.csv')\n",
    "validation = pd.read_csv(DATA_DIR+'validation.csv')\n",
    "test = pd.read_csv(DATA_DIR+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5Chd2u5AR6cR"
   },
   "outputs": [],
   "source": [
    "train['lang'] = 'en'\n",
    "\n",
    "train['set'] = 'train'\n",
    "validation['set'] = 'valid'\n",
    "test['set'] = 'test'\n",
    "\n",
    "test['toxic'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pZIaKjKXR6cU",
    "outputId": "f2b9d702-b64c-4838-bbcc-b92a243a8e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate', 'lang', 'set'],\n",
      "      dtype='object')\n",
      "Index(['id', 'comment_text', 'lang', 'toxic', 'set'], dtype='object')\n",
      "Index(['id', 'content', 'lang', 'set', 'toxic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(validation.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['es', 'it', 'tr'], dtype=object),\n",
       " array(['tr', 'ru', 'it', 'fr', 'pt', 'es'], dtype=object))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.lang.unique(), test.lang.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sNlk-kDDR6cY"
   },
   "outputs": [],
   "source": [
    "train.columns = ['id', 'text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'lang', 'set']\n",
    "validation.columns = ['id', 'text', 'lang', 'toxic', 'set']\n",
    "test.columns = ['id', 'text', 'lang', 'set', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "saymbsyYR6ca"
   },
   "outputs": [],
   "source": [
    "REQ_COLS = ['id', 'set', 'text', 'lang', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train[\"text\"].astype(str)\n",
    "validation['text'] = validation[\"text\"].astype(str)\n",
    "test['text'] = test[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2VCNIC95R6cf"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train[REQ_COLS].sample(SAMPLE_SIZE, random_state=seeded_value),\n",
    "                  validation[REQ_COLS]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-EPYDkCNR6ci",
    "outputId": "d36606e0-9c8d-4e2a-d2f8-82d4c9f15590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UlrDxcmLR6cl",
    "outputId": "7436086b-4453-44a1-d6be-627e2bd63171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>set</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164112</th>\n",
       "      <td>126f4ebe96739a72</td>\n",
       "      <td>train</td>\n",
       "      <td>\" \\n\\n *Support I have heard of the region before as Nakhichevan.    \\n *Support Nakhichevan is the common name of the area in English.   \"</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>4631</td>\n",
       "      <td>valid</td>\n",
       "      <td>¿Bastardilla?¿Pero no se decía cursiva?.Creo que sería mejor poner   cursiva  , en vez de   bastardilla  . Super</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id    set  \\\n",
       "164112  126f4ebe96739a72  train   \n",
       "4631                4631  valid   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "164112  \" \\n\\n *Support I have heard of the region before as Nakhichevan.    \\n *Support Nakhichevan is the common name of the area in English.   \"   \n",
       "4631                              ¿Bastardilla?¿Pero no se decía cursiva?.Creo que sería mejor poner   cursiva  , en vez de   bastardilla  . Super    \n",
       "\n",
       "       lang  toxic  \n",
       "164112   en      0  \n",
       "4631     es      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CpA5ibkQR6cn",
    "outputId": "dfd2a1f9-d403-43fb-edae-9ca8be5784dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <th>en</th>\n",
       "      <td>6000</td>\n",
       "      <td>0.092667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">valid</th>\n",
       "      <th>es</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.106667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     toxic\n",
       "set   lang                \n",
       "train en    6000  0.092667\n",
       "valid es    2500  0.168800\n",
       "      it    2500  0.195200\n",
       "      tr    3000  0.106667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "57X-txA_raVA",
    "outputId": "3830e4ae-e5f9-4ab5-a689-050b25913c48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">test</th>\n",
       "      <th>es</th>\n",
       "      <td>8438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>10920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>8494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>11012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>10948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>14000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  toxic\n",
       "set  lang              \n",
       "test es     8438      0\n",
       "     fr    10920      0\n",
       "     it     8494      0\n",
       "     pt    11012      0\n",
       "     ru    10948      0\n",
       "     tr    14000      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE>0:\n",
    "    data = data.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)\n",
    "    test = test.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LLX9UbtR6ct"
   },
   "source": [
    "# Tokenizer, Config & Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gnjg9vXYR6cu"
   },
   "source": [
    "1. https://arxiv.org/pdf/1911.02116.pdf\n",
    "2. https://huggingface.co/transformers/model_doc/xlmroberta.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5Z0gl2gTR6cu"
   },
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'xlm':\n",
    "    xlmr_tok = transformers.XLMRobertaTokenizer.from_pretrained(MODEL_DIR)\n",
    "else:\n",
    "    xlmr_tok = transformers.DistilBertTokenizer.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4ufXbNlyR6cy",
    "outputId": "58b13fa9-0347-40fe-9809-5a61db9ce3e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODEL_DIR+\"special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "xlmr_tok.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "W6qX9qMMR6c0",
    "outputId": "c81308f3-9c5b-4723-b4fc-fd7efdd76c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = xlmr_tok.vocab_size\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtEkdI5cR6c6"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens, X_att = [], []\n",
    "for t in data.text.tolist():\n",
    "    encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "    X_tokens.append(encoded_text['input_ids'])\n",
    "    X_att.append(encoded_text['attention_mask'])\n",
    "\n",
    "X_tokens, X_att, X_lang, Y_toxic = np.array(X_tokens), np.array(X_att), data['lang'].values, data['toxic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "s_cU7WAAxHjS",
    "outputId": "66058960-fec0-46a7-ecb5-a75bb7cbb6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (63812, 75) \t: X_tokens_test  \n",
      " (63812, 75) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens_test, X_att_test = [], []\n",
    "for t in test.text.tolist():\n",
    "    encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "    X_tokens_test.append(encoded_text['input_ids'])\n",
    "    X_att_test.append(encoded_text['attention_mask'])\n",
    "\n",
    "X_tokens_test, X_att_test, X_lang_test = np.array(X_tokens_test), np.array(X_att_test), test['lang'].values\n",
    "\n",
    "print(\"\\n\",\n",
    "      X_tokens_test.shape, \"\\t: X_tokens_test \", \"\\n\",\n",
    "      X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_DIR+\"ProcessedData.pickle\"):\n",
    "    with open(DATA_DIR+\"ProcessedData.pickle\", \"rb\") as f:\n",
    "        pickle.dump((X_tokens, X_att, X_lang, Y_toxic, X_tokens_test, X_att_test, X_lang_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_DIR+\"ProcessedData.pickle\"):\n",
    "    with open(DATA_DIR+\"ProcessedData.pickle\", \"rb\") as f:\n",
    "        X_tokens, X_att, X_lang, Y_toxic, X_tokens_test, X_att_test, X_lang_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeR6DmO2R6dN"
   },
   "source": [
    "# Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "PKWwyFjzR6dN"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    \n",
    "    if MODEL_NAME == 'xlm':\n",
    "        config = transformers.XLMRobertaConfig.from_pretrained(MODEL_DIR)\n",
    "        base_model = transformers.TFXLMRobertaModel.from_pretrained(MODEL_DIR, config=config) # TFXLMRobertaForSequenceClassification\n",
    "        x = base_model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    else:\n",
    "        config = transformers.DistilBertConfig.from_pretrained(MODEL_DIR)\n",
    "        base_model = transformers.TFDistilBertModel.from_pretrained(MODEL_DIR) # TFDistilBertForSequenceClassification\n",
    "        x = base_model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    toxic_output = tf.keras.layers.Activation('sigmoid', name=\"toxic_output\")(x1)\n",
    "    \n",
    "    model = Model([input_att_flags, input_sequences],\n",
    "                  [toxic_output])\n",
    "    \n",
    "    base_model = Model([input_att_flags, input_sequences],\n",
    "                       [x[0]])\n",
    "\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "n-x9lurmR6dQ"
   },
   "outputs": [],
   "source": [
    "model, base_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ugXKFEnqR6dT",
    "outputId": "2c509dc7-da68-4adc-f9d6-9c9151b0fad8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB ((None, 75, 768),)   134734080   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 75, 768)      0           tf_distil_bert_model[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 75, 768)      1180416     dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 75, 768)      3072        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 75, 768)      0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 75, 1)        769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 75, 1)        4           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 75, 1)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 75)           0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            76          flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "toxic_output (Activation)       (None, 1)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 135,918,417\n",
      "Trainable params: 135,916,879\n",
      "Non-trainable params: 1,538\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB ((None, 75, 768),)   134734080   words[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 134,734,080\n",
      "Trainable params: 134,734,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KFold Stratified train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUM_FOLDS = 3\n",
    "#skf = StratifiedKFold(NUM_FOLDS, shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one language out split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logo = LeaveOneGroupOut()\n",
    "#for t_index, v_index in logo.split(np.arange(X_tokens.shape[0]), np.arange(X_tokens.shape[0]), groups=X_lang):\n",
    "#    print(X_lang[t_index])\n",
    "#    #print(np.unique(X_lang[v_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple random train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_index, v_index = train_test_split(np.arange(X_tokens.shape[0]), shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeR6DmO2R6dN"
   },
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GGb6IwuR6dY"
   },
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros((len(X_tokens), 1))\n",
    "pred_test = np.zeros((len(X_tokens_test), 1))\n",
    "timings_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lpuHmXlWR6dq",
    "outputId": "249a868a-d0a0-482d-965d-e26bd49bff48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 10500 samples, validate on 3500 samples\n",
      "Epoch 1/4\n",
      "10500/10500 [==============================] - 67s 6ms/sample - loss: 0.4086 - accuracy: 0.8689 - auc: 0.7529 - val_loss: 0.3645 - val_accuracy: 0.8771 - val_auc: 0.8185\n",
      "Epoch 2/4\n",
      "10500/10500 [==============================] - 56s 5ms/sample - loss: 0.3645 - accuracy: 0.8836 - auc: 0.8380 - val_loss: 0.3504 - val_accuracy: 0.8811 - val_auc: 0.8524\n",
      "Epoch 3/4\n",
      "10500/10500 [==============================] - 56s 5ms/sample - loss: 0.3546 - accuracy: 0.8890 - auc: 0.8532 - val_loss: 0.3525 - val_accuracy: 0.8809 - val_auc: 0.8438\n",
      "Epoch 4/4\n",
      "10500/10500 [==============================] - 56s 5ms/sample - loss: 0.3478 - accuracy: 0.8926 - auc: 0.8622 - val_loss: 0.3373 - val_accuracy: 0.8894 - val_auc: 0.8616\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "valid_lang='all'\n",
    "start_time = time()\n",
    "\n",
    "if num>0:\n",
    "    del model\n",
    "    del mcp\n",
    "    del csvl\n",
    "    del adam\n",
    "    del history\n",
    "    del auc\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "\n",
    "model, base_model = build_model()\n",
    "auc = tf.keras.metrics.AUC()\n",
    "mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+valid_lang+\"_\"+str(num)+\".h5\", monitor='val_auc',\n",
    "                      verbose=0, save_best_only=True, save_weights_only=True, mode='max', save_freq='epoch')\n",
    "csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+valid_lang+\"_\"+str(num)+\".csv\",\n",
    "                 separator=\",\", append=True)\n",
    "\n",
    "model.layers[2].trainable = False\n",
    "adam = Adam(learning_rate=LR)\n",
    "model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy', auc])\n",
    "\n",
    "train_time = time()\n",
    "history = model.fit(x={\"att_flags\":X_att[t_index],\n",
    "                       \"words\":X_tokens[t_index]},\n",
    "                    y={\"toxic_output\":Y_toxic[t_index]},\n",
    "                    validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                      \"words\":X_tokens[v_index]},\n",
    "                                     {\"toxic_output\":Y_toxic[v_index]}),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS[0],\n",
    "                    shuffle=True,\n",
    "                    verbose=1, \n",
    "                    sample_weight=class_weight.compute_sample_weight('balanced',X_lang[t_index]),\n",
    "                    callbacks=[csvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC:\t 0.9013097266084913\n",
      "Valid ROC-AUC:\t 0.8617155282724736\n",
      "Train Accuracy:\t 0.9031428571428571\n",
      "Valid Accuracy:\t 0.8894285714285715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95      9176\n",
      "           1       0.69      0.42      0.52      1324\n",
      "\n",
      "    accuracy                           0.90     10500\n",
      "   macro avg       0.80      0.70      0.74     10500\n",
      "weighted avg       0.89      0.90      0.89     10500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      3038\n",
      "           1       0.63      0.39      0.48       462\n",
      "\n",
      "    accuracy                           0.89      3500\n",
      "   macro avg       0.77      0.68      0.71      3500\n",
      "weighted avg       0.88      0.89      0.88      3500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_initial = model.predict(x = {\"att_flags\":X_att,\n",
    "                                  \"words\":X_tokens},\n",
    "                             batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "print(\"Train ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic[t_index], y_score=pred_initial[t_index]))\n",
    "print(\"Valid ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic[v_index], y_score=pred_initial[v_index]))\n",
    "\n",
    "print(\"Train Accuracy:\\t\", accuracy_score(y_true=Y_toxic[t_index], y_pred=np.where(pred_initial[t_index]>0.5, 1, 0)))\n",
    "print(\"Valid Accuracy:\\t\", accuracy_score(y_true=Y_toxic[v_index], y_pred=np.where(pred_initial[v_index]>0.5, 1, 0)))\n",
    "\n",
    "print(classification_report(y_true=Y_toxic[t_index], y_pred=np.where(pred_initial[t_index]>0.5, 1, 0)))\n",
    "print(classification_report(y_true=Y_toxic[v_index], y_pred=np.where(pred_initial[v_index]>0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "lpuHmXlWR6dq",
    "outputId": "249a868a-d0a0-482d-965d-e26bd49bff48"
   },
   "outputs": [],
   "source": [
    "# Psuedo model fit\n",
    "psuedo_time = time()\n",
    "\n",
    "# Accumulate test results after training every fold\n",
    "pred_psuedo = model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                 \"words\":X_tokens_test},\n",
    "                            batch_size=PREDICT_BATCH_SIZE).reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['toxic_1'] = pred_psuedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    63812.000000\n",
       "mean         0.188803\n",
       "std          0.199487\n",
       "min          0.000529\n",
       "25%          0.038880\n",
       "50%          0.110128\n",
       "75%          0.275189\n",
       "max          0.949988\n",
       "Name: toxic_1, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['toxic_1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">test</th>\n",
       "      <th>es</th>\n",
       "      <td>8438</td>\n",
       "      <td>0.248041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>10920</td>\n",
       "      <td>0.308150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>8494</td>\n",
       "      <td>0.210303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>11012</td>\n",
       "      <td>0.120315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>10948</td>\n",
       "      <td>0.125336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>14000</td>\n",
       "      <td>0.150472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id   toxic_1\n",
       "set  lang                 \n",
       "test es     8438  0.248041\n",
       "     fr    10920  0.308150\n",
       "     it     8494  0.210303\n",
       "     pt    11012  0.120315\n",
       "     ru    10948  0.125336\n",
       "     tr    14000  0.150472"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic_1':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>toxic_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"48\" valign=\"top\">test</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">es</th>\n",
       "      <th>count</th>\n",
       "      <td>8438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.248041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.236975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.056279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.156498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.381731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.929942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">fr</th>\n",
       "      <th>count</th>\n",
       "      <td>10920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.308150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.218372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.261886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.455986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.949988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">it</th>\n",
       "      <th>count</th>\n",
       "      <td>8494.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.210303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.191497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.057510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.146646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.315850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.916670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">pt</th>\n",
       "      <th>count</th>\n",
       "      <td>11012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.120315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.163379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.017255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.051129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.149998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.902359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">ru</th>\n",
       "      <th>count</th>\n",
       "      <td>10948.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.125336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.140617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.031395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.072802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.164836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.880950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">tr</th>\n",
       "      <th>count</th>\n",
       "      <td>14000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.150472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.172732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.031579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.080276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.203653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.893340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      toxic_1\n",
       "set  lang                    \n",
       "test es   count   8438.000000\n",
       "          mean       0.248041\n",
       "          std        0.236975\n",
       "          min        0.000865\n",
       "          25%        0.056279\n",
       "          50%        0.156498\n",
       "          75%        0.381731\n",
       "          max        0.929942\n",
       "     fr   count  10920.000000\n",
       "          mean       0.308150\n",
       "          std        0.218372\n",
       "          min        0.002009\n",
       "          25%        0.124900\n",
       "          50%        0.261886\n",
       "          75%        0.455986\n",
       "          max        0.949988\n",
       "     it   count   8494.000000\n",
       "          mean       0.210303\n",
       "          std        0.191497\n",
       "          min        0.000968\n",
       "          25%        0.057510\n",
       "          50%        0.146646\n",
       "          75%        0.315850\n",
       "          max        0.916670\n",
       "     pt   count  11012.000000\n",
       "          mean       0.120315\n",
       "          std        0.163379\n",
       "          min        0.000529\n",
       "          25%        0.017255\n",
       "          50%        0.051129\n",
       "          75%        0.149998\n",
       "          max        0.902359\n",
       "     ru   count  10948.000000\n",
       "          mean       0.125336\n",
       "          std        0.140617\n",
       "          min        0.001007\n",
       "          25%        0.031395\n",
       "          50%        0.072802\n",
       "          75%        0.164836\n",
       "          max        0.880950\n",
       "     tr   count  14000.000000\n",
       "          mean       0.150472\n",
       "          std        0.172732\n",
       "          min        0.000715\n",
       "          25%        0.031579\n",
       "          50%        0.080276\n",
       "          75%        0.203653\n",
       "          max        0.893340"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"])[['toxic_1']].apply(pd.Series.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "lpuHmXlWR6dq",
    "outputId": "249a868a-d0a0-482d-965d-e26bd49bff48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of psuedo samples available: 21548\n",
      "Psuedo Toxicity: 2189\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'pt': 5658, 'tr': 5403, 'ru': 4298, 'es': 2593, 'it': 2087, 'fr': 1509})\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 32048 samples, validate on 3500 samples\n",
      "Epoch 1/8\n",
      "32048/32048 [==============================] - 120s 4ms/sample - loss: 0.2033 - accuracy: 0.9626 - auc: 0.9740 - val_loss: 0.3484 - val_accuracy: 0.8851 - val_auc: 0.8593\n",
      "Epoch 2/8\n",
      "32048/32048 [==============================] - 115s 4ms/sample - loss: 0.1982 - accuracy: 0.9638 - auc: 0.9754 - val_loss: 0.3456 - val_accuracy: 0.8857 - val_auc: 0.8700\n",
      "Epoch 3/8\n",
      "32048/32048 [==============================] - 116s 4ms/sample - loss: 0.1954 - accuracy: 0.9643 - auc: 0.9773 - val_loss: 0.3625 - val_accuracy: 0.8769 - val_auc: 0.8701\n",
      "Epoch 4/8\n",
      "32048/32048 [==============================] - 115s 4ms/sample - loss: 0.1932 - accuracy: 0.9646 - auc: 0.9793 - val_loss: 0.3495 - val_accuracy: 0.8840 - val_auc: 0.8752\n",
      "Epoch 5/8\n",
      "32048/32048 [==============================] - 115s 4ms/sample - loss: 0.1934 - accuracy: 0.9649 - auc: 0.9797 - val_loss: 0.3458 - val_accuracy: 0.8883 - val_auc: 0.8652\n",
      "Epoch 6/8\n",
      "32048/32048 [==============================] - 115s 4ms/sample - loss: 0.1907 - accuracy: 0.9661 - auc: 0.9810 - val_loss: 0.3436 - val_accuracy: 0.8920 - val_auc: 0.8702\n",
      "Epoch 7/8\n",
      "32048/32048 [==============================] - 114s 4ms/sample - loss: 0.1906 - accuracy: 0.9668 - auc: 0.9804 - val_loss: 0.3378 - val_accuracy: 0.8923 - val_auc: 0.8659\n",
      "Epoch 8/8\n",
      "32048/32048 [==============================] - 114s 4ms/sample - loss: 0.1885 - accuracy: 0.9666 - auc: 0.9823 - val_loss: 0.3415 - val_accuracy: 0.8926 - val_auc: 0.8771\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 32048 samples, validate on 3500 samples\n",
      "Epoch 1/2\n",
      "32048/32048 [==============================] - 377s 12ms/sample - loss: 0.1838 - accuracy: 0.9678 - auc: 0.9854 - val_loss: 0.3315 - val_accuracy: 0.8943 - val_auc: 0.8805\n",
      "Epoch 2/2\n",
      "32048/32048 [==============================] - 369s 12ms/sample - loss: 0.1807 - accuracy: 0.9694 - auc: 0.9873 - val_loss: 0.3300 - val_accuracy: 0.8934 - val_auc: 0.8847\n"
     ]
    }
   ],
   "source": [
    "Y_toxic_psuedo = np.where(pred_psuedo >= PSUEDO_PROB_THRESH_HIGH, 1, 0)\n",
    "psuedo_flag = (pred_psuedo >= PSUEDO_PROB_THRESH_HIGH) | (pred_psuedo <= PSUEDO_PROB_THRESH_LOW)\n",
    "\n",
    "print(\"Number of psuedo samples available:\", sum(psuedo_flag))\n",
    "print(\"Psuedo Toxicity:\", sum(Y_toxic_psuedo))\n",
    "print(Counter(test.lang.values))\n",
    "print(Counter(test.lang.values[psuedo_flag]))\n",
    "\n",
    "X_att_psuedo = np.concatenate((X_att[t_index], X_att_test[psuedo_flag]))\n",
    "X_tokens_psuedo = np.concatenate((X_tokens[t_index], X_tokens_test[psuedo_flag]))\n",
    "Y_toxic_psuedo = np.concatenate((Y_toxic[t_index], Y_toxic_psuedo[psuedo_flag]))\n",
    "X_lang_psuedo = np.concatenate((X_lang[t_index], X_lang_test[psuedo_flag]))\n",
    "\n",
    "shuffled_idxs = np.arange(Y_toxic_psuedo.shape[0])\n",
    "np.random.shuffle(shuffled_idxs)\n",
    "\n",
    "model.layers[2].trainable = False\n",
    "adam = Adam(learning_rate=LR)\n",
    "model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy', auc])\n",
    "\n",
    "history = model.fit(x={\"att_flags\":X_att_psuedo[shuffled_idxs],\n",
    "                       \"words\":X_tokens_psuedo[shuffled_idxs]},\n",
    "                    y={\"toxic_output\":Y_toxic_psuedo[shuffled_idxs]},\n",
    "                    validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                      \"words\":X_tokens[v_index]},\n",
    "                                     {\"toxic_output\":Y_toxic[v_index]}),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS[1],\n",
    "                    shuffle=True,\n",
    "                    verbose=1,\n",
    "                    callbacks=[mcp, csvl], \n",
    "                    sample_weight=class_weight.compute_sample_weight('balanced',X_lang_psuedo[shuffled_idxs]))\n",
    "\n",
    "if NUM_EPOCHS[2]>0:\n",
    "    model.layers[2].trainable = True\n",
    "    adam = Adam(learning_rate=LR*0.001)\n",
    "    model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy', auc])\n",
    "\n",
    "    history = model.fit(x={\"att_flags\":X_att_psuedo[shuffled_idxs],\n",
    "                           \"words\":X_tokens_psuedo[shuffled_idxs]},\n",
    "                        y={\"toxic_output\":Y_toxic_psuedo[shuffled_idxs]},\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X_tokens[v_index]},\n",
    "                                         {\"toxic_output\":Y_toxic[v_index]}),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[2],\n",
    "                        shuffle=True,\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl], \n",
    "                        sample_weight=class_weight.compute_sample_weight('balanced',X_lang_psuedo[shuffled_idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "lpuHmXlWR6dq",
    "outputId": "249a868a-d0a0-482d-965d-e26bd49bff48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC:\t 0.9379211037857855\n",
      "Valid ROC-AUC:\t 0.8848980731798375\n",
      "Train Accuracy:\t 0.9160952380952381\n",
      "Valid Accuracy:\t 0.8934285714285715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95      9176\n",
      "           1       0.72      0.55      0.62      1324\n",
      "\n",
      "    accuracy                           0.92     10500\n",
      "   macro avg       0.83      0.76      0.79     10500\n",
      "weighted avg       0.91      0.92      0.91     10500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      3038\n",
      "           1       0.62      0.49      0.55       462\n",
      "\n",
      "    accuracy                           0.89      3500\n",
      "   macro avg       0.77      0.72      0.74      3500\n",
      "weighted avg       0.89      0.89      0.89      3500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "infer_time = time()\n",
    "\n",
    "# Loading best weights per fold\n",
    "model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+valid_lang+\"_\"+str(num)+\".h5\")\n",
    "\n",
    "pred += model.predict(x = {\"att_flags\":X_att,\n",
    "                                \"words\":X_tokens},\n",
    "                           batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "pred_psuedo = model.predict(x = {\"att_flags\":X_att_psuedo,\n",
    "                                 \"words\":X_tokens_psuedo},\n",
    "                            batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "print(\"Train ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic[t_index], y_score=pred[t_index]))\n",
    "print(\"Valid ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic[v_index], y_score=pred[v_index]))\n",
    "\n",
    "print(\"Train Accuracy:\\t\", accuracy_score(y_true=Y_toxic[t_index], y_pred=np.where(pred[t_index]>0.5, 1, 0)))\n",
    "print(\"Valid Accuracy:\\t\", accuracy_score(y_true=Y_toxic[v_index], y_pred=np.where(pred[v_index]>0.5, 1, 0)))\n",
    "\n",
    "print(classification_report(y_true=Y_toxic[t_index], y_pred=np.where(pred[t_index]>0.5, 1, 0)))\n",
    "print(classification_report(y_true=Y_toxic[v_index], y_pred=np.where(pred[v_index]>0.5, 1, 0)))\n",
    "\n",
    "# Accumulate test results after training every fold\n",
    "\n",
    "pred_test += model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                \"words\":X_tokens_test},\n",
    "                           batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "end_time = time()\n",
    "timings_dict.update({num:{\n",
    "    'start_time' : ctime(start_time),\n",
    "    'train_time' : ctime(train_time),\n",
    "    'infer_time' : ctime(infer_time),\n",
    "    'psuedo_time' : ctime(psuedo_time),\n",
    "    'end_time' : ctime(end_time),\n",
    "}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>end_time</th>\n",
       "      <td>Sat Jun 20 18:55:06 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infer_time</th>\n",
       "      <td>Sat Jun 20 18:50:39 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psuedo_time</th>\n",
       "      <td>Sat Jun 20 18:20:18 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_time</th>\n",
       "      <td>Sat Jun 20 18:15:46 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>Sat Jun 20 18:15:49 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "end_time     Sat Jun 20 18:55:06 2020\n",
       "infer_time   Sat Jun 20 18:50:39 2020\n",
       "psuedo_time  Sat Jun 20 18:20:18 2020\n",
       "start_time   Sat Jun 20 18:15:46 2020\n",
       "train_time   Sat Jun 20 18:15:49 2020"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(timings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['toxic'] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','toxic']].to_csv(RESULTS_DIR+\"submission.csv\", index=False)\n",
    "test.to_csv(RESULTS_DIR+\"all_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    63812.000000\n",
       "mean         0.223970\n",
       "std          0.279298\n",
       "min          0.000910\n",
       "25%          0.030521\n",
       "50%          0.088178\n",
       "75%          0.309262\n",
       "max          0.999762\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['toxic'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">test</th>\n",
       "      <th>es</th>\n",
       "      <td>8438</td>\n",
       "      <td>0.293859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>10920</td>\n",
       "      <td>0.445883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>8494</td>\n",
       "      <td>0.238690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>11012</td>\n",
       "      <td>0.149391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>10948</td>\n",
       "      <td>0.122042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>14000</td>\n",
       "      <td>0.138193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     toxic\n",
       "set  lang                 \n",
       "test es     8438  0.293859\n",
       "     fr    10920  0.445883\n",
       "     it     8494  0.238690\n",
       "     pt    11012  0.149391\n",
       "     ru    10948  0.122042\n",
       "     tr    14000  0.138193"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>toxic_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"48\" valign=\"top\">test</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">es</th>\n",
       "      <th>count</th>\n",
       "      <td>8438.000000</td>\n",
       "      <td>8438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.293859</td>\n",
       "      <td>0.248041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.312769</td>\n",
       "      <td>0.236975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002746</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.051409</td>\n",
       "      <td>0.056279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.147622</td>\n",
       "      <td>0.156498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.455756</td>\n",
       "      <td>0.381731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.929942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">fr</th>\n",
       "      <th>count</th>\n",
       "      <td>10920.000000</td>\n",
       "      <td>10920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.445883</td>\n",
       "      <td>0.308150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.342267</td>\n",
       "      <td>0.218372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.002009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.111377</td>\n",
       "      <td>0.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.385216</td>\n",
       "      <td>0.261886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.784554</td>\n",
       "      <td>0.455986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.949988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">it</th>\n",
       "      <th>count</th>\n",
       "      <td>8494.000000</td>\n",
       "      <td>8494.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.238690</td>\n",
       "      <td>0.210303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.243598</td>\n",
       "      <td>0.191497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.054607</td>\n",
       "      <td>0.057510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.142983</td>\n",
       "      <td>0.146646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.348909</td>\n",
       "      <td>0.315850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.991881</td>\n",
       "      <td>0.916670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">pt</th>\n",
       "      <th>count</th>\n",
       "      <td>11012.000000</td>\n",
       "      <td>11012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.149391</td>\n",
       "      <td>0.120315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.226185</td>\n",
       "      <td>0.163379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.021393</td>\n",
       "      <td>0.017255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.046624</td>\n",
       "      <td>0.051129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.160955</td>\n",
       "      <td>0.149998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.994422</td>\n",
       "      <td>0.902359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">ru</th>\n",
       "      <th>count</th>\n",
       "      <td>10948.000000</td>\n",
       "      <td>10948.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.122042</td>\n",
       "      <td>0.125336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.179413</td>\n",
       "      <td>0.140617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.023257</td>\n",
       "      <td>0.031395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.050391</td>\n",
       "      <td>0.072802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.131782</td>\n",
       "      <td>0.164836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.987605</td>\n",
       "      <td>0.880950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">tr</th>\n",
       "      <th>count</th>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.138193</td>\n",
       "      <td>0.150472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.206122</td>\n",
       "      <td>0.172732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.000715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.020184</td>\n",
       "      <td>0.031579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.049187</td>\n",
       "      <td>0.080276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.150358</td>\n",
       "      <td>0.203653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.989695</td>\n",
       "      <td>0.893340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        toxic       toxic_1\n",
       "set  lang                                  \n",
       "test es   count   8438.000000   8438.000000\n",
       "          mean       0.293859      0.248041\n",
       "          std        0.312769      0.236975\n",
       "          min        0.002746      0.000865\n",
       "          25%        0.051409      0.056279\n",
       "          50%        0.147622      0.156498\n",
       "          75%        0.455756      0.381731\n",
       "          max        0.997260      0.929942\n",
       "     fr   count  10920.000000  10920.000000\n",
       "          mean       0.445883      0.308150\n",
       "          std        0.342267      0.218372\n",
       "          min        0.002208      0.002009\n",
       "          25%        0.111377      0.124900\n",
       "          50%        0.385216      0.261886\n",
       "          75%        0.784554      0.455986\n",
       "          max        0.999762      0.949988\n",
       "     it   count   8494.000000   8494.000000\n",
       "          mean       0.238690      0.210303\n",
       "          std        0.243598      0.191497\n",
       "          min        0.001306      0.000968\n",
       "          25%        0.054607      0.057510\n",
       "          50%        0.142983      0.146646\n",
       "          75%        0.348909      0.315850\n",
       "          max        0.991881      0.916670\n",
       "     pt   count  11012.000000  11012.000000\n",
       "          mean       0.149391      0.120315\n",
       "          std        0.226185      0.163379\n",
       "          min        0.001138      0.000529\n",
       "          25%        0.021393      0.017255\n",
       "          50%        0.046624      0.051129\n",
       "          75%        0.160955      0.149998\n",
       "          max        0.994422      0.902359\n",
       "     ru   count  10948.000000  10948.000000\n",
       "          mean       0.122042      0.125336\n",
       "          std        0.179413      0.140617\n",
       "          min        0.001692      0.001007\n",
       "          25%        0.023257      0.031395\n",
       "          50%        0.050391      0.072802\n",
       "          75%        0.131782      0.164836\n",
       "          max        0.987605      0.880950\n",
       "     tr   count  14000.000000  14000.000000\n",
       "          mean       0.138193      0.150472\n",
       "          std        0.206122      0.172732\n",
       "          min        0.000910      0.000715\n",
       "          25%        0.020184      0.031579\n",
       "          50%        0.049187      0.080276\n",
       "          75%        0.150358      0.203653\n",
       "          max        0.989695      0.893340"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"])[['toxic', 'toxic_1']].apply(pd.Series.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Mr6qXyMSR6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 20 18:55:07 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Gain using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = base_model.predict(x={\"att_flags\":X_att_psuedo,\n",
    "                                 \"words\":X_tokens_psuedo},\n",
    "                              batch_size=PREDICT_BATCH_SIZE).mean(axis=2)\n",
    "features = np.concatenate((features, pred_psuedo), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_valid = base_model.predict(x={\"att_flags\":X_att[v_index],\n",
    "                                       \"words\":X_tokens[v_index]},\n",
    "                              batch_size=PREDICT_BATCH_SIZE).mean(axis=2)\n",
    "features_valid = np.concatenate((features_valid, pred[v_index]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = base_model.predict(x={\"att_flags\":X_att_test,\n",
    "                                      \"words\":X_tokens_test},\n",
    "                                   batch_size=PREDICT_BATCH_SIZE).mean(axis=2)\n",
    "features_test = np.concatenate((features_test, pred_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TREES = 10\n",
    "NUM_CLASSES = 2\n",
    "NUM_EVALS = 25\n",
    "NUM_FOLDS=3\n",
    "\n",
    "auc_scorer = make_scorer(roc_auc_score, greater_is_better=True, needs_proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "    }\n",
    "\n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=NUM_TREES,\n",
    "        verbosity=1,\n",
    "        objective=\"binary:logistic\",\n",
    "        random_state=seeded_value, \n",
    "        booster='gbtree',\n",
    "        n_jobs=12,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    #score = cross_val_score(estimator=clf,\n",
    "    #                        X=features,\n",
    "    #                        y=Y_toxic_psuedo,\n",
    "    #                        scoring=auc_scorer,\n",
    "    #                        cv=StratifiedKFold(n_splits=NUM_FOLDS,\n",
    "    #                                           shuffle=True,\n",
    "    #                                           random_state=seeded_value)).mean()\n",
    "    \n",
    "    clf.fit(X=features, y=Y_toxic_psuedo)\n",
    "    preds = clf.predict(features_valid)\n",
    "    score = roc_auc_score(y_true=Y_toxic[v_index], y_score=preds)\n",
    "    \n",
    "    print(\"AUC Score {:.3f} params {}\".format(score, params))\n",
    "    return -1.0*score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score 0.681 params {'max_depth': 6, 'gamma': '0.181', 'colsample_bytree': '0.648'}                                                                     \n",
      "AUC Score 0.555 params {'max_depth': 7, 'gamma': '0.101', 'colsample_bytree': '0.489'}                                                                     \n",
      "AUC Score 0.714 params {'max_depth': 6, 'gamma': '0.379', 'colsample_bytree': '0.772'}                                                                     \n",
      "AUC Score 0.712 params {'max_depth': 7, 'gamma': '0.287', 'colsample_bytree': '0.817'}                                                                     \n",
      "AUC Score 0.554 params {'max_depth': 7, 'gamma': '0.490', 'colsample_bytree': '0.464'}                                                                     \n",
      "AUC Score 0.568 params {'max_depth': 6, 'gamma': '0.342', 'colsample_bytree': '0.514'}                                                                     \n",
      "AUC Score 0.570 params {'max_depth': 4, 'gamma': '0.208', 'colsample_bytree': '0.484'}                                                                     \n",
      "AUC Score 0.718 params {'max_depth': 3, 'gamma': '0.006', 'colsample_bytree': '0.866'}                                                                     \n",
      "AUC Score 0.575 params {'max_depth': 4, 'gamma': '0.058', 'colsample_bytree': '0.625'}                                                                     \n",
      "AUC Score 0.581 params {'max_depth': 3, 'gamma': '0.322', 'colsample_bytree': '0.618'}                                                                     \n",
      "AUC Score 0.678 params {'max_depth': 4, 'gamma': '0.183', 'colsample_bytree': '0.669'}                                                                     \n",
      "AUC Score 0.712 params {'max_depth': 5, 'gamma': '0.176', 'colsample_bytree': '0.774'}                                                                     \n",
      "AUC Score 0.552 params {'max_depth': 6, 'gamma': '0.326', 'colsample_bytree': '0.455'}                                                                     \n",
      "AUC Score 0.575 params {'max_depth': 4, 'gamma': '0.211', 'colsample_bytree': '0.629'}                                                                     \n",
      "AUC Score 0.561 params {'max_depth': 3, 'gamma': '0.149', 'colsample_bytree': '0.449'}                                                                     \n",
      "AUC Score 0.675 params {'max_depth': 3, 'gamma': '0.052', 'colsample_bytree': '0.692'}                                                                     \n",
      "AUC Score 0.567 params {'max_depth': 4, 'gamma': '0.347', 'colsample_bytree': '0.435'}                                                                     \n",
      "AUC Score 0.721 params {'max_depth': 3, 'gamma': '0.310', 'colsample_bytree': '0.979'}                                                                     \n",
      "AUC Score 0.572 params {'max_depth': 4, 'gamma': '0.246', 'colsample_bytree': '0.579'}                                                                     \n",
      "AUC Score 0.676 params {'max_depth': 7, 'gamma': '0.354', 'colsample_bytree': '0.718'}                                                                     \n",
      "AUC Score 0.730 params {'max_depth': 2, 'gamma': '0.423', 'colsample_bytree': '0.973'}                                                                     \n",
      "AUC Score 0.730 params {'max_depth': 2, 'gamma': '0.444', 'colsample_bytree': '0.993'}                                                                     \n",
      "AUC Score 0.730 params {'max_depth': 2, 'gamma': '0.496', 'colsample_bytree': '0.996'}                                                                     \n",
      "AUC Score 0.556 params {'max_depth': 2, 'gamma': '0.496', 'colsample_bytree': '0.307'}                                                                     \n",
      "AUC Score 0.723 params {'max_depth': 2, 'gamma': '0.410', 'colsample_bytree': '0.919'}                                                                     \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:20<00:00,  1.25trial/s, best loss: -0.7299010513296228]\n",
      "***Best Parameters: {'max_depth': 2, 'gamma': '0.423', 'colsample_bytree': '0.973'} ***\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=NUM_EVALS)\n",
    "\n",
    "optimal_params = {\n",
    "        'max_depth': int(best['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(best['gamma']),\n",
    "        'colsample_bytree': '{:.3f}'.format(best['colsample_bytree'])\n",
    "}\n",
    "\n",
    "print(\"***Best Parameters:\", optimal_params, \"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(**{\n",
    "    \"n_estimators\":NUM_TREES,\n",
    "    \"verbosity\":1,\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"random_state\":seeded_value,\n",
    "    'booster':'gbtree',\n",
    "    \"n_jobs\":12,\n",
    "    **optimal_params\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree='0.973', gamma='0.423',\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=10, n_jobs=12, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=987258, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X=features, y=Y_toxic_psuedo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = xgb_model.predict(features)\n",
    "pred_xgb_valid = xgb_model.predict(features_valid)\n",
    "pred_xgb_test = xgb_model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psued ROC-AUC:\t 0.9153086253860664\n",
      "Valid ROC-AUC:\t 0.7299010513296228\n",
      "Psued Accuracy:\t 0.9729780329505742\n",
      "Valid Accuracy:\t 0.8911428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     28535\n",
      "           1       0.91      0.84      0.87      3513\n",
      "\n",
      "    accuracy                           0.97     32048\n",
      "   macro avg       0.94      0.92      0.93     32048\n",
      "weighted avg       0.97      0.97      0.97     32048\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      3038\n",
      "           1       0.60      0.51      0.55       462\n",
      "\n",
      "    accuracy                           0.89      3500\n",
      "   macro avg       0.77      0.73      0.75      3500\n",
      "weighted avg       0.88      0.89      0.89      3500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Psued ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic_psuedo, y_score=pred_xgb))\n",
    "print(\"Valid ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic[v_index], y_score=pred_xgb_valid))\n",
    "\n",
    "print(\"Psued Accuracy:\\t\", accuracy_score(y_true=Y_toxic_psuedo, y_pred=np.where(pred_xgb>0.5, 1, 0)))\n",
    "print(\"Valid Accuracy:\\t\", accuracy_score(y_true=Y_toxic[v_index], y_pred=np.where(pred_xgb_valid>0.5, 1, 0)))\n",
    "\n",
    "print(classification_report(y_true=Y_toxic_psuedo, y_pred=np.where(pred_xgb>0.5, 1, 0)))\n",
    "print(classification_report(y_true=Y_toxic[v_index], y_pred=np.where(pred_xgb_valid>0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL_0': 0.0,\n",
       " 'MODEL_1': 0.0,\n",
       " 'MODEL_2': 0.0,\n",
       " 'MODEL_3': 0.0,\n",
       " 'MODEL_4': 0.0,\n",
       " 'MODEL_5': 0.0,\n",
       " 'MODEL_6': 0.0,\n",
       " 'MODEL_7': 0.0,\n",
       " 'MODEL_8': 0.0,\n",
       " 'MODEL_9': 0.0,\n",
       " 'MODEL_10': 0.0,\n",
       " 'MODEL_11': 0.0,\n",
       " 'MODEL_12': 0.0,\n",
       " 'MODEL_13': 0.0,\n",
       " 'MODEL_14': 0.0,\n",
       " 'MODEL_15': 0.0,\n",
       " 'MODEL_16': 0.0,\n",
       " 'MODEL_17': 0.0,\n",
       " 'MODEL_18': 0.0,\n",
       " 'MODEL_19': 0.0,\n",
       " 'MODEL_20': 0.0,\n",
       " 'MODEL_21': 0.0,\n",
       " 'MODEL_22': 0.0,\n",
       " 'MODEL_23': 0.0,\n",
       " 'MODEL_24': 0.0,\n",
       " 'MODEL_25': 0.0,\n",
       " 'MODEL_26': 0.0,\n",
       " 'MODEL_27': 0.0,\n",
       " 'MODEL_28': 0.0,\n",
       " 'MODEL_29': 0.0,\n",
       " 'MODEL_30': 0.0,\n",
       " 'MODEL_31': 0.0,\n",
       " 'MODEL_32': 0.0,\n",
       " 'MODEL_33': 0.0,\n",
       " 'MODEL_34': 0.0,\n",
       " 'MODEL_35': 0.0,\n",
       " 'MODEL_36': 0.0,\n",
       " 'MODEL_37': 0.0,\n",
       " 'MODEL_38': 0.0,\n",
       " 'MODEL_39': 0.0,\n",
       " 'MODEL_40': 0.0,\n",
       " 'MODEL_41': 0.0,\n",
       " 'MODEL_42': 0.0,\n",
       " 'MODEL_43': 0.0,\n",
       " 'MODEL_44': 0.0,\n",
       " 'MODEL_45': 0.0,\n",
       " 'MODEL_46': 0.0,\n",
       " 'MODEL_47': 0.0,\n",
       " 'MODEL_48': 0.0,\n",
       " 'MODEL_49': 0.0,\n",
       " 'MODEL_50': 0.0,\n",
       " 'MODEL_51': 0.0,\n",
       " 'MODEL_52': 0.0,\n",
       " 'MODEL_53': 0.0,\n",
       " 'MODEL_54': 0.0,\n",
       " 'MODEL_55': 0.0,\n",
       " 'MODEL_56': 0.0,\n",
       " 'MODEL_57': 0.0,\n",
       " 'MODEL_58': 0.0,\n",
       " 'MODEL_59': 0.0,\n",
       " 'MODEL_60': 0.0,\n",
       " 'MODEL_61': 0.0,\n",
       " 'MODEL_62': 0.0,\n",
       " 'MODEL_63': 0.0,\n",
       " 'MODEL_64': 0.0,\n",
       " 'MODEL_65': 0.0,\n",
       " 'MODEL_66': 0.0,\n",
       " 'MODEL_67': 0.0,\n",
       " 'MODEL_68': 0.0,\n",
       " 'MODEL_69': 0.0,\n",
       " 'MODEL_70': 0.0,\n",
       " 'MODEL_71': 0.0,\n",
       " 'MODEL_72': 0.0,\n",
       " 'MODEL_73': 0.0,\n",
       " 'MODEL_74': 0.0,\n",
       " 'PsuedoScore': 1.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{l:k for k,l in zip(xgb_model.feature_importances_, ['MODEL_'+str(i) for i in range(MAX_SEQ_LEN)]+['PsuedoScore'])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
