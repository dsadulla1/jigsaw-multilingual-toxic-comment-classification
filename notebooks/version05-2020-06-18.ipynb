{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yi9E6gwR6bw"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "attachments": {
    "186676f7-f9d2-4727-a904-889751b9f6b6.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:186676f7-f9d2-4727-a904-889751b9f6b6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PhXEkPQNR6bx"
   },
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V05\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "MODEL_NAME = 'distilbert' # options include 'xlm' or 'distilbert'\n",
    "\n",
    "NUM_EPOCHS = [2]\n",
    "LR = 5e-5\n",
    "MAX_SEQ_LEN = 75\n",
    "SAMPLE_SIZE = 3000\n",
    "PSUEDO_PROB_THRESH_LOW = 0.05\n",
    "PSUEDO_PROB_THRESH_HIGH = 0.65\n",
    "\n",
    "RUN_ON_SAMPLE = 0\n",
    "ON_KAGGLE = False\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    BATCH_SIZE = 64\n",
    "    PREDICT_BATCH_SIZE = 1024\n",
    "else:\n",
    "    BATCH_SIZE = 16\n",
    "    PREDICT_BATCH_SIZE = 256\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vBSH2FLlR6b1"
   },
   "outputs": [],
   "source": [
    "if ON_KAGGLE:\n",
    "    RESULTS_DIR = '../working/'\n",
    "    DATA_DIR = '../input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = '../input/tf-xlm-roberta-base/'\n",
    "    else:\n",
    "        MODEL_DIR = '../input/tf-distilbert-base-multilingual-cased/'\n",
    "else:\n",
    "    PATH = \"..\" #\"/content/drive/My Drive/Kaggle/jigsaw-multilingual-toxic-comment-classification\"\n",
    "    RESULTS_DIR = PATH+\"/results/\"\n",
    "    DATA_DIR = PATH+\"/data/\"\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = PATH+\"/models/tf-xlm-roberta-base/\"\n",
    "    else:\n",
    "        MODEL_DIR = PATH+\"/models/distilbert-base-multilingual-cased/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCnsk-7nR6b4"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oW_oT4SZR6b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.optimizers import TriangularCyclicalLearningRate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXbm0XVcR6b8"
   },
   "outputs": [],
   "source": [
    "seeded_value = 654123\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ybWWTMP8R6cA",
    "outputId": "c26e2379-92f7-4bcb-bd93-a07825d058f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 18 19:36:04 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ESteN4q4TAAW",
    "outputId": "a6428c7d-4959-486c-9c24-5dc489c8e04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR4HCYGTR6cG"
   },
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sJh7lbL1R6cH",
    "outputId": "68c74149-67dc-4a43-9d9d-8cec23e054d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iAdsW80fR6cL"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ8ZRVGTR6cO"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wQ_YumuXR6cO"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR+'jigsaw-toxic-comment-train.csv')\n",
    "validation = pd.read_csv(DATA_DIR+'validation.csv')\n",
    "test = pd.read_csv(DATA_DIR+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5Chd2u5AR6cR"
   },
   "outputs": [],
   "source": [
    "train['lang'] = 'en'\n",
    "\n",
    "train['set'] = 'train'\n",
    "validation['set'] = 'valid'\n",
    "test['set'] = 'test'\n",
    "\n",
    "test['toxic'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pZIaKjKXR6cU",
    "outputId": "f2b9d702-b64c-4838-bbcc-b92a243a8e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate', 'lang', 'set'],\n",
      "      dtype='object')\n",
      "Index(['id', 'comment_text', 'lang', 'toxic', 'set'], dtype='object')\n",
      "Index(['id', 'content', 'lang', 'set', 'toxic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(validation.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sNlk-kDDR6cY"
   },
   "outputs": [],
   "source": [
    "train.columns = ['id', 'text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'lang', 'set']\n",
    "validation.columns = ['id', 'text', 'lang', 'toxic', 'set']\n",
    "test.columns = ['id', 'text', 'lang', 'set', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "saymbsyYR6ca"
   },
   "outputs": [],
   "source": [
    "REQ_COLS = ['id', 'set', 'text', 'lang', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train[\"text\"].astype(str)\n",
    "validation['text'] = validation[\"text\"].astype(str)\n",
    "test['text'] = test[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2VCNIC95R6cf"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train[REQ_COLS].sample(SAMPLE_SIZE, random_state=seeded_value),\n",
    "                  validation[REQ_COLS]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-EPYDkCNR6ci",
    "outputId": "d36606e0-9c8d-4e2a-d2f8-82d4c9f15590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UlrDxcmLR6cl",
    "outputId": "7436086b-4453-44a1-d6be-627e2bd63171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>set</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205996</th>\n",
       "      <td>b8e77c1e7bc44922</td>\n",
       "      <td>train</td>\n",
       "      <td>:::Please read my comments at the top of this page. Bye.</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>5397</td>\n",
       "      <td>valid</td>\n",
       "      <td>Sağol ama bu değil. Geçmişte maddenin benim tarafından açılıp ilk önce 50 civarında katkı yaptığım gözükmesi gerekirdi:-/ Neyse boşver. selamlar    Erdall</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id    set  \\\n",
       "205996  b8e77c1e7bc44922  train   \n",
       "5397                5397  valid   \n",
       "\n",
       "                                                                                                                                                                  text  \\\n",
       "205996                                                                                                        :::Please read my comments at the top of this page. Bye.   \n",
       "5397    Sağol ama bu değil. Geçmişte maddenin benim tarafından açılıp ilk önce 50 civarında katkı yaptığım gözükmesi gerekirdi:-/ Neyse boşver. selamlar    Erdall       \n",
       "\n",
       "       lang  toxic  \n",
       "205996   en      0  \n",
       "5397     tr      0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CpA5ibkQR6cn",
    "outputId": "dfd2a1f9-d403-43fb-edae-9ca8be5784dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <th>en</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">valid</th>\n",
       "      <th>es</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.106667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     toxic\n",
       "set   lang                \n",
       "train en    3000  0.098000\n",
       "valid es    2500  0.168800\n",
       "      it    2500  0.195200\n",
       "      tr    3000  0.106667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "57X-txA_raVA",
    "outputId": "3830e4ae-e5f9-4ab5-a689-050b25913c48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">test</th>\n",
       "      <th>es</th>\n",
       "      <td>8438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>10920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>8494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>11012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>10948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>14000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  toxic\n",
       "set  lang              \n",
       "test es     8438      0\n",
       "     fr    10920      0\n",
       "     it     8494      0\n",
       "     pt    11012      0\n",
       "     ru    10948      0\n",
       "     tr    14000      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE>0:\n",
    "    data = data.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)\n",
    "    test = test.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LLX9UbtR6ct"
   },
   "source": [
    "# Tokenizer, Config & Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gnjg9vXYR6cu"
   },
   "source": [
    "1. https://arxiv.org/pdf/1911.02116.pdf\n",
    "2. https://huggingface.co/transformers/model_doc/xlmroberta.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5Z0gl2gTR6cu"
   },
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'xlm':\n",
    "    xlmr_tok = transformers.XLMRobertaTokenizer.from_pretrained(MODEL_DIR)\n",
    "else:\n",
    "    xlmr_tok = transformers.DistilBertTokenizer.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4ufXbNlyR6cy",
    "outputId": "58b13fa9-0347-40fe-9809-5a61db9ce3e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODEL_DIR+\"special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "xlmr_tok.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "W6qX9qMMR6c0",
    "outputId": "c81308f3-9c5b-4723-b4fc-fd7efdd76c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = xlmr_tok.vocab_size\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtEkdI5cR6c6"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "v7zJrrJ7y2tL"
   },
   "outputs": [],
   "source": [
    "def sample_data(data, SAMPLE_SIZE, MAX_SEQ_LEN, valid_lang):\n",
    "    \n",
    "    data_valid = data.loc[data['lang'] == valid_lang]\n",
    "    data_valid = data_valid.copy()\n",
    "    data_valid = data_valid.reset_index(drop=True)\n",
    "    \n",
    "    data_train = data.loc[data['lang'] != valid_lang]\n",
    "    data_train = data_train.copy()\n",
    "    data_train = data_train.reset_index(drop=True)\n",
    "\n",
    "    X_tokens_train, X_att_train = [], []\n",
    "    for t in data_train.text.tolist():\n",
    "        encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "        X_tokens_train.append(encoded_text['input_ids'])\n",
    "        X_att_train.append(encoded_text['attention_mask'])\n",
    "\n",
    "    X_tokens_train, X_att_train = np.array(X_tokens_train), np.array(X_att_train)\n",
    "    Y_toxic_train = data_train['toxic'].values\n",
    "\n",
    "    X_tokens_valid, X_att_valid = [], []\n",
    "    for t in data_valid.text.tolist():\n",
    "        encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "        X_tokens_valid.append(encoded_text['input_ids'])\n",
    "        X_att_valid.append(encoded_text['attention_mask'])\n",
    "\n",
    "    X_tokens_valid, X_att_valid = np.array(X_tokens_valid), np.array(X_att_valid)\n",
    "    Y_toxic_valid = data_valid['toxic'].values\n",
    "\n",
    "    return X_tokens_train, X_att_train, Y_toxic_train, X_tokens_valid, X_att_valid, Y_toxic_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hSV2TFE7R6c-",
    "outputId": "61d8677f-b8cd-497d-e741-c79b026516de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t Training Sample\n",
      " (8000, 75) \t: X_tokens_train  \n",
      " (8000, 75) \t: X_att_train  \n",
      " (8000,) \t: Y_toxic_train  \n",
      " \n",
      " \t Validation Sample\n",
      " (3000, 75) \t: X_tokens_valid  \n",
      " (3000, 75) \t: X_att_valid  \n",
      " (3000,) \t: Y_toxic_valid  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens_train, X_att_train, Y_toxic_train, X_tokens_valid, X_att_valid, Y_toxic_valid = sample_data(data, SAMPLE_SIZE, MAX_SEQ_LEN, valid_lang='en')\n",
    "\n",
    "print(\"\\n \\t Training Sample\\n\",\n",
    "      X_tokens_train.shape, \"\\t: X_tokens_train \", \"\\n\",\n",
    "      X_att_train.shape, \"\\t: X_att_train \", \"\\n\",\n",
    "      Y_toxic_train.shape, \"\\t: Y_toxic_train \", \"\\n\",\n",
    "      \"\\n \\t Validation Sample\\n\",\n",
    "      X_tokens_valid.shape, \"\\t: X_tokens_valid \", \"\\n\",\n",
    "      X_att_valid.shape, \"\\t: X_att_valid \", \"\\n\",\n",
    "      Y_toxic_valid.shape, \"\\t: Y_toxic_valid \", \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "s_cU7WAAxHjS",
    "outputId": "66058960-fec0-46a7-ecb5-a75bb7cbb6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (63812, 75) \t: X_tokens_test  \n",
      " (63812, 75) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens_test, X_att_test = [], []\n",
    "for t in test.text.tolist():\n",
    "    encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "    X_tokens_test.append(encoded_text['input_ids'])\n",
    "    X_att_test.append(encoded_text['attention_mask'])\n",
    "\n",
    "X_tokens_test, X_att_test = np.array(X_tokens_test), np.array(X_att_test)\n",
    "\n",
    "print(\"\\n\",\n",
    "      X_tokens_test.shape, \"\\t: X_tokens_test \", \"\\n\",\n",
    "      X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeR6DmO2R6dN"
   },
   "source": [
    "# Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PKWwyFjzR6dN"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    \n",
    "    if MODEL_NAME == 'xlm':\n",
    "        config = transformers.XLMRobertaConfig.from_pretrained(MODEL_DIR)\n",
    "        model = transformers.TFXLMRobertaModel.from_pretrained(MODEL_DIR, config=config) # TFXLMRobertaForSequenceClassification\n",
    "        x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    else:\n",
    "        config = transformers.DistilBertConfig.from_pretrained(MODEL_DIR)\n",
    "        model = transformers.TFDistilBertModel.from_pretrained(MODEL_DIR) # TFDistilBertForSequenceClassification\n",
    "        x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    toxic_output = tf.keras.layers.Activation('sigmoid', name=\"toxic_output\")(x1)\n",
    "    \n",
    "    model = Model([input_att_flags, input_sequences],\n",
    "                  [toxic_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vNdeazmhR5jP"
   },
   "outputs": [],
   "source": [
    "# def build_model():\n",
    "#     input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "#     input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    \n",
    "#     if MODEL_NAME == 'xlm':\n",
    "#         config = transformers.XLMRobertaConfig.from_pretrained(MODEL_DIR)\n",
    "#         model = transformers.TFXLMRobertaForSequenceClassification.from_pretrained(MODEL_DIR, config=config)\n",
    "#         x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "#     else:\n",
    "#         config = transformers.DistilBertConfig.from_pretrained(MODEL_DIR)\n",
    "#         model = transformers.TFDistilBertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "#         x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    \n",
    "#     x1 = tf.keras.layers.Dense(1)(x[0])\n",
    "#     toxic_output = tf.keras.layers.Activation('sigmoid', name=\"toxic_output\")(x1)\n",
    "    \n",
    "#     model = Model([input_att_flags, input_sequences],\n",
    "#                   [toxic_output])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n-x9lurmR6dQ"
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ugXKFEnqR6dT",
    "outputId": "2c509dc7-da68-4adc-f9d6-9c9151b0fad8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB ((None, 75, 768),)   134734080   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 75, 768)      0           tf_distil_bert_model[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 75, 768)      1180416     dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 75, 768)      3072        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 75, 768)      0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 75, 1)        769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 75, 1)        4           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 75, 1)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 75)           0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            76          flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "toxic_output (Activation)       (None, 1)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 135,918,417\n",
      "Trainable params: 135,916,879\n",
      "Non-trainable params: 1,538\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeR6DmO2R6dN"
   },
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GGb6IwuR6dY"
   },
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.zeros((len(X_tokens_test), 1))\n",
    "timings_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "lpuHmXlWR6dq",
    "outputId": "249a868a-d0a0-482d-965d-e26bd49bff48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Fold: 0 Valid_Lang: en  ==================\n",
      "Train on 8000 samples, validate on 3000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 108s 14ms/sample - loss: 0.4432 - accuracy: 0.8475 - auc: 0.7730 - val_loss: 0.3585 - val_accuracy: 0.9153 - val_auc: 0.8852\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 97s 12ms/sample - loss: 0.3512 - accuracy: 0.9041 - auc: 0.9256 - val_loss: 0.3978 - val_accuracy: 0.8840 - val_auc: 0.8749\n",
      "Number of psuedo samples available: 11825\n",
      "Psuedo Toxicity: 9644\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'es': 2639, 'tr': 2572, 'fr': 2303, 'pt': 1972, 'it': 1424, 'ru': 915})\n",
      "Train on 19825 samples, validate on 3000 samples\n",
      "Epoch 1/2\n",
      "19825/19825 [==============================] - 261s 13ms/sample - loss: 0.3748 - accuracy: 0.8936 - auc: 0.9560 - val_loss: 0.5848 - val_accuracy: 0.7483 - val_auc: 0.8760\n",
      "Epoch 2/2\n",
      "19825/19825 [==============================] - 252s 13ms/sample - loss: 0.2754 - accuracy: 0.9554 - auc: 0.9892 - val_loss: 0.4678 - val_accuracy: 0.8353 - val_auc: 0.8800\n",
      "Train on 19825 samples, validate on 3000 samples\n",
      "19825/19825 [==============================] - 261s 13ms/sample - loss: 0.2223 - accuracy: 0.9862 - auc: 0.9989 - val_loss: 0.4741 - val_accuracy: 0.8573 - val_auc: 0.8876\n",
      "Train ROC-AUC:\t 0.9964202423412714\n",
      "Valid ROC-AUC:\t 0.8870248025300289\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      6770\n",
      "           1       0.90      0.96      0.93      1230\n",
      "\n",
      "    accuracy                           0.98      8000\n",
      "   macro avg       0.95      0.97      0.96      8000\n",
      "weighted avg       0.98      0.98      0.98      8000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92      2706\n",
      "           1       0.38      0.76      0.51       294\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.68      0.81      0.71      3000\n",
      "weighted avg       0.91      0.86      0.88      3000\n",
      "\n",
      "================== Fold: 1 Valid_Lang: es  ==================\n",
      "Train on 8500 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "8500/8500 [==============================] - 124s 15ms/sample - loss: 0.3887 - accuracy: 0.8808 - auc: 0.8434 - val_loss: 0.4384 - val_accuracy: 0.8432 - val_auc: 0.8268\n",
      "Epoch 2/2\n",
      "8500/8500 [==============================] - 111s 13ms/sample - loss: 0.3185 - accuracy: 0.9272 - auc: 0.9510 - val_loss: 0.4911 - val_accuracy: 0.8492 - val_auc: 0.7904\n",
      "Number of psuedo samples available: 26059\n",
      "Psuedo Toxicity: 2263\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'tr': 8770, 'pt': 5151, 'fr': 3492, 'es': 3485, 'it': 3377, 'ru': 1784})\n",
      "Train on 34559 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "34559/34559 [==============================] - 431s 12ms/sample - loss: 0.2634 - accuracy: 0.9663 - auc: 0.9701 - val_loss: 0.5485 - val_accuracy: 0.8124 - val_auc: 0.8027\n",
      "Epoch 2/2\n",
      "34559/34559 [==============================] - 421s 12ms/sample - loss: 0.2326 - accuracy: 0.9827 - auc: 0.9889 - val_loss: 0.4879 - val_accuracy: 0.8528 - val_auc: 0.7661\n",
      "Train on 34559 samples, validate on 2500 samples\n",
      "34559/34559 [==============================] - 434s 13ms/sample - loss: 0.2115 - accuracy: 0.9935 - auc: 0.9982 - val_loss: 0.5400 - val_accuracy: 0.8492 - val_auc: 0.7814\n",
      "Train ROC-AUC:\t 0.9615371594520322\n",
      "Valid ROC-AUC:\t 0.8021748947447646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      7398\n",
      "           1       0.73      0.80      0.76      1102\n",
      "\n",
      "    accuracy                           0.94      8500\n",
      "   macro avg       0.85      0.88      0.86      8500\n",
      "weighted avg       0.94      0.94      0.94      8500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88      2078\n",
      "           1       0.45      0.56      0.50       422\n",
      "\n",
      "    accuracy                           0.81      2500\n",
      "   macro avg       0.68      0.71      0.69      2500\n",
      "weighted avg       0.83      0.81      0.82      2500\n",
      "\n",
      "================== Fold: 2 Valid_Lang: it  ==================\n",
      "Train on 8500 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "8500/8500 [==============================] - 123s 14ms/sample - loss: 0.3808 - accuracy: 0.8922 - auc: 0.8390 - val_loss: 0.5332 - val_accuracy: 0.8244 - val_auc: 0.7824\n",
      "Epoch 2/2\n",
      "8500/8500 [==============================] - 110s 13ms/sample - loss: 0.3166 - accuracy: 0.9276 - auc: 0.9424 - val_loss: 0.5148 - val_accuracy: 0.7960 - val_auc: 0.7627\n",
      "Number of psuedo samples available: 22680\n",
      "Psuedo Toxicity: 4413\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'tr': 5458, 'pt': 5132, 'es': 3890, 'fr': 3364, 'it': 2464, 'ru': 2372})\n",
      "Train on 31180 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "31180/31180 [==============================] - 392s 13ms/sample - loss: 0.2697 - accuracy: 0.9604 - auc: 0.9809 - val_loss: 0.7693 - val_accuracy: 0.6844 - val_auc: 0.7501\n",
      "Epoch 2/2\n",
      "31180/31180 [==============================] - 385s 12ms/sample - loss: 0.2290 - accuracy: 0.9831 - auc: 0.9955 - val_loss: 0.7461 - val_accuracy: 0.7376 - val_auc: 0.7563\n",
      "Train on 31180 samples, validate on 2500 samples\n",
      "31180/31180 [==============================] - 395s 13ms/sample - loss: 0.2086 - accuracy: 0.9944 - auc: 0.9997 - val_loss: 0.7257 - val_accuracy: 0.7604 - val_auc: 0.7441\n",
      "Train ROC-AUC:\t 0.9919611561492591\n",
      "Valid ROC-AUC:\t 0.756180132320829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      7464\n",
      "           1       0.82      0.95      0.88      1036\n",
      "\n",
      "    accuracy                           0.97      8500\n",
      "   macro avg       0.91      0.96      0.93      8500\n",
      "weighted avg       0.97      0.97      0.97      8500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.77      0.83      2012\n",
      "           1       0.39      0.60      0.47       488\n",
      "\n",
      "    accuracy                           0.74      2500\n",
      "   macro avg       0.64      0.68      0.65      2500\n",
      "weighted avg       0.79      0.74      0.76      2500\n",
      "\n",
      "================== Fold: 3 Valid_Lang: tr  ==================\n",
      "Train on 8000 samples, validate on 3000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 108s 13ms/sample - loss: 0.4254 - accuracy: 0.8568 - auc: 0.8067 - val_loss: 0.3682 - val_accuracy: 0.8987 - val_auc: 0.8486\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 99s 12ms/sample - loss: 0.3386 - accuracy: 0.9111 - auc: 0.9397 - val_loss: 0.4030 - val_accuracy: 0.8973 - val_auc: 0.7714\n",
      "Number of psuedo samples available: 22529\n",
      "Psuedo Toxicity: 7356\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'tr': 4930, 'it': 4027, 'es': 3744, 'pt': 3400, 'fr': 3370, 'ru': 3058})\n",
      "Train on 30529 samples, validate on 3000 samples\n",
      "Epoch 1/2\n",
      "30529/30529 [==============================] - 392s 13ms/sample - loss: 0.3081 - accuracy: 0.9364 - auc: 0.9767 - val_loss: 0.4066 - val_accuracy: 0.8877 - val_auc: 0.8126\n",
      "Epoch 2/2\n",
      "30529/30529 [==============================] - 381s 12ms/sample - loss: 0.2495 - accuracy: 0.9735 - auc: 0.9926 - val_loss: 0.4887 - val_accuracy: 0.8957 - val_auc: 0.7047\n",
      "Train on 30529 samples, validate on 3000 samples\n",
      "30529/30529 [==============================] - 395s 13ms/sample - loss: 0.2190 - accuracy: 0.9886 - auc: 0.9989 - val_loss: 0.4647 - val_accuracy: 0.8957 - val_auc: 0.6843\n",
      "Train ROC-AUC:\t 0.9546212326383117\n",
      "Valid ROC-AUC:\t 0.8129757462686568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92      6796\n",
      "           1       0.55      0.92      0.69      1204\n",
      "\n",
      "    accuracy                           0.87      8000\n",
      "   macro avg       0.77      0.89      0.80      8000\n",
      "weighted avg       0.92      0.87      0.89      8000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      2680\n",
      "           1       0.44      0.20      0.28       320\n",
      "\n",
      "    accuracy                           0.89      3000\n",
      "   macro avg       0.68      0.58      0.61      3000\n",
      "weighted avg       0.86      0.89      0.87      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, valid_lang in enumerate(data.lang.unique()):\n",
    "    print(\"================== Fold:\", num, \"Valid_Lang:\", valid_lang,\" ==================\")\n",
    "    start_time = time()\n",
    "    X_tokens_train, X_att_train, Y_toxic_train, X_tokens_valid, X_att_valid, Y_toxic_valid = sample_data(data,\n",
    "                                                                                                         SAMPLE_SIZE,\n",
    "                                                                                                         MAX_SEQ_LEN,\n",
    "                                                                                                         valid_lang=valid_lang)\n",
    "    \n",
    "    # First fit\n",
    "    if num>0:\n",
    "        del model\n",
    "        del mcp\n",
    "        del csvl\n",
    "        del adam\n",
    "        del history\n",
    "        del auc\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "    model = build_model()\n",
    "    auc = tf.keras.metrics.AUC()\n",
    "    mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+valid_lang+\"_\"+str(num)+\".h5\", monitor='val_auc',\n",
    "                          verbose=0, save_best_only=True, save_weights_only=True, mode='max', save_freq='epoch')\n",
    "    csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+valid_lang+\"_\"+str(num)+\".csv\",\n",
    "                     separator=\",\", append=True)\n",
    "\n",
    "    model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=LR)\n",
    "    model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy', auc])\n",
    "    \n",
    "    train_time = time()\n",
    "    history = model.fit(x={\"att_flags\":X_att_train,\n",
    "                           \"words\":X_tokens_train},\n",
    "                        y={\"toxic_output\":Y_toxic_train},\n",
    "                        validation_data=({\"att_flags\":X_att_valid,\n",
    "                                          \"words\":X_tokens_valid},\n",
    "                                         {\"toxic_output\":Y_toxic_valid}),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[0],\n",
    "                        shuffle=True,\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    # Psuedo model fit\n",
    "    psuedo_time = time()\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_psuedo = model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                     \"words\":X_tokens_test},\n",
    "                                batch_size=PREDICT_BATCH_SIZE).reshape((-1))\n",
    "    \n",
    "    Y_toxic_psuedo = np.where(pred_psuedo >= PSUEDO_PROB_THRESH_HIGH, 1, 0)\n",
    "    psuedo_flag = (pred_psuedo >= PSUEDO_PROB_THRESH_HIGH) | (pred_psuedo <= PSUEDO_PROB_THRESH_LOW)\n",
    "    \n",
    "    print(\"Number of psuedo samples available:\", sum(psuedo_flag))\n",
    "    print(\"Psuedo Toxicity:\", sum(Y_toxic_psuedo))\n",
    "    print(Counter(test.lang.values))\n",
    "    print(Counter(test.lang.values[psuedo_flag]))\n",
    "    \n",
    "    X_att_psuedo = np.concatenate((X_att_train, X_att_test[psuedo_flag]))\n",
    "    X_tokens_psuedo = np.concatenate((X_tokens_train, X_tokens_test[psuedo_flag]))\n",
    "    Y_toxic_psuedo = np.concatenate((Y_toxic_train, Y_toxic_psuedo[psuedo_flag]))\n",
    "    \n",
    "    shuffled_idxs = np.arange(Y_toxic_psuedo.shape[0])\n",
    "    np.random.shuffle(shuffled_idxs)\n",
    "    \n",
    "    del model\n",
    "    del mcp\n",
    "    del csvl\n",
    "    del adam\n",
    "    del history\n",
    "    del auc\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "\n",
    "    model = build_model()\n",
    "    auc = tf.keras.metrics.AUC()\n",
    "    mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+valid_lang+\"_\"+str(num)+\".h5\", monitor='val_auc',\n",
    "                          verbose=0, save_best_only=True, save_weights_only=True, mode='max', save_freq='epoch')\n",
    "    csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+valid_lang+\"_\"+str(num)+\".csv\",\n",
    "                     separator=\",\", append=True)\n",
    "\n",
    "    model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=LR)\n",
    "    model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy', auc])\n",
    "        \n",
    "    history = model.fit(x={\"att_flags\":X_att_psuedo[shuffled_idxs],\n",
    "                           \"words\":X_tokens_psuedo[shuffled_idxs]},\n",
    "                        y={\"toxic_output\":Y_toxic_psuedo[shuffled_idxs]},\n",
    "                        validation_data=({\"att_flags\":X_att_valid,\n",
    "                                          \"words\":X_tokens_valid},\n",
    "                                         {\"toxic_output\":Y_toxic_valid}),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[0],\n",
    "                        shuffle=True,\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=LR*0.1)\n",
    "    model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy', auc])\n",
    "    \n",
    "    history = model.fit(x={\"att_flags\":X_att_psuedo[shuffled_idxs],\n",
    "                           \"words\":X_tokens_psuedo[shuffled_idxs]},\n",
    "                        y={\"toxic_output\":Y_toxic_psuedo[shuffled_idxs]},\n",
    "                        validation_data=({\"att_flags\":X_att_valid,\n",
    "                                          \"words\":X_tokens_valid},\n",
    "                                         {\"toxic_output\":Y_toxic_valid}),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        epochs=1,\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    infer_time = time()\n",
    "    \n",
    "    # Loading best weights per fold\n",
    "    model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+valid_lang+\"_\"+str(num)+\".h5\")\n",
    "\n",
    "    pred_train = model.predict(x = {\"att_flags\":X_att_train,\n",
    "                                    \"words\":X_tokens_train},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_valid = model.predict(x = {\"att_flags\":X_att_valid,\n",
    "                                    \"words\":X_tokens_valid},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    print(\"Train ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic_train, y_score=pred_train))\n",
    "    print(\"Valid ROC-AUC:\\t\", roc_auc_score(y_true=Y_toxic_valid, y_score=pred_valid))\n",
    "    \n",
    "    print(classification_report(y_true=Y_toxic_train, y_pred=np.where(pred_train>0.5, 1, 0)))\n",
    "    print(classification_report(y_true=Y_toxic_valid, y_pred=np.where(pred_valid>0.5, 1, 0)))\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test += model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                    \"words\":X_tokens_test},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    end_time = time()\n",
    "    timings_dict.update({num:{\n",
    "        'start_time' : ctime(start_time),\n",
    "        'train_time' : ctime(train_time),\n",
    "        'infer_time' : ctime(infer_time),\n",
    "        'psuedo_time' : ctime(psuedo_time),\n",
    "        'end_time' : ctime(end_time),\n",
    "    }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start_time</th>\n",
       "      <td>Thu Jun 18 19:38:18 2020</td>\n",
       "      <td>Thu Jun 18 20:00:33 2020</td>\n",
       "      <td>Thu Jun 18 20:31:51 2020</td>\n",
       "      <td>Thu Jun 18 21:01:13 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>Thu Jun 18 19:38:38 2020</td>\n",
       "      <td>Thu Jun 18 20:00:54 2020</td>\n",
       "      <td>Thu Jun 18 20:32:12 2020</td>\n",
       "      <td>Thu Jun 18 21:01:34 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infer_time</th>\n",
       "      <td>Thu Jun 18 19:57:33 2020</td>\n",
       "      <td>Thu Jun 18 20:28:51 2020</td>\n",
       "      <td>Thu Jun 18 20:58:13 2020</td>\n",
       "      <td>Thu Jun 18 21:27:06 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psuedo_time</th>\n",
       "      <td>Thu Jun 18 19:42:04 2020</td>\n",
       "      <td>Thu Jun 18 20:04:49 2020</td>\n",
       "      <td>Thu Jun 18 20:36:06 2020</td>\n",
       "      <td>Thu Jun 18 21:05:01 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_time</th>\n",
       "      <td>Thu Jun 18 20:00:33 2020</td>\n",
       "      <td>Thu Jun 18 20:31:51 2020</td>\n",
       "      <td>Thu Jun 18 21:01:13 2020</td>\n",
       "      <td>Thu Jun 18 21:30:07 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0                         1  \\\n",
       "start_time   Thu Jun 18 19:38:18 2020  Thu Jun 18 20:00:33 2020   \n",
       "train_time   Thu Jun 18 19:38:38 2020  Thu Jun 18 20:00:54 2020   \n",
       "infer_time   Thu Jun 18 19:57:33 2020  Thu Jun 18 20:28:51 2020   \n",
       "psuedo_time  Thu Jun 18 19:42:04 2020  Thu Jun 18 20:04:49 2020   \n",
       "end_time     Thu Jun 18 20:00:33 2020  Thu Jun 18 20:31:51 2020   \n",
       "\n",
       "                                    2                         3  \n",
       "start_time   Thu Jun 18 20:31:51 2020  Thu Jun 18 21:01:13 2020  \n",
       "train_time   Thu Jun 18 20:32:12 2020  Thu Jun 18 21:01:34 2020  \n",
       "infer_time   Thu Jun 18 20:58:13 2020  Thu Jun 18 21:27:06 2020  \n",
       "psuedo_time  Thu Jun 18 20:36:06 2020  Thu Jun 18 21:05:01 2020  \n",
       "end_time     Thu Jun 18 21:01:13 2020  Thu Jun 18 21:30:07 2020  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(timings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['toxic'] = pred_test/float(data.lang.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','toxic']].to_csv(RESULTS_DIR+\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    63812.000000\n",
       "mean         0.366454\n",
       "std          0.307259\n",
       "min          0.035796\n",
       "25%          0.070884\n",
       "50%          0.287731\n",
       "75%          0.628366\n",
       "max          0.970421\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['toxic'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">test</th>\n",
       "      <th>es</th>\n",
       "      <td>8438</td>\n",
       "      <td>0.408146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>10920</td>\n",
       "      <td>0.512181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>8494</td>\n",
       "      <td>0.316508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>11012</td>\n",
       "      <td>0.388255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>10948</td>\n",
       "      <td>0.446772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>14000</td>\n",
       "      <td>0.178007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     toxic\n",
       "set  lang                 \n",
       "test es     8438  0.408146\n",
       "     fr    10920  0.512181\n",
       "     it     8494  0.316508\n",
       "     pt    11012  0.388255\n",
       "     ru    10948  0.446772\n",
       "     tr    14000  0.178007"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"48\" valign=\"top\">test</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">es</th>\n",
       "      <th>count</th>\n",
       "      <td>8438.000000</td>\n",
       "      <td>8438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32405.344039</td>\n",
       "      <td>0.408146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18373.331400</td>\n",
       "      <td>0.340838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.038540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16665.000000</td>\n",
       "      <td>0.081433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32646.500000</td>\n",
       "      <td>0.272717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48250.250000</td>\n",
       "      <td>0.760034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63811.000000</td>\n",
       "      <td>0.970421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">fr</th>\n",
       "      <th>count</th>\n",
       "      <td>10920.000000</td>\n",
       "      <td>10920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31676.724176</td>\n",
       "      <td>0.512181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18378.065199</td>\n",
       "      <td>0.287206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.037535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15838.500000</td>\n",
       "      <td>0.290624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31451.500000</td>\n",
       "      <td>0.511828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47618.750000</td>\n",
       "      <td>0.753426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63800.000000</td>\n",
       "      <td>0.967604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">it</th>\n",
       "      <th>count</th>\n",
       "      <td>8494.000000</td>\n",
       "      <td>8494.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31916.568048</td>\n",
       "      <td>0.316508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18416.472108</td>\n",
       "      <td>0.289545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.038970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16118.000000</td>\n",
       "      <td>0.075702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31959.500000</td>\n",
       "      <td>0.176869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47677.250000</td>\n",
       "      <td>0.520319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63809.000000</td>\n",
       "      <td>0.962840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">pt</th>\n",
       "      <th>count</th>\n",
       "      <td>11012.000000</td>\n",
       "      <td>11012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31795.557937</td>\n",
       "      <td>0.388255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18540.963518</td>\n",
       "      <td>0.309186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.039122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15683.750000</td>\n",
       "      <td>0.071135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31609.000000</td>\n",
       "      <td>0.360859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47991.500000</td>\n",
       "      <td>0.651028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63798.000000</td>\n",
       "      <td>0.967343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">ru</th>\n",
       "      <th>count</th>\n",
       "      <td>10948.000000</td>\n",
       "      <td>10948.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31952.666423</td>\n",
       "      <td>0.446772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18307.410026</td>\n",
       "      <td>0.267714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16051.750000</td>\n",
       "      <td>0.275082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32195.000000</td>\n",
       "      <td>0.398015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47751.750000</td>\n",
       "      <td>0.642218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63803.000000</td>\n",
       "      <td>0.967013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">tr</th>\n",
       "      <th>count</th>\n",
       "      <td>14000.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31825.560000</td>\n",
       "      <td>0.178007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18477.674436</td>\n",
       "      <td>0.231994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15613.750000</td>\n",
       "      <td>0.054796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31902.500000</td>\n",
       "      <td>0.066594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47873.500000</td>\n",
       "      <td>0.148136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63810.000000</td>\n",
       "      <td>0.963970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id         toxic\n",
       "set  lang                                  \n",
       "test es   count   8438.000000   8438.000000\n",
       "          mean   32405.344039      0.408146\n",
       "          std    18373.331400      0.340838\n",
       "          min        7.000000      0.038540\n",
       "          25%    16665.000000      0.081433\n",
       "          50%    32646.500000      0.272717\n",
       "          75%    48250.250000      0.760034\n",
       "          max    63811.000000      0.970421\n",
       "     fr   count  10920.000000  10920.000000\n",
       "          mean   31676.724176      0.512181\n",
       "          std    18378.065199      0.287206\n",
       "          min        5.000000      0.037535\n",
       "          25%    15838.500000      0.290624\n",
       "          50%    31451.500000      0.511828\n",
       "          75%    47618.750000      0.753426\n",
       "          max    63800.000000      0.967604\n",
       "     it   count   8494.000000   8494.000000\n",
       "          mean   31916.568048      0.316508\n",
       "          std    18416.472108      0.289545\n",
       "          min        2.000000      0.038970\n",
       "          25%    16118.000000      0.075702\n",
       "          50%    31959.500000      0.176869\n",
       "          75%    47677.250000      0.520319\n",
       "          max    63809.000000      0.962840\n",
       "     pt   count  11012.000000  11012.000000\n",
       "          mean   31795.557937      0.388255\n",
       "          std    18540.963518      0.309186\n",
       "          min        6.000000      0.039122\n",
       "          25%    15683.750000      0.071135\n",
       "          50%    31609.000000      0.360859\n",
       "          75%    47991.500000      0.651028\n",
       "          max    63798.000000      0.967343\n",
       "     ru   count  10948.000000  10948.000000\n",
       "          mean   31952.666423      0.446772\n",
       "          std    18307.410026      0.267714\n",
       "          min        1.000000      0.035796\n",
       "          25%    16051.750000      0.275082\n",
       "          50%    32195.000000      0.398015\n",
       "          75%    47751.750000      0.642218\n",
       "          max    63803.000000      0.967013\n",
       "     tr   count  14000.000000  14000.000000\n",
       "          mean   31825.560000      0.178007\n",
       "          std    18477.674436      0.231994\n",
       "          min        0.000000      0.036216\n",
       "          25%    15613.750000      0.054796\n",
       "          50%    31902.500000      0.066594\n",
       "          75%    47873.500000      0.148136\n",
       "          max    63810.000000      0.963970"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"]).apply(pd.Series.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Mr6qXyMSR6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 18 21:30:08 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
