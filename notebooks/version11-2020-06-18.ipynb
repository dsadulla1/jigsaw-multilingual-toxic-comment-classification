{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changelog:\n",
    "1. This notebook is a copy of Kaggle's V8V8 with a Public LB score of 0.8826\n",
    "2. This has been enhanced with Cross validation on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yi9E6gwR6bw"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "attachments": {
    "186676f7-f9d2-4727-a904-889751b9f6b6.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAB5CAYAAAAwN0luAAANdUlEQVR4Ae3d3XHqSBCGYeIiICI5ARDKXpEMaey9tgYsLPZgSyOEeqR+qHItx+hv3n6n+Rgwe+jcEEAAAQQQQAABBBBIQuCQZJyGiQACCCCAAAIIIIBAJ/ySAAEEEEAAAQQQQCANAeE3TakNFAEEEEAAAQQQQED45QACCCCAAAIIIIBAGgLCb5pSGygCCCCAAAIIIICA8MsBBBBAAAEEEEAAgTQEhN80pTZQBBBAAAEEEEAAAeGXAwgggAACCCCAAAJpCAi/aUptoAgggAACCCCAAALCLwcQQAABBBBAAAEE0hAQftOU2kARQAABBBBAAAEEhF8OIIAAAggggAACCKQhIPymKbWBIoAAAggggAACCAi/HEAAAQQQQAABBBBIQ0D4TVNqA0UAAQQQQAABBBAQfjmAAAIIIIAAAgggkIaA8Jum1AaKAAIIIIAAAgggIPxyAAEEEEAAAQQQQCANAeE3TakNFAEEEEAAAQQQQED45QACCCCAAAIIIIBAGgLCb5pSGygCCCCAAAIIIICA8MsBBBBAAAEEEEAAgTQEhN80pTZQBBBAAAEEEEAAgZDwezgcOj8YcIADHOAABzjAgdwORETxsPAbMVjnRKAnUJqtGwKRBDgYSd+5CwEO8iCaQJSDIQkgarDRRXb+dghwsJ1aZL0SDmatfDvj5mA7tch6JVEOCr9ZjUs+7qgJlxy74Q8IcHAAw90QAhwMwe6kAwJRDgq/gyK4m4dA1ITLQ9hIxwhwcIyQxz9NgIOfJuz4YwSiHBR+xyrj8V0SiJpwu4RpULMIcHAWNjstSICDC8J0qFkEohwUfmeVy05bJxA14bbOzfUvR4CDy7F0pHkEODiPm72WIxDloPC7XA0daUMEoibchhC51A8T4GAl4Ou5O96+JvPYna+V+9r8JQEOvsTilysSiHJQ+F2xyE7VDoGoCdcOAVcSTYCDlRV4EX4vp6/vRz1dKg9m80KAg3Ue8K2O15StoxwUfqdUxza7IxA14XYH0oBmE+DgbHSPHYWRB4pZdzhYh41vdbymbB3loPA7pTpB21zPp6+3+e6rG8fTpft+t+/anY/l96fucr10p9v9frvzYLugi2/8tFETrnEsLy/vdw+77no5d8eHf8fudPm29OUB/fJGgIOVIjyt/Pb972vlt/+/hloBroLKwam4fvPt0p2Kf8dzd7n0z9mnznsR09hGOSj8TqvP6ls9XmH2Tb3/77EPtj9Mxn47TwK/1ixqwv16UQ0+OOrhI5AMQ4jGP6WUHJxCabDNw7Xymd8f+p++NwA2fpeD44zuW/zm21f47Z97b//VA6eSjXJQ+J1aoTW3e2ryXycuq7u3SdX/scdgMh7L6m/Z7to9wsojJK954ds5V9SE2w6hotOLPzD6v4eX0+1zg4fi4E3BS3fu729qsOtfLAcrmb/w8dHvhN5KmPfNOViH7bVv/XPzoTs8vTtbd+ysW0c5KPy2aFwfKJ5eSX6vrN37/Hf4fer7/b7C76+VjZpwv15Uaw/2Lv3q4aDxHw7d8fFCrLXBtHc9HKysifBbCWx8cw6OMxpu8Xv47Remhnu4P0YgykHhd6wyEY9PCh3C7zuliZpw71zz6vtO8rBc1bW7PH0+3ZPAlFpxcAqlwTbC7wDGMnc5WMdR+K3jNWXrKAeF3ynVWXubF03+70sQfv9mMv03URNu+hU2sOUUD8sfeDze6vt28uiLWEcLyMFRRM8bvPDxdRh53s2/fibAwZ/ZvHrktW/9u19e9L9iNva7KAeF37HKBD3+mGR/veXcf5D+O2j42EN9kaImXP2Vxu4x6uEPq8NPTsYOodmzc7CyNC/C7/V8vH/mvO+TxKuCysEqXN1r34TfOorPW0c5KPw+16Ghf5U/Xjs+fdVZkeT21Wa3qxR+3ylW1IR755pj9h33sHh6d7N83Y+vOptaJw5OJfW13YvwWz5ycx745x2HOqYcrOP12jfht5bicPsoB4XfYRXcT0MgasKlAWygowQ4OIrIBh8mwMEPA3b4UQJRDgq/o6WxwR4JRE24PbI0pnkEODiPm72WI8DB5Vg60jwCUQ4Kv/PqZa+NE4iacBvH5vIXJMDBBWE61CwCHJyFzU4LEohyUPhdsIgOtR0CURNuO4Rc6acJcPDThB1/jAAHxwh5/NMEohwUfj9dWcdvkkDUhGsShosKIcDBEOxOOiDAwQEMd0MIRDko/IaU20mjCURNuOhxO387BDjYTi2yXgkHs1a+nXFHORgWfsuA/WDAAQ5wgAMc4AAH8joQEcXDwm/XlVP72TqD0rC6Df7crpt/u5iDHNRHo/soBznYgoP//vOn29rP/bl4/fhbjF39Jnjsp1Fo+vupZXTznnt+DnJwrjtL7cdBDi7l0tzjFAe3FnzL9d7mzuop9E559dPeB2uyzJW8pf00fR5H+8hBDnJw3jtwnov3M3dKLYXf6XG2VH71mwm3rwnnYw/7qWd0iJhzfuGXf3O8WXIfDnJwSZ/mHEv4rYuyxdjVb8LvfhqFpr+fWs5puC3sw0EORnvIQQ624KCV3+lxthi7+k343U+j0PT3U8vo5j33/Bzk4Fx3ltqPgxxcyqW5xykOCr/T42wxdvXbrVH4S3t/aR/4LREc3M+TleCxn1rOfeKP3o+DHGzBQeF3epwVfoXwt0K4pq/pt9D0fe6ch5Ee6oP8i/SvnLs4KPwKv28FumiJt3R+TV/Tj/aVgxzkoG97iHYg+vzC7/TgW7YsXXP12+3JyorrLgK64CF4tND0rfzyMNJDfZB/kf6Vcwu/dVFW+BXC3wrhmr6m30LTF355GOmhPsi/SP+E37rgW7YWfoVf4ZcDbznQQtMXfoWPSA+FX/5F+lfObeW3LgALv4LPW8FH09f0W2j6wi8PIz3UB/kX6Z/wWxd8rfwKvm8F337CCR4af2TjFzz4F+mfPsi/aP96B33bw/QQXKxd/XZ7shI83w6erUw44Vfzj3RR+OVfpH998NAHeRjpYemDwu/0OCv8CuFvhXDBQ8OPbPiCB/+i/eMgB1txUPgVft8KdC2IvJVrEH41/mhXOchBDvqe32gHos9v5Xd68C1blq65+u32ZGXFdRcBXfAQPFpo+t5y5mGkh/og/yL9K+cWfuuirPArhL8VwjV9Tb+Fpi/88jDSQ32Qf5H+Cb91wbdsLfwKv8IvB95yoIWmL/wKH5EeCr/8i/SvnNvKb10AFn4Fn7eCj6av6bfQ9IVfHkZ6qA/yL9I/4bcu+Fr5FXzfCr79hBM8NP7Ixi948C/SP32Qf9H+9Q76tofpIbhYu/rt9mQleL4dPFuZcMKv5h/povDLv0j/+uChD/Iw0sPSB4Xf6XE2LPyWQvnBgAMc4AAHOMABDuR1YHpkXW7LsPAb+QrJub1C5wAHOMCB7A6UwLnF1ULX/Gc3dSsORtxCznofrMabvfEavznAAQ5wIM4B4Xc/IXKrLwiEX58B3sVngD2RxT2RYY89BzhQ44DwK/xGh2bhV/gVfjnAAQ5wgAOrOSD8Cr/C74ofurgnfa/Qa16h25YvHOAABziwpAPCr/Ar/Aq/q73aXrJ5OZYnQw5wgAMcmOOA8Cv8Cr/Cr/Dr7UYOcIADHEjjgPAr/Aq/wm+ahjdnhcA+VpY4wAEO7MsB4Vf4FX6FX+HXig8HOMABDqRxQPgVfoVf4TdNw7N6s6/VG/VUTw5wYI4Dwq/wK/wKv8KvFR8OcIADHEjjgPAr/Aq/wm+ahjdnhcA+VpY4wAEO7MsB4Vf4FX6FX+HXig8HOMABDqRxQPgVfoVf4TdNw7N6s6/VG/VUTw5wYI4Dwq/wK/wKv8KvFR8OcIADHEjjgPAr/Aq/wm+ahjdnhcA+VpY4wAEO7MsB4Vf4FX6FX+HXig8HOMABDqRxQPgVfoVf4TdNw7N6s6/VG/VUTw5wYI4Dwq/wK/wKv8KvFR8OcIADHEjjgPAr/Aq/wm+ahjdnhcA+VpY4wAEO7MsB4Vf4FX6FX+HXig8HOMABDqRxQPgVfoVf4TdNw7N6s6/VG/VUTw5wYI4Dwq/wK/wKv8KvFR8OcIADHEjjgPAr/Aq/K4ffMun8YMABDnCAAxzgAAfyOrBi/HycqrxX5IYAAggggAACCCCAQAoCwm+KMhskAggggAACCCCAQCEg/PIAAQQQQAABBBBAIA0B4TdNqQ0UAQQQQAABBBBAQPjlAAIIIIAAAggggEAaAsJvmlIbKAIIIIAAAggggIDwywEEEEAAAQQQQACBNASE3zSlNlAEEEAAAQQQQAAB4ZcDCCCAAAIIIIAAAmkICL9pSm2gCCCAAAIIIIAAAsIvBxBAAAEEEEAAAQTSEBB+05TaQBFAAAEEEEAAAQSEXw4ggAACCCCAAAIIpCEg/KYptYEigAACCCCAAAIICL8cQAABBBBAAAEEEEhDQPhNU2oDRQABBBBAAAEEEBB+OYAAAggggAACCCCQhoDwm6bUBooAAggggAACCCAg/HIAAQQQQAABBBBAIA0B4TdNqQ0UAQQQQAABBBBAQPjlAAIIIIAAAggggEAaAsJvmlIbKAIIIIAAAggggIDwywEEEEAAAQQQQACBNASE3zSlNlAEEEAAAQQQQACB/wCtYPgfC8HBfwAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:186676f7-f9d2-4727-a904-889751b9f6b6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PhXEkPQNR6bx"
   },
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V11\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "MODEL_NAME = 'distilbert' # options include 'xlm' or 'distilbert'\n",
    "\n",
    "NUM_EPOCHS = [4, 8, 2]\n",
    "LR = 5e-4\n",
    "MAX_SEQ_LEN = 128\n",
    "SAMPLE_SIZE = 6000\n",
    "PSUEDO_QUANTILE_THRESH_HIGH = 0.95\n",
    "PSUEDO_QUANTILE_THRESH_LOW = 0.05\n",
    "\n",
    "RUN_ON_SAMPLE = 0\n",
    "if RUN_ON_SAMPLE>0:\n",
    "    SAMPLE_SIZE = RUN_ON_SAMPLE\n",
    "\n",
    "ON_KAGGLE = False\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    BATCH_SIZE = 32\n",
    "    PREDICT_BATCH_SIZE = 512\n",
    "else:\n",
    "    BATCH_SIZE = 16\n",
    "    PREDICT_BATCH_SIZE = 256\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vBSH2FLlR6b1"
   },
   "outputs": [],
   "source": [
    "if ON_KAGGLE:\n",
    "    RESULTS_DIR = '../working/'\n",
    "    DATA_DIR = '../input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = '../input/tf-xlm-roberta-base/'\n",
    "    else:\n",
    "        MODEL_DIR = '../input/tf-distilbert-base-multilingual-cased/'\n",
    "else:\n",
    "    PATH = \"..\"\n",
    "    RESULTS_DIR = PATH+\"/results/\"\n",
    "    DATA_DIR = PATH+\"/data/\"\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = PATH+\"/models/tf-xlm-roberta-base/\"\n",
    "    else:\n",
    "        MODEL_DIR = PATH+\"/models/distilbert-base-multilingual-cased/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCnsk-7nR6b4"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oW_oT4SZR6b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold, LeaveOneGroupOut\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.optimizers import TriangularCyclicalLearningRate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXbm0XVcR6b8"
   },
   "outputs": [],
   "source": [
    "seeded_value = 987258\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ybWWTMP8R6cA",
    "outputId": "c26e2379-92f7-4bcb-bd93-a07825d058f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 20 23:28:37 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ESteN4q4TAAW",
    "outputId": "a6428c7d-4959-486c-9c24-5dc489c8e04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR4HCYGTR6cG"
   },
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sJh7lbL1R6cH",
    "outputId": "68c74149-67dc-4a43-9d9d-8cec23e054d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iAdsW80fR6cL"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ8ZRVGTR6cO"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wQ_YumuXR6cO"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR+'jigsaw-toxic-comment-train.csv')\n",
    "validation = pd.read_csv(DATA_DIR+'validation.csv')\n",
    "test = pd.read_csv(DATA_DIR+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5Chd2u5AR6cR"
   },
   "outputs": [],
   "source": [
    "train['lang'] = 'en'\n",
    "\n",
    "train['set'] = 'train'\n",
    "validation['set'] = 'valid'\n",
    "test['set'] = 'test'\n",
    "\n",
    "test['toxic'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pZIaKjKXR6cU",
    "outputId": "f2b9d702-b64c-4838-bbcc-b92a243a8e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate', 'lang', 'set'],\n",
      "      dtype='object')\n",
      "Index(['id', 'comment_text', 'lang', 'toxic', 'set'], dtype='object')\n",
      "Index(['id', 'content', 'lang', 'set', 'toxic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(validation.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['es', 'it', 'tr'], dtype=object),\n",
       " array(['tr', 'ru', 'it', 'fr', 'pt', 'es'], dtype=object))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.lang.unique(), test.lang.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sNlk-kDDR6cY"
   },
   "outputs": [],
   "source": [
    "train.columns = ['id', 'text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'lang', 'set']\n",
    "validation.columns = ['id', 'text', 'lang', 'toxic', 'set']\n",
    "test.columns = ['id', 'text', 'lang', 'set', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "saymbsyYR6ca"
   },
   "outputs": [],
   "source": [
    "REQ_COLS = ['id', 'set', 'text', 'lang', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train[\"text\"].astype(str)\n",
    "validation['text'] = validation[\"text\"].astype(str)\n",
    "test['text'] = test[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2VCNIC95R6cf"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train[REQ_COLS].sample(SAMPLE_SIZE, random_state=seeded_value),\n",
    "                  validation[REQ_COLS]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-EPYDkCNR6ci",
    "outputId": "d36606e0-9c8d-4e2a-d2f8-82d4c9f15590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UlrDxcmLR6cl",
    "outputId": "7436086b-4453-44a1-d6be-627e2bd63171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>set</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164112</th>\n",
       "      <td>126f4ebe96739a72</td>\n",
       "      <td>train</td>\n",
       "      <td>\" \\n\\n *Support I have heard of the region before as Nakhichevan.    \\n *Support Nakhichevan is the common name of the area in English.   \"</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>4631</td>\n",
       "      <td>valid</td>\n",
       "      <td>¿Bastardilla?¿Pero no se decía cursiva?.Creo que sería mejor poner   cursiva  , en vez de   bastardilla  . Super</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id    set  \\\n",
       "164112  126f4ebe96739a72  train   \n",
       "4631                4631  valid   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "164112  \" \\n\\n *Support I have heard of the region before as Nakhichevan.    \\n *Support Nakhichevan is the common name of the area in English.   \"   \n",
       "4631                              ¿Bastardilla?¿Pero no se decía cursiva?.Creo que sería mejor poner   cursiva  , en vez de   bastardilla  . Super    \n",
       "\n",
       "       lang  toxic  \n",
       "164112   en      0  \n",
       "4631     es      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CpA5ibkQR6cn",
    "outputId": "dfd2a1f9-d403-43fb-edae-9ca8be5784dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <th>en</th>\n",
       "      <td>6000</td>\n",
       "      <td>0.092667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">valid</th>\n",
       "      <th>es</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.106667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     toxic\n",
       "set   lang                \n",
       "train en    6000  0.092667\n",
       "valid es    2500  0.168800\n",
       "      it    2500  0.195200\n",
       "      tr    3000  0.106667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "57X-txA_raVA",
    "outputId": "3830e4ae-e5f9-4ab5-a689-050b25913c48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">test</th>\n",
       "      <th>es</th>\n",
       "      <td>8438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>10920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>8494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>11012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>10948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>14000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  toxic\n",
       "set  lang              \n",
       "test es     8438      0\n",
       "     fr    10920      0\n",
       "     it     8494      0\n",
       "     pt    11012      0\n",
       "     ru    10948      0\n",
       "     tr    14000      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE>0:\n",
    "    data = data.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)\n",
    "    test = test.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LLX9UbtR6ct"
   },
   "source": [
    "# Tokenizer, Config & Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gnjg9vXYR6cu"
   },
   "source": [
    "1. https://arxiv.org/pdf/1911.02116.pdf\n",
    "2. https://huggingface.co/transformers/model_doc/xlmroberta.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5Z0gl2gTR6cu"
   },
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'xlm':\n",
    "    xlmr_tok = transformers.XLMRobertaTokenizer.from_pretrained(MODEL_DIR)\n",
    "else:\n",
    "    xlmr_tok = transformers.DistilBertTokenizer.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4ufXbNlyR6cy",
    "outputId": "58b13fa9-0347-40fe-9809-5a61db9ce3e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODEL_DIR+\"special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "xlmr_tok.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "W6qX9qMMR6c0",
    "outputId": "c81308f3-9c5b-4723-b4fc-fd7efdd76c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = xlmr_tok.vocab_size\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtEkdI5cR6c6"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens, X_att = [], []\n",
    "for t in data.text.tolist():\n",
    "    encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "    X_tokens.append(encoded_text['input_ids'])\n",
    "    X_att.append(encoded_text['attention_mask'])\n",
    "\n",
    "X_tokens, X_att, X_lang, Y_toxic = np.array(X_tokens), np.array(X_att), data['lang'].values, data['toxic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "s_cU7WAAxHjS",
    "outputId": "66058960-fec0-46a7-ecb5-a75bb7cbb6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (63812, 128) \t: X_tokens_test  \n",
      " (63812, 128) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens_test, X_att_test = [], []\n",
    "for t in test.text.tolist():\n",
    "    encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "    X_tokens_test.append(encoded_text['input_ids'])\n",
    "    X_att_test.append(encoded_text['attention_mask'])\n",
    "\n",
    "X_tokens_test, X_att_test, X_lang_test = np.array(X_tokens_test), np.array(X_att_test), test['lang'].values\n",
    "\n",
    "print(\"\\n\",\n",
    "      X_tokens_test.shape, \"\\t: X_tokens_test \", \"\\n\",\n",
    "      X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeR6DmO2R6dN"
   },
   "source": [
    "# Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PKWwyFjzR6dN"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    \n",
    "    if MODEL_NAME == 'xlm':\n",
    "        config = transformers.XLMRobertaConfig.from_pretrained(MODEL_DIR)\n",
    "        model = transformers.TFXLMRobertaModel.from_pretrained(MODEL_DIR, config=config) # TFXLMRobertaForSequenceClassification\n",
    "        x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    else:\n",
    "        config = transformers.DistilBertConfig.from_pretrained(MODEL_DIR)\n",
    "        model = transformers.TFDistilBertModel.from_pretrained(MODEL_DIR) # TFDistilBertForSequenceClassification\n",
    "        x = model(inputs=input_sequences, attention_mask=input_att_flags)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    toxic_output = tf.keras.layers.Activation('sigmoid', name=\"toxic_output\")(x1)\n",
    "    \n",
    "    model = Model([input_att_flags, input_sequences],\n",
    "                  [toxic_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "n-x9lurmR6dQ"
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ugXKFEnqR6dT",
    "outputId": "2c509dc7-da68-4adc-f9d6-9c9151b0fad8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB ((None, 128, 768),)  134734080   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 128, 768)     0           tf_distil_bert_model[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 128, 768)     1180416     dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 768)     3072        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128, 768)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128, 1)       769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 1)       4           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128, 1)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "toxic_output (Activation)       (None, 1)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 135,918,470\n",
      "Trainable params: 135,916,932\n",
      "Non-trainable params: 1,538\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KFold train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "kf = KFold(NUM_FOLDS, shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KFold Stratified train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUM_FOLDS = 3\n",
    "#skf = StratifiedKFold(NUM_FOLDS, shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one language out split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logo = LeaveOneGroupOut()\n",
    "#for t_index, v_index in logo.split(np.arange(X_tokens.shape[0]), np.arange(X_tokens.shape[0]), groups=X_lang):\n",
    "#    print(X_lang[t_index])\n",
    "#    #print(np.unique(X_lang[v_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple random train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_index, v_index = train_test_split(np.arange(X_tokens.shape[0]), shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeR6DmO2R6dN"
   },
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GGb6IwuR6dY"
   },
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = data[['id']]\n",
    "pred_df_test = test[['id']]\n",
    "timings_dict = {}\n",
    "cv_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 11200 samples, validate on 2800 samples\n",
      "Epoch 1/4\n",
      "11200/11200 [==============================] - 73s 7ms/sample - loss: 0.4529 - accuracy: 0.8691 - auc: 0.7358 - val_loss: 0.4058 - val_accuracy: 0.8775 - val_auc: 0.8230\n",
      "Epoch 2/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.4119 - accuracy: 0.8812 - auc: 0.8273 - val_loss: 0.3887 - val_accuracy: 0.8854 - val_auc: 0.8600\n",
      "Epoch 3/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.4035 - accuracy: 0.8866 - auc: 0.8424 - val_loss: 0.3797 - val_accuracy: 0.8914 - val_auc: 0.8731\n",
      "Epoch 4/4\n",
      "11200/11200 [==============================] - 67s 6ms/sample - loss: 0.3955 - accuracy: 0.8904 - auc: 0.8588 - val_loss: 0.3780 - val_accuracy: 0.8946 - val_auc: 0.8715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC:\t 0.8973039738164754\n",
      "Valid ROC-AUC:\t 0.8713572962341435\n",
      "Train Accuracy:\t 0.9008035714285715\n",
      "Valid Accuracy:\t 0.8946428571428572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94      9778\n",
      "           1       0.69      0.40      0.50      1422\n",
      "\n",
      "    accuracy                           0.90     11200\n",
      "   macro avg       0.80      0.69      0.72     11200\n",
      "weighted avg       0.89      0.90      0.89     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      2436\n",
      "           1       0.66      0.39      0.49       364\n",
      "\n",
      "    accuracy                           0.89      2800\n",
      "   macro avg       0.79      0.68      0.72      2800\n",
      "weighted avg       0.88      0.89      0.88      2800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSUEDO_PROB_THRESH_HIGH 0.602\n",
      "PSUEDO_PROB_THRESH_LOW 0.038\n",
      "Number of psuedo samples available: 6382\n",
      "Psuedo Toxicity: 3191\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'pt': 1485, 'es': 1266, 'tr': 1120, 'fr': 1110, 'it': 855, 'ru': 546})\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/8\n",
      "17582/17582 [==============================] - 122s 7ms/sample - loss: 0.3097 - accuracy: 0.9193 - auc: 0.9588 - val_loss: 0.3847 - val_accuracy: 0.8871 - val_auc: 0.8773\n",
      "Epoch 2/8\n",
      "17582/17582 [==============================] - 114s 7ms/sample - loss: 0.2999 - accuracy: 0.9249 - auc: 0.9633 - val_loss: 0.3764 - val_accuracy: 0.8982 - val_auc: 0.8778\n",
      "Epoch 3/8\n",
      "17582/17582 [==============================] - 115s 7ms/sample - loss: 0.2966 - accuracy: 0.9263 - auc: 0.9651 - val_loss: 0.3931 - val_accuracy: 0.8789 - val_auc: 0.8811\n",
      "Epoch 4/8\n",
      "17582/17582 [==============================] - 115s 7ms/sample - loss: 0.2950 - accuracy: 0.9258 - auc: 0.9664 - val_loss: 0.3798 - val_accuracy: 0.9007 - val_auc: 0.8821\n",
      "Epoch 5/8\n",
      "17582/17582 [==============================] - 115s 7ms/sample - loss: 0.2927 - accuracy: 0.9273 - auc: 0.9671 - val_loss: 0.3722 - val_accuracy: 0.8986 - val_auc: 0.8880- accuracy: 0.9272 - auc: \n",
      "Epoch 6/8\n",
      "17582/17582 [==============================] - 115s 7ms/sample - loss: 0.2907 - accuracy: 0.9300 - auc: 0.9689 - val_loss: 0.4013 - val_accuracy: 0.8661 - val_auc: 0.8897\n",
      "Epoch 7/8\n",
      "17582/17582 [==============================] - 113s 6ms/sample - loss: 0.2880 - accuracy: 0.9322 - auc: 0.9706 - val_loss: 0.3809 - val_accuracy: 0.8900 - val_auc: 0.8883\n",
      "Epoch 8/8\n",
      "17582/17582 [==============================] - 113s 6ms/sample - loss: 0.2877 - accuracy: 0.9309 - auc: 0.9711 - val_loss: 0.3783 - val_accuracy: 0.8986 - val_auc: 0.8833\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/2\n",
      "17582/17582 [==============================] - 320s 18ms/sample - loss: 0.2831 - accuracy: 0.9344 - auc: 0.9756 - val_loss: 0.3708 - val_accuracy: 0.9007 - val_auc: 0.8918\n",
      "Epoch 2/2\n",
      "17582/17582 [==============================] - 309s 18ms/sample - loss: 0.2796 - accuracy: 0.9360 - auc: 0.9776 - val_loss: 0.3691 - val_accuracy: 0.9007 - val_auc: 0.8932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC:\t 0.9247797230730372\n",
      "Valid ROC-AUC:\t 0.8931717912629243\n",
      "Train Accuracy:\t 0.9069642857142857\n",
      "Valid Accuracy:\t 0.9007142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      9778\n",
      "           1       0.68      0.50      0.58      1422\n",
      "\n",
      "    accuracy                           0.91     11200\n",
      "   macro avg       0.81      0.73      0.76     11200\n",
      "weighted avg       0.90      0.91      0.90     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      2436\n",
      "           1       0.65      0.52      0.58       364\n",
      "\n",
      "    accuracy                           0.90      2800\n",
      "   macro avg       0.79      0.74      0.76      2800\n",
      "weighted avg       0.89      0.90      0.90      2800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:165: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 1 ====================\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 11200 samples, validate on 2800 samples\n",
      "Epoch 1/4\n",
      "11200/11200 [==============================] - 72s 6ms/sample - loss: 0.4346 - accuracy: 0.8737 - auc: 0.7836 - val_loss: 0.4038 - val_accuracy: 0.8832 - val_auc: 0.8363\n",
      "Epoch 2/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.4069 - accuracy: 0.8863 - auc: 0.8408 - val_loss: 0.3911 - val_accuracy: 0.8839 - val_auc: 0.8398\n",
      "Epoch 3/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.3988 - accuracy: 0.8886 - auc: 0.8568 - val_loss: 0.3829 - val_accuracy: 0.8954 - val_auc: 0.8507\n",
      "Epoch 4/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.3919 - accuracy: 0.8909 - auc: 0.8717 - val_loss: 0.3954 - val_accuracy: 0.8836 - val_auc: 0.8411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC:\t 0.895617303253368\n",
      "Valid ROC-AUC:\t 0.8414618420917497\n",
      "Train Accuracy:\t 0.8961607142857143\n",
      "Valid Accuracy:\t 0.8835714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      9773\n",
      "           1       0.70      0.32      0.44      1427\n",
      "\n",
      "    accuracy                           0.90     11200\n",
      "   macro avg       0.80      0.65      0.69     11200\n",
      "weighted avg       0.88      0.90      0.88     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94      2441\n",
      "           1       0.59      0.29      0.39       359\n",
      "\n",
      "    accuracy                           0.88      2800\n",
      "   macro avg       0.75      0.63      0.66      2800\n",
      "weighted avg       0.86      0.88      0.87      2800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSUEDO_PROB_THRESH_HIGH 0.539\n",
      "PSUEDO_PROB_THRESH_LOW 0.009\n",
      "Number of psuedo samples available: 6382\n",
      "Psuedo Toxicity: 3191\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'ru': 1784, 'es': 1472, 'tr': 1334, 'fr': 708, 'it': 591, 'pt': 493})\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/8\n",
      "17582/17582 [==============================] - 122s 7ms/sample - loss: 0.3137 - accuracy: 0.9151 - auc: 0.9586 - val_loss: 0.3922 - val_accuracy: 0.8882 - val_auc: 0.8450\n",
      "Epoch 2/8\n",
      "17582/17582 [==============================] - 114s 6ms/sample - loss: 0.3089 - accuracy: 0.9161 - auc: 0.9595 - val_loss: 0.3947 - val_accuracy: 0.8814 - val_auc: 0.8572\n",
      "Epoch 3/8\n",
      "17582/17582 [==============================] - 112s 6ms/sample - loss: 0.3091 - accuracy: 0.9182 - auc: 0.9593 - val_loss: 0.3931 - val_accuracy: 0.8832 - val_auc: 0.8558\n",
      "Epoch 4/8\n",
      "17582/17582 [==============================] - 114s 6ms/sample - loss: 0.3028 - accuracy: 0.9232 - auc: 0.9629 - val_loss: 0.3997 - val_accuracy: 0.8775 - val_auc: 0.8476\n",
      "Epoch 5/8\n",
      "17582/17582 [==============================] - 114s 7ms/sample - loss: 0.3035 - accuracy: 0.9205 - auc: 0.9631 - val_loss: 0.3892 - val_accuracy: 0.8868 - val_auc: 0.8588\n",
      "Epoch 6/8\n",
      "17582/17582 [==============================] - 112s 6ms/sample - loss: 0.3014 - accuracy: 0.9239 - auc: 0.9644 - val_loss: 0.3900 - val_accuracy: 0.8839 - val_auc: 0.8583\n",
      "Epoch 7/8\n",
      "17582/17582 [==============================] - 114s 6ms/sample - loss: 0.2996 - accuracy: 0.9236 - auc: 0.9658 - val_loss: 0.3944 - val_accuracy: 0.8843 - val_auc: 0.8597\n",
      "Epoch 8/8\n",
      "17582/17582 [==============================] - 113s 6ms/sample - loss: 0.2993 - accuracy: 0.9237 - auc: 0.9659 - val_loss: 0.3911 - val_accuracy: 0.8886 - val_auc: 0.8503.3005 - accuracy: 0.923 - ETA: 28s - loss: 0.3003 - accuracy: 0.9237 - auc: 0.964 - ETA: 28s - lo - ETA: 24s - loss: 0.3005 - accuracy: 0.9243 - auc: - ETA: 23s - loss: 0.3001 - accuracy: - ETA: 16s - loss: 0.3005 - accuracy: 0 - ET - ETA: 1s - loss: 0.2995 - accuracy: 0.9240 - ETA: 1s - loss: 0.2993 - accu\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/2\n",
      "17582/17582 [==============================] - 318s 18ms/sample - loss: 0.2935 - accuracy: 0.9288 - auc: 0.9698 - val_loss: 0.3862 - val_accuracy: 0.8875 - val_auc: 0.8605\n",
      "Epoch 2/2\n",
      "17582/17582 [==============================] - 308s 18ms/sample - loss: 0.2938 - accuracy: 0.9290 - auc: 0.9694 - val_loss: 0.3868 - val_accuracy: 0.8861 - val_auc: 0.8620\n",
      "Train ROC-AUC:\t 0.9205598480030683\n",
      "Valid ROC-AUC:\t 0.8620342592138251\n",
      "Train Accuracy:\t 0.9029464285714286\n",
      "Valid Accuracy:\t 0.8860714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.95      9773\n",
      "           1       0.64      0.53      0.58      1427\n",
      "\n",
      "    accuracy                           0.90     11200\n",
      "   macro avg       0.79      0.74      0.76     11200\n",
      "weighted avg       0.90      0.90      0.90     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      2441\n",
      "           1       0.56      0.49      0.52       359\n",
      "\n",
      "    accuracy                           0.89      2800\n",
      "   macro avg       0.75      0.72      0.73      2800\n",
      "weighted avg       0.88      0.89      0.88      2800\n",
      "\n",
      "[INFO] ==================== FOLD# 2 ====================\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 11200 samples, validate on 2800 samples\n",
      "Epoch 1/4\n",
      "11200/11200 [==============================] - 72s 6ms/sample - loss: 0.4397 - accuracy: 0.8706 - auc: 0.7734 - val_loss: 0.4036 - val_accuracy: 0.8764 - val_auc: 0.8361\n",
      "Epoch 2/4\n",
      "11200/11200 [==============================] - 67s 6ms/sample - loss: 0.4084 - accuracy: 0.8837 - auc: 0.8417 - val_loss: 0.3822 - val_accuracy: 0.8925 - val_auc: 0.8542\n",
      "Epoch 3/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.3990 - accuracy: 0.8864 - auc: 0.8588 - val_loss: 0.3857 - val_accuracy: 0.8900 - val_auc: 0.8548\n",
      "Epoch 4/4\n",
      "11200/11200 [==============================] - 67s 6ms/sample - loss: 0.3952 - accuracy: 0.8912 - auc: 0.8655 - val_loss: 0.3815 - val_accuracy: 0.8846 - val_auc: 0.8610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC:\t 0.9042027080751016\n",
      "Valid ROC-AUC:\t 0.8611318452882585\n",
      "Train Accuracy:\t 0.8951785714285714\n",
      "Valid Accuracy:\t 0.8846428571428572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94      9771\n",
      "           1       0.64      0.41      0.50      1429\n",
      "\n",
      "    accuracy                           0.90     11200\n",
      "   macro avg       0.78      0.69      0.72     11200\n",
      "weighted avg       0.88      0.90      0.88     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      2443\n",
      "           1       0.57      0.37      0.45       357\n",
      "\n",
      "    accuracy                           0.88      2800\n",
      "   macro avg       0.74      0.67      0.69      2800\n",
      "weighted avg       0.87      0.88      0.87      2800\n",
      "\n",
      "PSUEDO_PROB_THRESH_HIGH 0.599\n",
      "PSUEDO_PROB_THRESH_LOW 0.023\n",
      "Number of psuedo samples available: 6382\n",
      "Psuedo Toxicity: 3191\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'tr': 1565, 'es': 1281, 'pt': 1146, 'fr': 1112, 'it': 860, 'ru': 418})\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/8\n",
      "17582/17582 [==============================] - 123s 7ms/sample - loss: 0.3081 - accuracy: 0.9194 - auc: 0.9606 - val_loss: 0.4282 - val_accuracy: 0.8568 - val_auc: 0.8664\n",
      "Epoch 2/8\n",
      "17582/17582 [==============================] - 116s 7ms/sample - loss: 0.2974 - accuracy: 0.9238 - auc: 0.9648 - val_loss: 0.3866 - val_accuracy: 0.8850 - val_auc: 0.8675\n",
      "Epoch 3/8\n",
      "17582/17582 [==============================] - 113s 6ms/sample - loss: 0.2952 - accuracy: 0.9241 - auc: 0.9656 - val_loss: 0.3912 - val_accuracy: 0.8907 - val_auc: 0.8628\n",
      "Epoch 4/8\n",
      "17582/17582 [==============================] - 114s 7ms/sample - loss: 0.2938 - accuracy: 0.9271 - auc: 0.9668 - val_loss: 0.3833 - val_accuracy: 0.8839 - val_auc: 0.8671\n",
      "Epoch 5/8\n",
      "17582/17582 [==============================] - 116s 7ms/sample - loss: 0.2898 - accuracy: 0.9302 - auc: 0.9697 - val_loss: 0.3886 - val_accuracy: 0.8789 - val_auc: 0.8691\n",
      "Epoch 6/8\n",
      "17582/17582 [==============================] - 114s 6ms/sample - loss: 0.2882 - accuracy: 0.9299 - auc: 0.9702 - val_loss: 0.3852 - val_accuracy: 0.8886 - val_auc: 0.8676\n",
      "Epoch 7/8\n",
      "17582/17582 [==============================] - 114s 6ms/sample - loss: 0.2873 - accuracy: 0.9308 - auc: 0.9711 - val_loss: 0.3861 - val_accuracy: 0.8836 - val_auc: 0.8711\n",
      "Epoch 8/8\n",
      "17582/17582 [==============================] - 112s 6ms/sample - loss: 0.2855 - accuracy: 0.9315 - auc: 0.9728 - val_loss: 0.3914 - val_accuracy: 0.8793 - val_auc: 0.8701\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/2\n",
      "17582/17582 [==============================] - 318s 18ms/sample - loss: 0.2795 - accuracy: 0.9372 - auc: 0.9767 - val_loss: 0.3817 - val_accuracy: 0.8886 - val_auc: 0.8749\n",
      "Epoch 2/2\n",
      "17582/17582 [==============================] - 310s 18ms/sample - loss: 0.2778 - accuracy: 0.9378 - auc: 0.9780 - val_loss: 0.3802 - val_accuracy: 0.8889 - val_auc: 0.8759\n",
      "Train ROC-AUC:\t 0.9277415731375154\n",
      "Valid ROC-AUC:\t 0.8759125426674967\n",
      "Train Accuracy:\t 0.9091071428571429\n",
      "Valid Accuracy:\t 0.8889285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      9771\n",
      "           1       0.69      0.52      0.59      1429\n",
      "\n",
      "    accuracy                           0.91     11200\n",
      "   macro avg       0.81      0.74      0.77     11200\n",
      "weighted avg       0.90      0.91      0.90     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94      2443\n",
      "           1       0.58      0.46      0.52       357\n",
      "\n",
      "    accuracy                           0.89      2800\n",
      "   macro avg       0.75      0.71      0.73      2800\n",
      "weighted avg       0.88      0.89      0.88      2800\n",
      "\n",
      "[INFO] ==================== FOLD# 3 ====================\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 11200 samples, validate on 2800 samples\n",
      "Epoch 1/4\n",
      "11200/11200 [==============================] - 72s 6ms/sample - loss: 0.4536 - accuracy: 0.8645 - auc: 0.7289 - val_loss: 0.4002 - val_accuracy: 0.8821 - val_auc: 0.8396\n",
      "Epoch 2/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.4117 - accuracy: 0.8829 - auc: 0.8310 - val_loss: 0.3966 - val_accuracy: 0.8796 - val_auc: 0.8532\n",
      "Epoch 3/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.4022 - accuracy: 0.8882 - auc: 0.8509 - val_loss: 0.3819 - val_accuracy: 0.8843 - val_auc: 0.8710\n",
      "Epoch 4/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.3961 - accuracy: 0.8876 - auc: 0.8612 - val_loss: 0.3757 - val_accuracy: 0.8889 - val_auc: 0.8772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC:\t 0.8960157580652399\n",
      "Valid ROC-AUC:\t 0.8772554484547616\n",
      "Train Accuracy:\t 0.8947321428571429\n",
      "Valid Accuracy:\t 0.8889285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      9775\n",
      "           1       0.68      0.32      0.44      1425\n",
      "\n",
      "    accuracy                           0.89     11200\n",
      "   macro avg       0.80      0.65      0.69     11200\n",
      "weighted avg       0.88      0.89      0.88     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      2439\n",
      "           1       0.66      0.29      0.40       361\n",
      "\n",
      "    accuracy                           0.89      2800\n",
      "   macro avg       0.78      0.63      0.67      2800\n",
      "weighted avg       0.87      0.89      0.87      2800\n",
      "\n",
      "PSUEDO_PROB_THRESH_HIGH 0.535\n",
      "PSUEDO_PROB_THRESH_LOW 0.025\n",
      "Number of psuedo samples available: 6382\n",
      "Psuedo Toxicity: 3191\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'tr': 2161, 'es': 1235, 'fr': 1040, 'ru': 700, 'pt': 672, 'it': 574})\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/8\n",
      "17582/17582 [==============================] - 122s 7ms/sample - loss: 0.3099 - accuracy: 0.9203 - auc: 0.9586 - val_loss: 0.3956 - val_accuracy: 0.8896 - val_auc: 0.8587 auc\n",
      "Epoch 2/8\n",
      "17582/17582 [==============================] - 114s 6ms/sample - loss: 0.3017 - accuracy: 0.9245 - auc: 0.9613 - val_loss: 0.3893 - val_accuracy: 0.8829 - val_auc: 0.8687\n",
      "Epoch 3/8\n",
      "17582/17582 [==============================] - 114s 6ms/sample - loss: 0.2993 - accuracy: 0.9242 - auc: 0.9630 - val_loss: 0.3860 - val_accuracy: 0.8850 - val_auc: 0.8827\n",
      "Epoch 4/8\n",
      "17582/17582 [==============================] - 113s 6ms/sample - loss: 0.2959 - accuracy: 0.9279 - auc: 0.9658 - val_loss: 0.3991 - val_accuracy: 0.8811 - val_auc: 0.8763\n",
      "Epoch 5/8\n",
      "17582/17582 [==============================] - 112s 6ms/sample - loss: 0.2942 - accuracy: 0.9284 - auc: 0.9669 - val_loss: 0.3878 - val_accuracy: 0.8875 - val_auc: 0.86016 -  - ETA: 1s - loss: 0.2941 - accura\n",
      "Epoch 6/8\n",
      "17582/17582 [==============================] - 112s 6ms/sample - loss: 0.2915 - accuracy: 0.9307 - auc: 0.9684 - val_loss: 0.4041 - val_accuracy: 0.8768 - val_auc: 0.8802\n",
      "Epoch 7/8\n",
      "17582/17582 [==============================] - 114s 6ms/sample - loss: 0.2901 - accuracy: 0.9311 - auc: 0.9694 - val_loss: 0.3782 - val_accuracy: 0.8861 - val_auc: 0.8842\n",
      "Epoch 8/8\n",
      "17582/17582 [==============================] - 116s 7ms/sample - loss: 0.2883 - accuracy: 0.9320 - auc: 0.9704 - val_loss: 0.3816 - val_accuracy: 0.8904 - val_auc: 0.8868\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/2\n",
      "17582/17582 [==============================] - 318s 18ms/sample - loss: 0.2830 - accuracy: 0.9357 - auc: 0.9746 - val_loss: 0.3762 - val_accuracy: 0.8896 - val_auc: 0.8908\n",
      "Epoch 2/2\n",
      "17582/17582 [==============================] - 309s 18ms/sample - loss: 0.2826 - accuracy: 0.9356 - auc: 0.9749 - val_loss: 0.3760 - val_accuracy: 0.8904 - val_auc: 0.8908\n",
      "Train ROC-AUC:\t 0.921756862745098\n",
      "Valid ROC-AUC:\t 0.8906538372862953\n",
      "Train Accuracy:\t 0.9064285714285715\n",
      "Valid Accuracy:\t 0.8903571428571428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      9775\n",
      "           1       0.68      0.49      0.57      1425\n",
      "\n",
      "    accuracy                           0.91     11200\n",
      "   macro avg       0.81      0.73      0.76     11200\n",
      "weighted avg       0.90      0.91      0.90     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      2439\n",
      "           1       0.60      0.45      0.51       361\n",
      "\n",
      "    accuracy                           0.89      2800\n",
      "   macro avg       0.76      0.70      0.73      2800\n",
      "weighted avg       0.88      0.89      0.88      2800\n",
      "\n",
      "[INFO] ==================== FOLD# 4 ====================\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 11200 samples, validate on 2800 samples\n",
      "Epoch 1/4\n",
      "11200/11200 [==============================] - 72s 6ms/sample - loss: 0.4514 - accuracy: 0.8599 - auc: 0.7615 - val_loss: 0.4020 - val_accuracy: 0.8821 - val_auc: 0.8243\n",
      "Epoch 2/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.4122 - accuracy: 0.8871 - auc: 0.8339 - val_loss: 0.3983 - val_accuracy: 0.8836 - val_auc: 0.8344\n",
      "Epoch 3/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.4030 - accuracy: 0.8879 - auc: 0.8532 - val_loss: 0.3972 - val_accuracy: 0.8861 - val_auc: 0.8310\n",
      "Epoch 4/4\n",
      "11200/11200 [==============================] - 66s 6ms/sample - loss: 0.3959 - accuracy: 0.8903 - auc: 0.8645 - val_loss: 0.3880 - val_accuracy: 0.8921 - val_auc: 0.8435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC:\t 0.8973524252315644\n",
      "Valid ROC-AUC:\t 0.84367779450397\n",
      "Train Accuracy:\t 0.8961607142857143\n",
      "Valid Accuracy:\t 0.8921428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      9759\n",
      "           1       0.70      0.34      0.46      1441\n",
      "\n",
      "    accuracy                           0.90     11200\n",
      "   macro avg       0.80      0.66      0.70     11200\n",
      "weighted avg       0.88      0.90      0.88     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      2455\n",
      "           1       0.65      0.27      0.38       345\n",
      "\n",
      "    accuracy                           0.89      2800\n",
      "   macro avg       0.78      0.62      0.66      2800\n",
      "weighted avg       0.87      0.89      0.87      2800\n",
      "\n",
      "PSUEDO_PROB_THRESH_HIGH 0.573\n",
      "PSUEDO_PROB_THRESH_LOW 0.019\n",
      "Number of psuedo samples available: 6382\n",
      "Psuedo Toxicity: 3191\n",
      "Counter({'tr': 14000, 'pt': 11012, 'ru': 10948, 'fr': 10920, 'it': 8494, 'es': 8438})\n",
      "Counter({'tr': 2386, 'fr': 1152, 'es': 1057, 'pt': 858, 'it': 493, 'ru': 436})\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/8\n",
      "17582/17582 [==============================] - 122s 7ms/sample - loss: 0.3073 - accuracy: 0.9233 - auc: 0.9607 - val_loss: 0.3831 - val_accuracy: 0.8918 - val_auc: 0.8596\n",
      "Epoch 2/8\n",
      "17582/17582 [==============================] - 113s 6ms/sample - loss: 0.2978 - accuracy: 0.9284 - auc: 0.9644 - val_loss: 0.4045 - val_accuracy: 0.8925 - val_auc: 0.8488\n",
      "Epoch 3/8\n",
      "17582/17582 [==============================] - 114s 6ms/sample - loss: 0.2965 - accuracy: 0.9286 - auc: 0.9645 - val_loss: 0.3865 - val_accuracy: 0.8907 - val_auc: 0.8543\n",
      "Epoch 4/8\n",
      "17582/17582 [==============================] - 112s 6ms/sample - loss: 0.2917 - accuracy: 0.9317 - auc: 0.9684 - val_loss: 0.3885 - val_accuracy: 0.8821 - val_auc: 0.8552\n",
      "Epoch 5/8\n",
      "17582/17582 [==============================] - 114s 7ms/sample - loss: 0.2901 - accuracy: 0.9319 - auc: 0.9697 - val_loss: 0.3919 - val_accuracy: 0.8914 - val_auc: 0.8693\n",
      "Epoch 6/8\n",
      "17582/17582 [==============================] - 113s 6ms/sample - loss: 0.2898 - accuracy: 0.9312 - auc: 0.9702 - val_loss: 0.3807 - val_accuracy: 0.8857 - val_auc: 0.8660\n",
      "Epoch 7/8\n",
      "17582/17582 [==============================] - 113s 6ms/sample - loss: 0.2869 - accuracy: 0.9331 - auc: 0.9714 - val_loss: 0.3806 - val_accuracy: 0.8943 - val_auc: 0.8646\n",
      "Epoch 8/8\n",
      "17582/17582 [==============================] - 113s 6ms/sample - loss: 0.2857 - accuracy: 0.9345 - auc: 0.9725 - val_loss: 0.3823 - val_accuracy: 0.8950 - val_auc: 0.8639\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 17582 samples, validate on 2800 samples\n",
      "Epoch 1/2\n",
      "17582/17582 [==============================] - 317s 18ms/sample - loss: 0.2814 - accuracy: 0.9374 - auc: 0.9756 - val_loss: 0.3775 - val_accuracy: 0.8925 - val_auc: 0.8664\n",
      "Epoch 2/2\n",
      "17582/17582 [==============================] - 308s 17ms/sample - loss: 0.2798 - accuracy: 0.9382 - auc: 0.9772 - val_loss: 0.3772 - val_accuracy: 0.8946 - val_auc: 0.8673\n",
      "Train ROC-AUC:\t 0.9168219886922294\n",
      "Valid ROC-AUC:\t 0.8699229611263615\n",
      "Train Accuracy:\t 0.900625\n",
      "Valid Accuracy:\t 0.8914285714285715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      9759\n",
      "           1       0.72      0.38      0.49      1441\n",
      "\n",
      "    accuracy                           0.90     11200\n",
      "   macro avg       0.81      0.68      0.72     11200\n",
      "weighted avg       0.89      0.90      0.89     11200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      2455\n",
      "           1       0.64      0.28      0.38       345\n",
      "\n",
      "    accuracy                           0.89      2800\n",
      "   macro avg       0.77      0.63      0.66      2800\n",
      "weighted avg       0.87      0.89      0.87      2800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, (t_index, v_index) in enumerate(kf.split(X_tokens, Y_toxic)):\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    start_time = time()\n",
    "\n",
    "    if num>0:\n",
    "        del model\n",
    "        del mcp\n",
    "        del csvl\n",
    "        del adam\n",
    "        del history\n",
    "        del auc\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "    model = build_model()\n",
    "    auc = tf.keras.metrics.AUC(name='auc')\n",
    "    mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_auc',\n",
    "                          verbose=0, save_best_only=True, save_weights_only=True, mode='max', save_freq='epoch')\n",
    "    csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                     separator=\",\", append=True)\n",
    "\n",
    "    model.layers[2].trainable = False\n",
    "    adam = Adam(learning_rate=LR)\n",
    "    model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy', auc])\n",
    "\n",
    "    train_time = time()\n",
    "    history = model.fit(x={\"att_flags\":X_att[t_index],\n",
    "                           \"words\":X_tokens[t_index]},\n",
    "                        y={\"toxic_output\":Y_toxic[t_index]},\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X_tokens[v_index]},\n",
    "                                         {\"toxic_output\":Y_toxic[v_index]}),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[0],\n",
    "                        shuffle=True,\n",
    "                        verbose=1, \n",
    "                        sample_weight=class_weight.compute_sample_weight('balanced', X_lang[t_index]),\n",
    "                        callbacks=[csvl])\n",
    "\n",
    "    pred_initial = model.predict(x = {\"att_flags\":X_att,\n",
    "                                      \"words\":X_tokens},\n",
    "                                 batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_df['fold_'+str(num)] = 0\n",
    "    pred_df['fold_'+str(num)].iloc[t_index] = 'train'\n",
    "    pred_df['fold_'+str(num)].iloc[v_index] = 'valid'\n",
    "    pred_df['initial_'+str(num)] = pred_initial\n",
    "    \n",
    "    roc_t = roc_auc_score(y_true=Y_toxic[t_index], y_score=pred_initial[t_index])\n",
    "    roc_v = roc_auc_score(y_true=Y_toxic[v_index], y_score=pred_initial[v_index])\n",
    "    acc_t = accuracy_score(y_true=Y_toxic[t_index], y_pred=np.where(pred_initial[t_index]>0.5, 1, 0))\n",
    "    acc_v = accuracy_score(y_true=Y_toxic[v_index], y_pred=np.where(pred_initial[v_index]>0.5, 1, 0))\n",
    "    \n",
    "    print(\"Train ROC-AUC:\\t\", roc_t)\n",
    "    print(\"Valid ROC-AUC:\\t\", roc_v)\n",
    "    print(\"Train Accuracy:\\t\", acc_t)\n",
    "    print(\"Valid Accuracy:\\t\", acc_v)\n",
    "\n",
    "    print(classification_report(y_true=Y_toxic[t_index], y_pred=np.where(pred_initial[t_index]>0.5, 1, 0)))\n",
    "    print(classification_report(y_true=Y_toxic[v_index], y_pred=np.where(pred_initial[v_index]>0.5, 1, 0)))\n",
    "\n",
    "    # Psuedo model fit\n",
    "    psuedo_time = time()\n",
    "\n",
    "    # Accumulate test results after training every fold\n",
    "    pred_psuedo = model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                     \"words\":X_tokens_test},\n",
    "                                batch_size=PREDICT_BATCH_SIZE).reshape((-1))\n",
    "\n",
    "    pred_df_test['psuedo_'+str(num)] = pred_psuedo\n",
    "    PSUEDO_PROB_THRESH_HIGH = pred_df_test['psuedo_'+str(num)].quantile(PSUEDO_QUANTILE_THRESH_HIGH)\n",
    "    PSUEDO_PROB_THRESH_LOW = pred_df_test['psuedo_'+str(num)].quantile(PSUEDO_QUANTILE_THRESH_LOW)\n",
    "    \n",
    "    print(\"PSUEDO_PROB_THRESH_HIGH\", np.round(PSUEDO_PROB_THRESH_HIGH,3))\n",
    "    print(\"PSUEDO_PROB_THRESH_LOW\", np.round(PSUEDO_PROB_THRESH_LOW,3))\n",
    "\n",
    "    Y_toxic_psuedo = np.where(pred_psuedo >= PSUEDO_PROB_THRESH_HIGH, 1, 0)\n",
    "    psuedo_flag = (pred_psuedo >= PSUEDO_PROB_THRESH_HIGH) | (pred_psuedo <= PSUEDO_PROB_THRESH_LOW)\n",
    "\n",
    "    print(\"Number of psuedo samples available:\", sum(psuedo_flag))\n",
    "    print(\"Psuedo Toxicity:\", sum(Y_toxic_psuedo))\n",
    "    print(Counter(test.lang.values))\n",
    "    print(Counter(test.lang.values[psuedo_flag]))\n",
    "\n",
    "    X_att_psuedo = np.concatenate((X_att[t_index], X_att_test[psuedo_flag]))\n",
    "    X_tokens_psuedo = np.concatenate((X_tokens[t_index], X_tokens_test[psuedo_flag]))\n",
    "    Y_toxic_psuedo = np.concatenate((Y_toxic[t_index], Y_toxic_psuedo[psuedo_flag]))\n",
    "    X_lang_psuedo = np.concatenate((X_lang[t_index], X_lang_test[psuedo_flag]))\n",
    "\n",
    "    shuffled_idxs = np.arange(Y_toxic_psuedo.shape[0])\n",
    "    np.random.shuffle(shuffled_idxs)\n",
    "\n",
    "    model.layers[2].trainable = False\n",
    "    adam = Adam(learning_rate=LR)\n",
    "    model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy', auc])\n",
    "\n",
    "    history = model.fit(x={\"att_flags\":X_att_psuedo[shuffled_idxs],\n",
    "                           \"words\":X_tokens_psuedo[shuffled_idxs]},\n",
    "                        y={\"toxic_output\":Y_toxic_psuedo[shuffled_idxs]},\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X_tokens[v_index]},\n",
    "                                         {\"toxic_output\":Y_toxic[v_index]}),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[1],\n",
    "                        shuffle=True,\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl], \n",
    "                        sample_weight=class_weight.compute_sample_weight('balanced', X_lang_psuedo[shuffled_idxs]))\n",
    "\n",
    "    if NUM_EPOCHS[2]>0:\n",
    "        model.layers[2].trainable = True\n",
    "        adam = Adam(learning_rate=LR*0.0001)\n",
    "        model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                      optimizer=adam,\n",
    "                      metrics=['accuracy', auc])\n",
    "\n",
    "        history = model.fit(x={\"att_flags\":X_att_psuedo[shuffled_idxs],\n",
    "                               \"words\":X_tokens_psuedo[shuffled_idxs]},\n",
    "                            y={\"toxic_output\":Y_toxic_psuedo[shuffled_idxs]},\n",
    "                            validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                              \"words\":X_tokens[v_index]},\n",
    "                                             {\"toxic_output\":Y_toxic[v_index]}),\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            epochs=NUM_EPOCHS[2],\n",
    "                            shuffle=True,\n",
    "                            verbose=1,\n",
    "                            callbacks=[mcp, csvl], \n",
    "                            sample_weight=class_weight.compute_sample_weight('balanced',X_lang_psuedo[shuffled_idxs]))\n",
    "\n",
    "    infer_time = time()\n",
    "\n",
    "    # Loading best weights per fold\n",
    "    model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")\n",
    "\n",
    "    pred = model.predict(x = {\"att_flags\":X_att,\n",
    "                                    \"words\":X_tokens},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_df['final_'+str(num)] = pred\n",
    "\n",
    "    final_roc_t = roc_auc_score(y_true=Y_toxic[t_index], y_score=pred[t_index])\n",
    "    final_roc_v = roc_auc_score(y_true=Y_toxic[v_index], y_score=pred[v_index])\n",
    "    final_acc_t = accuracy_score(y_true=Y_toxic[t_index], y_pred=np.where(pred[t_index]>0.5, 1, 0))\n",
    "    final_acc_v = accuracy_score(y_true=Y_toxic[v_index], y_pred=np.where(pred[v_index]>0.5, 1, 0))    \n",
    "    \n",
    "    print(\"Train ROC-AUC:\\t\", final_roc_t)\n",
    "    print(\"Valid ROC-AUC:\\t\", final_roc_v)\n",
    "    print(\"Train Accuracy:\\t\", final_acc_t)\n",
    "    print(\"Valid Accuracy:\\t\", final_acc_v)\n",
    "\n",
    "    print(classification_report(y_true=Y_toxic[t_index], y_pred=np.where(pred[t_index]>0.5, 1, 0)))\n",
    "    print(classification_report(y_true=Y_toxic[v_index], y_pred=np.where(pred[v_index]>0.5, 1, 0)))\n",
    "\n",
    "    # Accumulate test results after training every fold\n",
    "\n",
    "    pred_test = model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                    \"words\":X_tokens_test},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_df_test['final_'+str(num)] = pred_test\n",
    "\n",
    "    end_time = time()\n",
    "    timings_dict.update({num:{\n",
    "        'start_time' : ctime(start_time),\n",
    "        'train_time' : ctime(train_time),\n",
    "        'infer_time' : ctime(infer_time),\n",
    "        'psuedo_time' : ctime(psuedo_time),\n",
    "        'end_time' : ctime(end_time),\n",
    "    }})\n",
    "    \n",
    "    cv_stats.update({num:{\n",
    "        'preliminary_roc_t':roc_t,\n",
    "        'final_roc_t':final_roc_t,\n",
    "        'preliminary_roc_v':roc_v,\n",
    "        'final_roc_v':final_roc_v,\n",
    "        'preliminary_acc_t':acc_t,\n",
    "        'final_acc_t':final_acc_t,\n",
    "        'preliminary_acc_v':acc_v,\n",
    "        'final_acc_v':final_acc_v,\n",
    "    }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start_time</th>\n",
       "      <td>Sat Jun 20 23:30:58 2020</td>\n",
       "      <td>Sun Jun 21 00:11:55 2020</td>\n",
       "      <td>Sun Jun 21 00:52:44 2020</td>\n",
       "      <td>Sun Jun 21 01:33:59 2020</td>\n",
       "      <td>Sun Jun 21 02:14:50 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>Sat Jun 20 23:31:00 2020</td>\n",
       "      <td>Sun Jun 21 00:11:58 2020</td>\n",
       "      <td>Sun Jun 21 00:52:47 2020</td>\n",
       "      <td>Sun Jun 21 01:34:02 2020</td>\n",
       "      <td>Sun Jun 21 02:14:52 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infer_time</th>\n",
       "      <td>Sun Jun 21 00:06:40 2020</td>\n",
       "      <td>Sun Jun 21 00:47:28 2020</td>\n",
       "      <td>Sun Jun 21 01:28:44 2020</td>\n",
       "      <td>Sun Jun 21 02:09:33 2020</td>\n",
       "      <td>Sun Jun 21 02:50:20 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psuedo_time</th>\n",
       "      <td>Sat Jun 20 23:36:30 2020</td>\n",
       "      <td>Sun Jun 21 00:17:27 2020</td>\n",
       "      <td>Sun Jun 21 00:58:16 2020</td>\n",
       "      <td>Sun Jun 21 01:39:30 2020</td>\n",
       "      <td>Sun Jun 21 02:20:22 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_time</th>\n",
       "      <td>Sun Jun 21 00:11:55 2020</td>\n",
       "      <td>Sun Jun 21 00:52:44 2020</td>\n",
       "      <td>Sun Jun 21 01:33:59 2020</td>\n",
       "      <td>Sun Jun 21 02:14:50 2020</td>\n",
       "      <td>Sun Jun 21 02:55:37 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0                         1  \\\n",
       "start_time   Sat Jun 20 23:30:58 2020  Sun Jun 21 00:11:55 2020   \n",
       "train_time   Sat Jun 20 23:31:00 2020  Sun Jun 21 00:11:58 2020   \n",
       "infer_time   Sun Jun 21 00:06:40 2020  Sun Jun 21 00:47:28 2020   \n",
       "psuedo_time  Sat Jun 20 23:36:30 2020  Sun Jun 21 00:17:27 2020   \n",
       "end_time     Sun Jun 21 00:11:55 2020  Sun Jun 21 00:52:44 2020   \n",
       "\n",
       "                                    2                         3  \\\n",
       "start_time   Sun Jun 21 00:52:44 2020  Sun Jun 21 01:33:59 2020   \n",
       "train_time   Sun Jun 21 00:52:47 2020  Sun Jun 21 01:34:02 2020   \n",
       "infer_time   Sun Jun 21 01:28:44 2020  Sun Jun 21 02:09:33 2020   \n",
       "psuedo_time  Sun Jun 21 00:58:16 2020  Sun Jun 21 01:39:30 2020   \n",
       "end_time     Sun Jun 21 01:33:59 2020  Sun Jun 21 02:14:50 2020   \n",
       "\n",
       "                                    4  \n",
       "start_time   Sun Jun 21 02:14:50 2020  \n",
       "train_time   Sun Jun 21 02:14:52 2020  \n",
       "infer_time   Sun Jun 21 02:50:20 2020  \n",
       "psuedo_time  Sun Jun 21 02:20:22 2020  \n",
       "end_time     Sun Jun 21 02:55:37 2020  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(timings_dict).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>preliminary_roc_t</th>\n",
       "      <td>0.897304</td>\n",
       "      <td>0.895617</td>\n",
       "      <td>0.904203</td>\n",
       "      <td>0.896016</td>\n",
       "      <td>0.897352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final_roc_t</th>\n",
       "      <td>0.924780</td>\n",
       "      <td>0.920560</td>\n",
       "      <td>0.927742</td>\n",
       "      <td>0.921757</td>\n",
       "      <td>0.916822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preliminary_roc_v</th>\n",
       "      <td>0.871357</td>\n",
       "      <td>0.841462</td>\n",
       "      <td>0.861132</td>\n",
       "      <td>0.877255</td>\n",
       "      <td>0.843678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final_roc_v</th>\n",
       "      <td>0.893172</td>\n",
       "      <td>0.862034</td>\n",
       "      <td>0.875913</td>\n",
       "      <td>0.890654</td>\n",
       "      <td>0.869923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preliminary_acc_t</th>\n",
       "      <td>0.900804</td>\n",
       "      <td>0.896161</td>\n",
       "      <td>0.895179</td>\n",
       "      <td>0.894732</td>\n",
       "      <td>0.896161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final_acc_t</th>\n",
       "      <td>0.906964</td>\n",
       "      <td>0.902946</td>\n",
       "      <td>0.909107</td>\n",
       "      <td>0.906429</td>\n",
       "      <td>0.900625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preliminary_acc_v</th>\n",
       "      <td>0.894643</td>\n",
       "      <td>0.883571</td>\n",
       "      <td>0.884643</td>\n",
       "      <td>0.888929</td>\n",
       "      <td>0.892143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final_acc_v</th>\n",
       "      <td>0.900714</td>\n",
       "      <td>0.886071</td>\n",
       "      <td>0.888929</td>\n",
       "      <td>0.890357</td>\n",
       "      <td>0.891429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0         1         2         3         4\n",
       "preliminary_roc_t  0.897304  0.895617  0.904203  0.896016  0.897352\n",
       "final_roc_t        0.924780  0.920560  0.927742  0.921757  0.916822\n",
       "preliminary_roc_v  0.871357  0.841462  0.861132  0.877255  0.843678\n",
       "final_roc_v        0.893172  0.862034  0.875913  0.890654  0.869923\n",
       "preliminary_acc_t  0.900804  0.896161  0.895179  0.894732  0.896161\n",
       "final_acc_t        0.906964  0.902946  0.909107  0.906429  0.900625\n",
       "preliminary_acc_v  0.894643  0.883571  0.884643  0.888929  0.892143\n",
       "final_acc_v        0.900714  0.886071  0.888929  0.890357  0.891429"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(cv_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['toxic_pred'] = pred_df[[i for i in pred_df_test.columns if i.startswith('final')]].mean(axis=1)\n",
    "pred_df_test['toxic'] = pred_df_test[[i for i in pred_df_test.columns if i.startswith('final')]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_test[['id','toxic']].to_csv(RESULTS_DIR+\"submission.csv\", index=False)\n",
    "pred_df_test.to_csv(RESULTS_DIR+\"all_test_results_\"+MODEL_PREFIX+\".csv\", index=False)\n",
    "pred_df.to_csv(RESULTS_DIR+\"all_results_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>roc</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>0.944303</td>\n",
       "      <td>0.934333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es</td>\n",
       "      <td>0.881716</td>\n",
       "      <td>0.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it</td>\n",
       "      <td>0.878831</td>\n",
       "      <td>0.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr</td>\n",
       "      <td>0.955097</td>\n",
       "      <td>0.922667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang       roc       acc\n",
       "0   en  0.944303  0.934333\n",
       "1   es  0.881716  0.868000\n",
       "2   it  0.878831  0.856800\n",
       "3   tr  0.955097  0.922667"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_group = pd.concat((data, pred_df), axis=1).groupby(['lang'])\n",
    "lang_group[['toxic','toxic_pred']].apply(lambda x: pd.Series({'roc':roc_auc_score(y_true=x.toxic,\n",
    "                                                                                  y_score=x.toxic_pred), \n",
    "                                                              'acc':accuracy_score(y_true=np.round(x.toxic),\n",
    "                                                                                   y_pred=np.round(x.toxic_pred).astype(int))})).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    63812.000000\n",
       "mean         0.312475\n",
       "std          0.272785\n",
       "min          0.003874\n",
       "25%          0.088462\n",
       "50%          0.209212\n",
       "75%          0.487585\n",
       "max          0.997265\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df_test['toxic'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">es</th>\n",
       "      <th>count</th>\n",
       "      <td>8438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.303513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.292887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.007358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.074023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.179264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.453948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.991030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">fr</th>\n",
       "      <th>count</th>\n",
       "      <td>10920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.618398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.235977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.028718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.431032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.648536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.819187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">it</th>\n",
       "      <th>count</th>\n",
       "      <td>8494.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.234843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.220324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.003874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.070719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.156499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.324012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">pt</th>\n",
       "      <th>count</th>\n",
       "      <td>11012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.321357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.252040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.013137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.116639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.231348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.482308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.992207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">ru</th>\n",
       "      <th>count</th>\n",
       "      <td>10948.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.205850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.018225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.112270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.211995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.385408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.969440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">tr</th>\n",
       "      <th>count</th>\n",
       "      <td>14000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.149225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.164133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.004780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.048162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.086946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.176331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.953946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   toxic\n",
       "lang                    \n",
       "es   count   8438.000000\n",
       "     mean       0.303513\n",
       "     std        0.292887\n",
       "     min        0.007358\n",
       "     25%        0.074023\n",
       "     50%        0.179264\n",
       "     75%        0.453948\n",
       "     max        0.991030\n",
       "fr   count  10920.000000\n",
       "     mean       0.618398\n",
       "     std        0.235977\n",
       "     min        0.028718\n",
       "     25%        0.431032\n",
       "     50%        0.648536\n",
       "     75%        0.819187\n",
       "     max        0.997265\n",
       "it   count   8494.000000\n",
       "     mean       0.234843\n",
       "     std        0.220324\n",
       "     min        0.003874\n",
       "     25%        0.070719\n",
       "     50%        0.156499\n",
       "     75%        0.324012\n",
       "     max        0.980412\n",
       "pt   count  11012.000000\n",
       "     mean       0.321357\n",
       "     std        0.252040\n",
       "     min        0.013137\n",
       "     25%        0.116639\n",
       "     50%        0.231348\n",
       "     75%        0.482308\n",
       "     max        0.992207\n",
       "ru   count  10948.000000\n",
       "     mean       0.274299\n",
       "     std        0.205850\n",
       "     min        0.018225\n",
       "     25%        0.112270\n",
       "     50%        0.211995\n",
       "     75%        0.385408\n",
       "     max        0.969440\n",
       "tr   count  14000.000000\n",
       "     mean       0.149225\n",
       "     std        0.164133\n",
       "     min        0.004780\n",
       "     25%        0.048162\n",
       "     50%        0.086946\n",
       "     75%        0.176331\n",
       "     max        0.953946"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df_test.groupby(test['lang'])[['toxic']].apply(pd.Series.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Mr6qXyMSR6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 21 02:55:39 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
