{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changelog:\n",
    "1. Same as V11\n",
    "2. stricter psuedo-label selection\n",
    "3. training without english samples\n",
    "4. triangular cyclical learning rates\n",
    "5. Lang added to Text\n",
    "6. Frozen Tranformers Model's Output (for performance gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yi9E6gwR6bw"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "attachments": {
    "186676f7-f9d2-4727-a904-889751b9f6b6.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:186676f7-f9d2-4727-a904-889751b9f6b6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PhXEkPQNR6bx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm\n"
     ]
    }
   ],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V12\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "MODEL_NAME = ['xlm', 'xlm-large', 'distilbert'][0]; print(MODEL_NAME);\n",
    "\n",
    "NUM_EPOCHS = [32, 32]\n",
    "NUM_FOLDS = 5\n",
    "MIN_LR = 1e-7\n",
    "MAX_LR = 1e-3\n",
    "STEP_SIZE = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "SAMPLE_SIZE = 6000\n",
    "PSUEDO_QUANTILE_THRESH_HIGH = 0.98\n",
    "PSUEDO_QUANTILE_THRESH_LOW = 0.02\n",
    "\n",
    "RUN_ON_SAMPLE = 0\n",
    "if RUN_ON_SAMPLE>0:\n",
    "    SAMPLE_SIZE = RUN_ON_SAMPLE\n",
    "\n",
    "ON_KAGGLE = False\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    BATCH_SIZE = 32\n",
    "    PREDICT_BATCH_SIZE = 512\n",
    "else:\n",
    "    BATCH_SIZE = 32\n",
    "    PREDICT_BATCH_SIZE = 512\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vBSH2FLlR6b1"
   },
   "outputs": [],
   "source": [
    "if ON_KAGGLE:\n",
    "    RESULTS_DIR = '../working/'\n",
    "    DATA_DIR = '../input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = '../input/tf-xlm-roberta-base/'\n",
    "    else:\n",
    "        MODEL_DIR = '../input/tf-distilbert-base-multilingual-cased/'\n",
    "else:\n",
    "    PATH = \"..\"\n",
    "    RESULTS_DIR = PATH+\"/results/\"\n",
    "    DATA_DIR = PATH+\"/data/\"\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = PATH+\"/models/tf-xlm-roberta-base/\"\n",
    "    elif MODEL_NAME == 'xlm-large':\n",
    "        MODEL_DIR = PATH+\"/models/tf-xlm-roberta-large-base/\"\n",
    "    elif MODEL_NAME == 'distilbert-large':\n",
    "        MODEL_DIR = PATH+\"/models/distilbert-base-multilingual-cased-large/\"\n",
    "    elif MODEL_NAME == 'distilbert':\n",
    "        MODEL_DIR = PATH+\"/models/distilbert-base-multilingual-cased/\"\n",
    "    else:\n",
    "        print(\"No Model selected\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCnsk-7nR6b4"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oW_oT4SZR6b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold, LeaveOneGroupOut\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.optimizers import TriangularCyclicalLearningRate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXbm0XVcR6b8"
   },
   "outputs": [],
   "source": [
    "seeded_value = 987258\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ybWWTMP8R6cA",
    "outputId": "c26e2379-92f7-4bcb-bd93-a07825d058f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 22 23:51:35 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ESteN4q4TAAW",
    "outputId": "a6428c7d-4959-486c-9c24-5dc489c8e04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.2.0', '2.11.0', '0.7.0']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR4HCYGTR6cG"
   },
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sJh7lbL1R6cH",
    "outputId": "68c74149-67dc-4a43-9d9d-8cec23e054d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iAdsW80fR6cL"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ8ZRVGTR6cO"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wQ_YumuXR6cO"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR+'jigsaw-toxic-comment-train.csv')\n",
    "validation = pd.read_csv(DATA_DIR+'validation.csv')\n",
    "test = pd.read_csv(DATA_DIR+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5Chd2u5AR6cR"
   },
   "outputs": [],
   "source": [
    "train['lang'] = 'en'\n",
    "\n",
    "train['set'] = 'train'\n",
    "validation['set'] = 'valid'\n",
    "test['set'] = 'test'\n",
    "\n",
    "test['toxic'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pZIaKjKXR6cU",
    "outputId": "f2b9d702-b64c-4838-bbcc-b92a243a8e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate', 'lang', 'set'],\n",
      "      dtype='object')\n",
      "Index(['id', 'comment_text', 'lang', 'toxic', 'set'], dtype='object')\n",
      "Index(['id', 'content', 'lang', 'set', 'toxic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(validation.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['es', 'it', 'tr'], dtype=object),\n",
       " array(['tr', 'ru', 'it', 'fr', 'pt', 'es'], dtype=object))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.lang.unique(), test.lang.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sNlk-kDDR6cY"
   },
   "outputs": [],
   "source": [
    "train.columns = ['id', 'text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'lang', 'set']\n",
    "validation.columns = ['id', 'text', 'lang', 'toxic', 'set']\n",
    "test.columns = ['id', 'text', 'lang', 'set', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "saymbsyYR6ca"
   },
   "outputs": [],
   "source": [
    "REQ_COLS = ['id', 'set', 'text', 'lang', 'toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train[\"text\"].astype(str)\n",
    "validation['text'] = validation[\"text\"].astype(str)\n",
    "test['text'] = test[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2VCNIC95R6cf"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([validation[REQ_COLS]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. XLM-RoBerta uses these special tokens \"sep_token\": \"\\</s\\>\", \"pad_token\": \"\\<pad\\>\", \"cls_token\": \"\\<s\\>\"\n",
    "2. Distilbert uses these special tokens \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {\n",
    "    'en':' english ',\n",
    "    'tr':' turkish ',\n",
    "    'ru':' russian ',\n",
    "    'it':' italian ',\n",
    "    'fr':' french ',\n",
    "    'pt':' portuguese ',\n",
    "    'es':' spanish ',\n",
    "}\n",
    "\n",
    "if MODEL_NAME == 'xlm':\n",
    "    data['text'] = data['lang'].apply(lambda x:languages[x]) + data['text']\n",
    "    test['text'] = test['lang'].apply(lambda x:languages[x]) + test['text']\n",
    "elif MODEL_NAME == 'xlm-large':\n",
    "    data['text'] = data['lang'].apply(lambda x:languages[x]) + data['text']\n",
    "    test['text'] = test['lang'].apply(lambda x:languages[x]) + test['text']\n",
    "elif MODEL_NAME == 'distilbert-large':\n",
    "    data['text'] = data['lang'].apply(lambda x:languages[x])  + data['text']\n",
    "    test['text'] = test['lang'].apply(lambda x:languages[x])  + test['text']\n",
    "elif MODEL_NAME == 'distilbert':\n",
    "    data['text'] = data['lang'].apply(lambda x:languages[x])  + data['text']\n",
    "    test['text'] = test['lang'].apply(lambda x:languages[x])  + test['text']\n",
    "else:\n",
    "    print(\"Nothing Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-EPYDkCNR6ci",
    "outputId": "d36606e0-9c8d-4e2a-d2f8-82d4c9f15590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UlrDxcmLR6cl",
    "outputId": "7436086b-4453-44a1-d6be-627e2bd63171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>set</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>4153</td>\n",
       "      <td>valid</td>\n",
       "      <td>turkish İşte ben de diyorum ki Türkçede farklı bir ada sahip olmadığını iddia ediyorsanız muhtemelen konu ilgi alanınızda değil. Kişisel itham olarak algılamayın bir olasılık olarak söylüyorum. Konuya dair yakın dönem Türkçe hangi kaynağa bakarsanız bakın Kobane/Kobani görürsünüz.    Kud      yaz</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>2736</td>\n",
       "      <td>valid</td>\n",
       "      <td>italian Continuo anch io a pensare che la frase rispecchia perfettamente il pensiero di Odifreddi, che è quello di cui parla la voce su Odifreddi. Se la voce Adolf Hitler iniziasse con la citazione  gli ebrei sono dei parassiti nel corpo delle altre nazioni  ci sarebbero gli stessi problemi? .mau. ✉</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    set  \\\n",
       "4153  4153  valid   \n",
       "2736  2736  valid   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                text  \\\n",
       "4153   turkish İşte ben de diyorum ki Türkçede farklı bir ada sahip olmadığını iddia ediyorsanız muhtemelen konu ilgi alanınızda değil. Kişisel itham olarak algılamayın bir olasılık olarak söylüyorum. Konuya dair yakın dönem Türkçe hangi kaynağa bakarsanız bakın Kobane/Kobani görürsünüz.    Kud      yaz       \n",
       "2736   italian Continuo anch io a pensare che la frase rispecchia perfettamente il pensiero di Odifreddi, che è quello di cui parla la voce su Odifreddi. Se la voce Adolf Hitler iniziasse con la citazione  gli ebrei sono dei parassiti nel corpo delle altre nazioni  ci sarebbero gli stessi problemi? .mau. ✉    \n",
       "\n",
       "     lang  toxic  \n",
       "4153   tr      0  \n",
       "2736   it      0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CpA5ibkQR6cn",
    "outputId": "dfd2a1f9-d403-43fb-edae-9ca8be5784dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">valid</th>\n",
       "      <th>es</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.106667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     toxic\n",
       "set   lang                \n",
       "valid es    2500  0.168800\n",
       "      it    2500  0.195200\n",
       "      tr    3000  0.106667"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "57X-txA_raVA",
    "outputId": "3830e4ae-e5f9-4ab5-a689-050b25913c48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">test</th>\n",
       "      <th>es</th>\n",
       "      <td>8438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>10920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>8494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>11012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>10948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>14000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  toxic\n",
       "set  lang              \n",
       "test es     8438      0\n",
       "     fr    10920      0\n",
       "     it     8494      0\n",
       "     pt    11012      0\n",
       "     ru    10948      0\n",
       "     tr    14000      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby([\"set\", \"lang\"]).agg({'id':'count', 'toxic':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE>0:\n",
    "    data = data.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)\n",
    "    test = test.sample(RUN_ON_SAMPLE).copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LLX9UbtR6ct"
   },
   "source": [
    "# Tokenizer, Config & Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gnjg9vXYR6cu"
   },
   "source": [
    "1. https://arxiv.org/pdf/1911.02116.pdf\n",
    "2. https://huggingface.co/transformers/model_doc/xlmroberta.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"**Sample Text**\")\n",
    "# t = data['text'][0]\n",
    "# encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "# decoded_tokens = xlmr_tok.decode(encoded_text['input_ids'])\n",
    "# print(t)\n",
    "# print(\"**Encoded Tokens**\")\n",
    "# print(encoded_text['input_ids'], sep=\",\")\n",
    "# print(\"**Decoded Tokens**\")\n",
    "# print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INFERENCE_BATCHSIZE = {\n",
    "    'xlm' : 128,\n",
    "    'distilbert' : 256,\n",
    "    'xlm-large' : 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model_path(MODEL_NAME):\n",
    "    PATH = \"..\"\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        MODEL_DIR = PATH+\"/models/tf-xlm-roberta-base/\"\n",
    "    elif MODEL_NAME == 'xlm-large':\n",
    "        MODEL_DIR = PATH+\"/models/tf-xlm-roberta-large-base/\"\n",
    "    elif MODEL_NAME == 'distilbert-large':\n",
    "        MODEL_DIR = PATH+\"/models/distilbert-base-multilingual-cased-large/\"\n",
    "    elif MODEL_NAME == 'distilbert':\n",
    "        MODEL_DIR = PATH+\"/models/distilbert-base-multilingual-cased/\"\n",
    "    else:\n",
    "        print(\"No Model selected\")\n",
    "        pass\n",
    "    return MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(MODEL_NAME):\n",
    "    \n",
    "    MODEL_DIR = find_model_path(MODEL_NAME)\n",
    "\n",
    "    print(\"Loading Tokenizer\")\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        xlmr_tok = transformers.XLMRobertaTokenizer.from_pretrained(MODEL_DIR)\n",
    "    elif MODEL_NAME == 'xlm-large':\n",
    "        xlmr_tok = transformers.XLMRobertaTokenizer.from_pretrained(MODEL_DIR)\n",
    "    elif MODEL_NAME == 'distilbert-large':\n",
    "        xlmr_tok = transformers.DistilBertTokenizer.from_pretrained(MODEL_DIR)\n",
    "    elif MODEL_NAME == 'distilbert':\n",
    "        xlmr_tok = transformers.DistilBertTokenizer.from_pretrained(MODEL_DIR)\n",
    "    else:\n",
    "        print(\"No tokenizer Selected\")\n",
    "\n",
    "    print(\"Tokenization on Training Set\")\n",
    "    X_tokens, X_att = [], []\n",
    "    for n, t in enumerate(tqdm(data.text.tolist())):\n",
    "        encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "        X_tokens.append(encoded_text['input_ids'])\n",
    "        X_att.append(encoded_text['attention_mask'])\n",
    "\n",
    "    X_tokens, X_att = np.array(X_tokens, dtype=np.int32), np.array(X_att, dtype=np.int32)\n",
    "    X_tokens, X_att, X_lang, Y_toxic = X_tokens, X_att, data['lang'].values, data['toxic'].values\n",
    "\n",
    "    print(\"Tokenization on Test Set\")\n",
    "    X_tokens_test, X_att_test = [], []\n",
    "    for n, t in enumerate(tqdm(test.text.tolist())):\n",
    "        encoded_text = xlmr_tok.encode_plus(t, pad_to_max_length=True, max_length=MAX_SEQ_LEN)\n",
    "        X_tokens_test.append(encoded_text['input_ids'])\n",
    "        X_att_test.append(encoded_text['attention_mask'])\n",
    "    \n",
    "    X_tokens_test, X_att_test = np.array(X_tokens_test, dtype=np.int32), np.array(X_att_test, dtype=np.int32)\n",
    "    X_tokens_test, X_att_test, X_lang_test = X_tokens_test, X_att_test, test['lang'].values\n",
    "    return (X_tokens, X_att, X_lang, Y_toxic, X_tokens_test, X_att_test, X_lang_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(MODEL_NAME, BATCH):\n",
    "    \n",
    "    MODEL_DIR = find_model_path(MODEL_NAME)\n",
    "    \n",
    "    input_tokens = Input((MAX_SEQ_LEN), name=\"input_tokens\", dtype=tf.int32)\n",
    "    input_atts = Input((MAX_SEQ_LEN), name=\"input_atts\", dtype=tf.int32)\n",
    "    \n",
    "    print(\"Loading Model\", MODEL_NAME)\n",
    "    if MODEL_NAME == 'xlm':\n",
    "        config = transformers.XLMRobertaConfig.from_pretrained(MODEL_DIR)\n",
    "        base_model = transformers.TFXLMRobertaModel.from_pretrained(MODEL_DIR, config=config)\n",
    "    elif MODEL_NAME == 'xlm-large':\n",
    "        config = transformers.XLMRobertaConfig.from_pretrained(MODEL_DIR)\n",
    "        base_model = transformers.TFXLMRobertaModel.from_pretrained(MODEL_DIR, config=config)\n",
    "    elif MODEL_NAME == 'distilbert-large':\n",
    "        config = transformers.DistilBertConfig.from_pretrained(MODEL_DIR)\n",
    "        base_model = transformers.TFDistilBertModel.from_pretrained(MODEL_DIR, config=config)\n",
    "    elif MODEL_NAME == 'distilbert':\n",
    "        config = transformers.DistilBertConfig.from_pretrained(MODEL_DIR)\n",
    "        base_model = transformers.TFDistilBertModel.from_pretrained(MODEL_DIR, config=config)\n",
    "    else:\n",
    "        print(\"No Model Chosen\")\n",
    "    \n",
    "    if MODEL_NAME=='distilbert':\n",
    "        o1 = base_model(inputs=input_tokens, attention_mask=input_atts)[0]\n",
    "    else:\n",
    "        o1, o2 = base_model(inputs=input_tokens, attention_mask=input_atts)\n",
    "    \n",
    "    model = Model([input_tokens, input_atts], o1)\n",
    "    \n",
    "    X_inputs = model.predict([X_tokens, X_att], batch_size=BATCH)\n",
    "    X_inputs_test = model.predict([X_tokens_test, X_att_test], batch_size=BATCH)\n",
    "    \n",
    "#     NUM_BATCHES = np.int(np.ceil(len(X_tokens)/BATCH))\n",
    "#     print(\"There are\", NUM_BATCHES, \"Batches in Training Data\")\n",
    "#     for n, (t, a) in enumerate(tqdm(zip(np.array_split(X_tokens, NUM_BATCHES),\n",
    "#                                         np.array_split(X_att, NUM_BATCHES)), total=NUM_BATCHES)):\n",
    "#         if MODEL_NAME=='distilbert':\n",
    "#             o1 = base_model(inputs=t, attention_mask=a)[0]\n",
    "#         else:\n",
    "#             o1, o2 = base_model(inputs=t, attention_mask=a)\n",
    "#         if n==0:\n",
    "#             X_inputs = np.array(o1)\n",
    "#             print(X_inputs.shape)\n",
    "#         else:\n",
    "#             X_inputs = np.concatenate((X_inputs, np.array(o1)))\n",
    "    \n",
    "#     X_inputs = np.array(X_inputs)\n",
    "\n",
    "#     NUM_BATCHES = np.int(np.ceil(len(X_tokens_test)/BATCH))\n",
    "#     print(\"There are\", NUM_BATCHES, \"Batches in Testing Data\")\n",
    "#     for n, (t, a) in enumerate(tqdm(zip(np.array_split(X_tokens_test, NUM_BATCHES),\n",
    "#                                         np.array_split(X_att_test, NUM_BATCHES)), total=NUM_BATCHES)):\n",
    "#         if MODEL_NAME=='distilbert':\n",
    "#             o1 = base_model(inputs=t, attention_mask=a)[0]\n",
    "#         else:\n",
    "#             o1, o2 = base_model(inputs=t, attention_mask=a)\n",
    "#         if n==0:\n",
    "#             X_inputs_test = np.array(o1)\n",
    "#             print(X_inputs_test.shape)\n",
    "#         else:\n",
    "#             X_inputs_test = np.concatenate((X_inputs_test, np.array(o1)))\n",
    "\n",
    "#     X_inputs_test = np.array(X_inputs_test)\n",
    "\n",
    "    del config\n",
    "    del base_model\n",
    "    del model\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "\n",
    "    return (X_inputs, X_inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert'\n",
    "X_tokens, X_att, X_lang, Y_toxic, X_tokens_test, X_att_test, X_lang_test = prepare_data(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model distilbert\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[256,128,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_distil_bert_model_1/distilbert/transformer/layer_._1/ffn/activation_14/truediv (defined at /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/transformers/modeling_tf_distilbert.py:53) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_8508]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/tf_distil_bert_model_1/distilbert/transformer/layer_._1/ffn/activation_14/truediv:\n model/tf_distil_bert_model_1/distilbert/transformer/layer_._1/ffn/lin1/BiasAdd (defined at /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/transformers/modeling_tf_distilbert.py:280)\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-46f85c7e27c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m X_distilbert_inputs, X_distilbert_inputs_test = extract_features(MODEL_NAME=MODEL_NAME,\n\u001b[0;32m----> 2\u001b[0;31m                                                                  BATCH = MODEL_INFERENCE_BATCHSIZE[MODEL_NAME])\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(\"\\n**\",MODEL_NAME,\"**\",\n\u001b[1;32m      5\u001b[0m       \u001b[0;34m\"\\n**Training Set Inputs**\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-680e9c2b9787>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(MODEL_NAME, BATCH)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mX_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_att\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mX_inputs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_tokens_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_att_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#     NUM_BATCHES = np.int(np.ceil(len(X_tokens)/BATCH))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[256,128,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_distil_bert_model_1/distilbert/transformer/layer_._1/ffn/activation_14/truediv (defined at /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/transformers/modeling_tf_distilbert.py:53) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_8508]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/tf_distil_bert_model_1/distilbert/transformer/layer_._1/ffn/activation_14/truediv:\n model/tf_distil_bert_model_1/distilbert/transformer/layer_._1/ffn/lin1/BiasAdd (defined at /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/transformers/modeling_tf_distilbert.py:280)\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "X_distilbert_inputs, X_distilbert_inputs_test = extract_features(MODEL_NAME=MODEL_NAME,\n",
    "                                                                 BATCH = MODEL_INFERENCE_BATCHSIZE[MODEL_NAME])\n",
    "\n",
    "print(\"\\n**\",MODEL_NAME,\"**\",\n",
    "      \"\\n**Training Set Inputs**\\n\",\n",
    "      X_tokens.shape, \"\\t: X_tokens \", \"\\n\",\n",
    "      X_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "      X_lang.shape, \"\\t: X_lang \", \"\\n\",\n",
    "      Y_toxic.shape, \"\\t: Y_toxic \", \"\\n\",\n",
    "      \"\\n**Training Set Outputs**\\n\",\n",
    "      X_distilbert_inputs.shape, \"\\t: X_distilbert_inputs \", \"\\n\",          \n",
    "      \"\\n**Test Set Inputs**\\n\",\n",
    "      X_tokens_test.shape, \"\\t: X_tokens_test \", \"\\n\",\n",
    "      X_att_test.shape, \"\\t: X_att_test \", \"\\n\",\n",
    "      X_lang_test.shape, \"\\t: X_lang_test \", \"\\n\"\n",
    "      \"\\n**Test Set Outputs**\\n\",\n",
    "      X_distilbert_inputs_test.shape, \"\\t: X_distilbert_inputs_test \", \"\\n\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'xlm'\n",
    "X_tokens, X_att, X_lang, Y_toxic, X_tokens_test, X_att_test, X_lang_test = prepare_data(MODEL_NAME)\n",
    "X_xlm_inputs, X_xlm_inputs_test = extract_features(MODEL_NAME=MODEL_NAME,\n",
    "                                                   BATCH = MODEL_INFERENCE_BATCHSIZE[MODEL_NAME])\n",
    "\n",
    "print(\"\\n**\",MODEL_NAME,\"**\",\n",
    "      \"\\n**Training Set Inputs**\\n\",\n",
    "      X_tokens.shape, \"\\t: X_tokens \", \"\\n\",\n",
    "      X_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "      X_lang.shape, \"\\t: X_lang \", \"\\n\",\n",
    "      Y_toxic.shape, \"\\t: Y_toxic \", \"\\n\",\n",
    "      \"\\n**Training Set Outputs**\\n\",\n",
    "      X_xlm_inputs.shape, \"\\t: X_xlm_inputs \", \"\\n\",          \n",
    "      \"\\n**Test Set Inputs**\\n\",\n",
    "      X_tokens_test.shape, \"\\t: X_tokens_test \", \"\\n\",\n",
    "      X_att_test.shape, \"\\t: X_att_test \", \"\\n\",\n",
    "      X_lang_test.shape, \"\\t: X_lang_test \", \"\\n\"\n",
    "      \"\\n**Test Set Outputs**\\n\",\n",
    "      X_xlm_inputs_test.shape, \"\\t: X_xlm_inputs_test \", \"\\n\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKWwyFjzR6dN"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    distilbert_features = Input((MAX_SEQ_LEN, X_distilbert_inputs.shape[2]), name=\"distilbert_features\")\n",
    "    x1 = tf.keras.layers.Conv1D(X_distilbert_inputs.shape[2], 2, padding='same')(distilbert_features)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    \n",
    "    xlm_roberta_features = Input((MAX_SEQ_LEN, X_xlm_inputs.shape[2]), name=\"xlm_roberta_features\")\n",
    "    x2 = tf.keras.layers.Conv1D(X_xlm_inputs.shape[2], 2, padding='same')(xlm_roberta_features)\n",
    "    x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dropout(DROPOUT)(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    \n",
    "    x_concat = tf.keras.layers.concatenate([x1, x2])\n",
    "    toxic_output = tf.keras.layers.Dense(1, activation='sigmoid', name='toxic_output')(x_concat)\n",
    "    \n",
    "    model = Model([distilbert_features, xlm_roberta_features],\n",
    "                  [toxic_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KFold train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(NUM_FOLDS, shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeR6DmO2R6dN"
   },
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GGb6IwuR6dY"
   },
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = data[['id']]\n",
    "pred_df_test = test[['id']]\n",
    "timings_dict = {}\n",
    "cv_stats = {}\n",
    "MODEL_NAME = \"2Combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, (t_index, v_index) in enumerate(kf.split(X_tokens, Y_toxic)):\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    start_time = time()\n",
    "\n",
    "    if num>0:\n",
    "        del model\n",
    "        del mcp\n",
    "        del csvl\n",
    "        del adam\n",
    "        del history\n",
    "        del auc\n",
    "        del tclr\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "    model = build_model()\n",
    "    auc = tf.keras.metrics.AUC(name='auc')\n",
    "    mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\"_\"+MODEL_NAME+\"_\"+MODEL_PREFIX+\".h5\",\n",
    "                          monitor='val_auc', verbose=0, save_best_only=True, save_weights_only=True,\n",
    "                          mode='max', save_freq='epoch')\n",
    "    csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_\"+str(num)+\"_LossLogs_\"+MODEL_NAME+\"_\"+MODEL_PREFIX+\".csv\",\n",
    "                     separator=\",\", append=True)\n",
    "    tclr = tfa.optimizers.TriangularCyclicalLearningRate(\n",
    "        initial_learning_rate=MIN_LR,\n",
    "        maximal_learning_rate=MAX_LR,\n",
    "        step_size=STEP_SIZE*len(t_index)\n",
    "    )\n",
    "\n",
    "    model.layers[2].trainable = False\n",
    "    adam = Adam(learning_rate=tclr)\n",
    "    model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy', auc])\n",
    "\n",
    "    train_time = time()\n",
    "    history = model.fit(x={'distilbert_features':X_distilbert_inputs[t_index],\n",
    "                           'xlm_roberta_features':X_xlm_inputs[t_index]},\n",
    "                        y={\"toxic_output\":Y_toxic[t_index]},\n",
    "                        validation_data=({'distilbert_features':X_distilbert_inputs[v_index],\n",
    "                                          'xlm_roberta_features':X_xlm_inputs[v_index]},\n",
    "                                         {\"toxic_output\":Y_toxic[v_index]}),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[0],\n",
    "                        shuffle=True,\n",
    "                        verbose=1, \n",
    "                        sample_weight=class_weight.compute_sample_weight('balanced', X_lang[t_index]),\n",
    "                        callbacks=[mcp, csvl])\n",
    "\n",
    "    # Loading best weights per fold\n",
    "    model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\"_\"+MODEL_NAME+\"_\"+MODEL_PREFIX+\".h5\")\n",
    "    \n",
    "    pred_initial = model.predict(x = {'distilbert_features':X_distilbert_inputs,\n",
    "                                      'xlm_roberta_features':X_xlm_inputs},\n",
    "                                 batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_df['fold_'+str(num)] = 0\n",
    "    pred_df['fold_'+str(num)].iloc[t_index] = 'train'\n",
    "    pred_df['fold_'+str(num)].iloc[v_index] = 'valid'\n",
    "    pred_df['initial_'+str(num)] = pred_initial\n",
    "    \n",
    "    roc_t = roc_auc_score(y_true=Y_toxic[t_index], y_score=pred_initial[t_index])\n",
    "    roc_v = roc_auc_score(y_true=Y_toxic[v_index], y_score=pred_initial[v_index])\n",
    "    acc_t = accuracy_score(y_true=Y_toxic[t_index], y_pred=np.where(pred_initial[t_index]>0.5, 1, 0))\n",
    "    acc_v = accuracy_score(y_true=Y_toxic[v_index], y_pred=np.where(pred_initial[v_index]>0.5, 1, 0))\n",
    "    \n",
    "    print(\"Train ROC-AUC:\\t\", roc_t)\n",
    "    print(\"Valid ROC-AUC:\\t\", roc_v)\n",
    "    print(\"Train Accuracy:\\t\", acc_t)\n",
    "    print(\"Valid Accuracy:\\t\", acc_v)\n",
    "\n",
    "    print(classification_report(y_true=Y_toxic[t_index], y_pred=np.where(pred_initial[t_index]>0.5, 1, 0)))\n",
    "    print(classification_report(y_true=Y_toxic[v_index], y_pred=np.where(pred_initial[v_index]>0.5, 1, 0)))\n",
    "\n",
    "    # Psuedo model fit\n",
    "    psuedo_time = time()\n",
    "\n",
    "    # Accumulate test results after training every fold\n",
    "    pred_psuedo = model.predict(x = {'distilbert_features':X_distilbert_inputs_test,\n",
    "                                     'xlm_roberta_features':X_xlm_inputs_test},\n",
    "                                batch_size=PREDICT_BATCH_SIZE).reshape((-1))\n",
    "\n",
    "    pred_df_test['psuedo_'+str(num)] = pred_psuedo\n",
    "    PSUEDO_PROB_THRESH_HIGH = pred_df_test['psuedo_'+str(num)].quantile(PSUEDO_QUANTILE_THRESH_HIGH)\n",
    "    PSUEDO_PROB_THRESH_LOW = pred_df_test['psuedo_'+str(num)].quantile(PSUEDO_QUANTILE_THRESH_LOW)\n",
    "    \n",
    "    print(\"PSUEDO_PROB_THRESH_HIGH\", np.round(PSUEDO_PROB_THRESH_HIGH,3))\n",
    "    print(\"PSUEDO_PROB_THRESH_LOW\", np.round(PSUEDO_PROB_THRESH_LOW,3))\n",
    "\n",
    "    Y_toxic_psuedo = np.where(pred_psuedo >= PSUEDO_PROB_THRESH_HIGH, 1, 0)\n",
    "    psuedo_flag = (pred_psuedo >= PSUEDO_PROB_THRESH_HIGH) | (pred_psuedo <= PSUEDO_PROB_THRESH_LOW)\n",
    "\n",
    "    print(\"Number of psuedo samples available:\", sum(psuedo_flag))\n",
    "    print(\"Psuedo Toxicity:\", sum(Y_toxic_psuedo))\n",
    "    print(Counter(test.lang.values))\n",
    "    print(Counter(test.lang.values[psuedo_flag]))\n",
    "\n",
    "    X_distilbert_inputs_psuedo = np.concatenate((X_distilbert_inputs[t_index], X_distilbert_inputs_test[psuedo_flag]))\n",
    "    X_xlm_inputs_psuedo = np.concatenate((X_xlm_inputs[t_index], X_xlm_inputs_test[psuedo_flag]))\n",
    "    Y_toxic_psuedo = np.concatenate((Y_toxic[t_index], Y_toxic_psuedo[psuedo_flag]))\n",
    "    X_lang_psuedo = np.concatenate((X_lang[t_index], X_lang_test[psuedo_flag]))\n",
    "\n",
    "    shuffled_idxs = np.arange(Y_toxic_psuedo.shape[0])\n",
    "    np.random.shuffle(shuffled_idxs)\n",
    "\n",
    "    model.layers[2].trainable = False\n",
    "    adam = Adam(learning_rate=tclr)\n",
    "    model.compile(loss={\"toxic_output\":tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING_PARAM)},\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy', auc])\n",
    "\n",
    "    history = model.fit(x={'distilbert_features':X_distilbert_inputs_psuedo[shuffled_idxs],\n",
    "                           'xlm_roberta_features':X_xlm_inputs_psuedo[shuffled_idxs]},\n",
    "                        y={\"toxic_output\":Y_toxic_psuedo[shuffled_idxs]},\n",
    "                        validation_data=({'distilbert_features':X_distilbert_inputs[v_index],\n",
    "                                          'xlm_roberta_features':X_xlm_inputs[v_index]},\n",
    "                                         {\"toxic_output\":Y_toxic[v_index]}),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[1],\n",
    "                        shuffle=True,\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl], \n",
    "                        sample_weight=class_weight.compute_sample_weight('balanced', X_lang_psuedo[shuffled_idxs]))\n",
    "\n",
    "    # Loading best weights per fold\n",
    "    model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\"_\"+MODEL_NAME+\"_\"+MODEL_PREFIX+\".h5\")\n",
    "\n",
    "    infer_time = time()\n",
    "\n",
    "    pred = model.predict(x = {'distilbert_features':X_distilbert_inputs,\n",
    "                              'xlm_roberta_features':X_xlm_inputs},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_df['final_'+str(num)] = pred\n",
    "\n",
    "    final_roc_t = roc_auc_score(y_true=Y_toxic[t_index], y_score=pred[t_index])\n",
    "    final_roc_v = roc_auc_score(y_true=Y_toxic[v_index], y_score=pred[v_index])\n",
    "    final_acc_t = accuracy_score(y_true=Y_toxic[t_index], y_pred=np.where(pred[t_index]>0.5, 1, 0))\n",
    "    final_acc_v = accuracy_score(y_true=Y_toxic[v_index], y_pred=np.where(pred[v_index]>0.5, 1, 0))    \n",
    "    \n",
    "    print(\"Train ROC-AUC:\\t\", final_roc_t)\n",
    "    print(\"Valid ROC-AUC:\\t\", final_roc_v)\n",
    "    print(\"Train Accuracy:\\t\", final_acc_t)\n",
    "    print(\"Valid Accuracy:\\t\", final_acc_v)\n",
    "\n",
    "    print(classification_report(y_true=Y_toxic[t_index], y_pred=np.where(pred[t_index]>0.5, 1, 0)))\n",
    "    print(classification_report(y_true=Y_toxic[v_index], y_pred=np.where(pred[v_index]>0.5, 1, 0)))\n",
    "\n",
    "    # Accumulate test results after training every fold\n",
    "\n",
    "    pred_test = model.predict(x = {'distilbert_features':X_distilbert_inputs_test,\n",
    "                                   'xlm_roberta_features':X_xlm_inputs_test},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_df_test['final_'+str(num)] = pred_test\n",
    "\n",
    "    end_time = time()\n",
    "    timings_dict.update({num:{\n",
    "        'start_time' : ctime(start_time),\n",
    "        'train_time' : ctime(train_time),\n",
    "        'infer_time' : ctime(infer_time),\n",
    "        'psuedo_time' : ctime(psuedo_time),\n",
    "        'end_time' : ctime(end_time),\n",
    "    }})\n",
    "    \n",
    "    cv_stats.update({num:{\n",
    "        'preliminary_roc_t':roc_t,\n",
    "        'final_roc_t':final_roc_t,\n",
    "        'preliminary_roc_v':roc_v,\n",
    "        'final_roc_v':final_roc_v,\n",
    "        'preliminary_acc_t':acc_t,\n",
    "        'final_acc_t':final_acc_t,\n",
    "        'preliminary_acc_v':acc_v,\n",
    "        'final_acc_v':final_acc_v,\n",
    "    }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(timings_dict).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(cv_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['toxic_pred'] = pred_df[[i for i in pred_df_test.columns if i.startswith('final')]].mean(axis=1)\n",
    "pred_df_test['toxic'] = pred_df_test[[i for i in pred_df_test.columns if i.startswith('final')]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_test[['id','toxic']].to_csv(RESULTS_DIR+\"submission_\"+MODEL_NAME+\"_\"+MODEL_PREFIX+\".csv\", index=False)\n",
    "pred_df_test.to_csv(RESULTS_DIR+\"all_test_results_\"+MODEL_NAME+\"_\"+MODEL_PREFIX+\".csv\", index=False)\n",
    "pred_df.to_csv(RESULTS_DIR+\"all_results_\"+MODEL_NAME+\"_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_group = pd.concat((data, pred_df), axis=1).groupby(['lang'])\n",
    "lang_group[['toxic','toxic_pred']].apply(lambda x: pd.Series({'roc':roc_auc_score(y_true=x.toxic,\n",
    "                                                                                  y_score=x.toxic_pred), \n",
    "                                                              'acc':accuracy_score(y_true=np.round(x.toxic),\n",
    "                                                                                   y_pred=np.round(x.toxic_pred).astype(int))})).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_test['toxic'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_test.groupby(test['lang'])[['toxic']].apply(pd.Series.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mr6qXyMSR6d6"
   },
   "outputs": [],
   "source": [
    "print(ctime(time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "conda-env-py37_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
